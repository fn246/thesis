{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "I=0\n",
    "def _parse_function(example_proto):\n",
    "    print('1')\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    print(\"i was here\")\n",
    "    print('2')\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "def rbm_layer(n_visible, n_hidden, num_epochs, num_cases, lr, ws, bs, layer_n, len_data, directories):\n",
    "    Data_path = directories[0]\n",
    "    tfrecord_folder_parent = directories[1]\n",
    "    tfrecord_folder = directories[2]\n",
    "    \n",
    "    tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "    sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "    trainfilenames_x = []\n",
    "    for i in sorted_names_x:\n",
    "        trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path_x,i)))\n",
    "#     filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "#     dataset_x = tf.data.TFRecordDataset(filenames_x)\n",
    "    dataset_x = tf.data.TFRecordDataset(trainfilenames_x)\n",
    "    dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "    dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "    dataset_x = dataset_x.batch(num_cases)\n",
    "#     iterator_x = dataset_x.make_initializable_iterator()\n",
    "    \n",
    "    weightcost  = 0.0002\n",
    "#     initialmomentum  = 0.5\n",
    "#     finalmomentum    = 0.9\n",
    "    momentum  = 0.5\n",
    "    numcases = 32\n",
    "    W_adder  = tf.zeros((n_visible,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bh_adder = tf.zeros((1,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bv_adder = tf.zeros((1,n_visible),dtype=tf.dtypes.float32)\n",
    "#     x  = tf.placeholder(tf.float32, [None, n_visible], name=\"x\") #The placeholder variable that holds our data\n",
    "#     h = tf.placeholder(tf.float32, [None, n_hidden], name=\"h\")\n",
    "#     h = tf.zeros((numcases, n_hidden),dtype=tf.dtypes.float32)\n",
    "#     h_empty  = np.zeros_like(h)\n",
    "#     h_empty = np.zeros((, n_hidden),dtype=np.float32)\n",
    "#     m  = tf.Variable(0.5, dtype=np.float32, name='m')\n",
    "    W  = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "    bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "    bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "    def sample(probs):\n",
    "        #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "        return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "\n",
    "#     hk = sample(tf.sigmoid(tf.matmul(x, W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "#     #128*512\n",
    "\n",
    "#     #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "#     posprods  = tf.matmul(tf.transpose(x),hk) #771*512\n",
    "#     poshidacts = tf.reduce_sum(hk) #(512)\n",
    "#     posvisacts = tf.reduce_sum(x,axis=0)#771\n",
    "    \n",
    "#     xk   = tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)#128*711\n",
    "#     neghidprobs   = tf.sigmoid(tf.matmul(xk, W) + bh) #128*512\n",
    "#     negprods  = tf.matmul(tf.transpose(xk),neghidprobs); #771*512##################################\n",
    "#     neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "#     negvisacts = tf.reduce_sum(xk, axis=0)\n",
    "#     print(negvisacts)#771\n",
    "# #     print(type(posprod-negprod))\n",
    "# #     print(type(lr))\n",
    "#     m=0.5\n",
    "#     print('here')\n",
    "#     W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "#     bv_adder = (m * bv_adder)+ ((lr/numcases)*(posvisacts-negvisacts))\n",
    "#     bh_adder = (m * bh_adder)+ ((lr/numcases)*(poshidacts-neghidacts))\n",
    "#     #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "#     updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]\n",
    "\n",
    "    ### Run the graph!\n",
    "    # Now it's time to start a session and run the graph! \n",
    "\n",
    "#     with tf.Session() as sess:\n",
    "        #First, we train the model\n",
    "        #initialize the variables of the model\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         print(m)\n",
    "#         sess.run(iterator_x.initializer, feed_dict={filenames_x: trainfilenames_x})\n",
    "#         print(iterator_x.get_next()[0])\n",
    "        #Run through all of the training data num_epochs times\n",
    "#         for epoch in tqdm(range(num_epochs)):\n",
    "            #Train the RBM on batch_size examples at a time\n",
    "#             for X_batch in da(batch_size, layer_n, ws, bs, len_data, name, data_name):\n",
    "#             Data = sess.run(iterator_x.get_next()[0])\n",
    "#             print('blah')\n",
    "#             print(Data.shape)\n",
    "#             for Data in dataset_x:\n",
    "#                 for j in range(layer_n):\n",
    "#                     Data = np.matmul(Data,ws[j])+bs[j]\n",
    "    #             if epoch>5:\n",
    "    #                 momentum=finalmomentum;\n",
    "    #             else:\n",
    "    #                 momentum=initialmomentum;\n",
    "#                 feed_dict = {x: Data}\n",
    "#                 var1 = sess.run(updt, feed_dict)\n",
    "    count=0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        count+=1\n",
    "        for x in dataset_x:\n",
    "            hk = sample(tf.sigmoid(tf.matmul(x[0], W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "            #128*512\n",
    "            #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "            posprods  = tf.matmul(tf.transpose(x[0]),hk) #771*512\n",
    "            poshidacts = tf.reduce_sum(hk) #(512)\n",
    "            posvisacts = tf.reduce_sum(x[0],axis=0)#771\n",
    "\n",
    "            xk   = tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)#128*711\n",
    "            neghidprobs   = tf.sigmoid(tf.matmul(xk, W) + bh) #128*512\n",
    "            negprods  = tf.matmul(tf.transpose(xk),neghidprobs); #771*512##################################\n",
    "            neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "            negvisacts = tf.reduce_sum(xk, axis=0)\n",
    "#             print(negvisacts)#771\n",
    "        #     print(type(posprod-negprod))\n",
    "        #     print(type(lr))\n",
    "            m=0.5\n",
    "            W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "            bv_adder = (m * bv_adder)+ ((lr/numcases)*(posvisacts-negvisacts))\n",
    "            bh_adder = (m * bh_adder)+ ((lr/numcases)*(poshidacts-neghidacts))\n",
    "            #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "            updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)] \n",
    "        print(count)\n",
    "    return updt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "1\n",
      "i was here\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = os.getcwd()\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'tfrecord_files_10h_norm'\n",
    "tfrecord_val_folder = 'validation_10h_norm'\n",
    "ckpt_folder = '3'\n",
    "dirs = [Data_path, tfrecord_folder_parent, tfrecord_folder]\n",
    " \n",
    "# len_data = (684108, 257)\n",
    "len_data = (100000, 257)\n",
    "val_len = (97278,257)\n",
    "w=3\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=200, win_length=500, window='hann')\n",
    "    return recon\n",
    "#######################\n",
    "I=0\n",
    "global batch_size\n",
    "batch_size = 32\n",
    "# epochs_num=50\n",
    "global datalen\n",
    "datalen=len_data[0]\n",
    "\n",
    "h = [512,512]\n",
    "seed = 7\n",
    "rate1 = 0.1\n",
    "rate2 = 0.2\n",
    "from tensorflow.keras.layers import Activation\n",
    "# from keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model.add(layers.Dropout(rate1, noise_shape=None, seed=None))\n",
    "# ,kernel_regularizer=regularizers.l2(0.001)\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act1)\n",
    "# model.add(Activation('sigmoid'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(layers.Dropout(rate2, noise_shape=None, seed=None))\n",
    "model.add(Dense(h[1]))\n",
    "model.add(act2)\n",
    "# act3=layers.LeakyReLU(alpha=0.1)\n",
    "# # model.add(layers.Dropout(rate, noise_shape=None, seed=None))\n",
    "# model.add(Dense(h[2]))\n",
    "# model.add(act3)\n",
    "act=layers.LeakyReLU(alpha=0.01)\n",
    "model.add(Dense(len_data[1]))\n",
    "#############################################\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "# def _parse_function(example_proto):\n",
    "#     features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "#               \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "#     parsed_features = tf.parse_single_example(example_proto, features)\n",
    "#     return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "# tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "# sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "# trainfilenames = []\n",
    "# for i in sorted_names:\n",
    "#     trainfilenames.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# filenames = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset = tf.data.TFRecordDataset(filenames)\n",
    "# dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "# dataset = dataset.batch(batch_size)\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# # orig_path = os.getcwd()\n",
    "# tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "# sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "# trainfilenames_x = []\n",
    "# for i in sorted_names_x:\n",
    "#     trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset_x = tf.data.TFRecordDataset(filenames_x)\n",
    "# dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "# dataset_x = dataset_x.batch(batch_size)\n",
    "# iterator_x = dataset_x.make_initializable_iterator()\n",
    "\n",
    "########################\n",
    "visible = w*len_data[1]\n",
    "hidden = h[0]\n",
    "visible1 = h[0]\n",
    "hidden1 = h[1]\n",
    "visible2 = h[1]\n",
    "hidden2 = len_data[1]\n",
    "\n",
    "layer1 = rbm_layer(visible, hidden, 50, batch_size, 0.0005, [np.eye(visible)], [np.zeros((1,visible))], 1, len_data[0],dirs)\n",
    "# layer2 = rbm_layer(visible1, hidden1, 50, batch_size, 0.01, [np.eye(visible),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, len_data[0], dirs)\n",
    "# layer3 = rbm_layer(visible2, hidden2, 50, batch_size, 0.01, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, len_data[0], dirs)\n",
    "\n",
    "###############################\n",
    "\n",
    "# tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "# sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "# trainfilenames_val = []\n",
    "# for i in sorted_names_val:\n",
    "#     trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "# filenames_val = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset_val = tf.data.TFRecordDataset(filenames_val)\n",
    "# dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "# dataset_val = dataset_val.batch(128)\n",
    "# iterator_val = dataset_val.make_initializable_iterator()\n",
    "\n",
    "# epochs_num = 50\n",
    "# steps = len_data[0] // batch_size\n",
    "# val_steps = val_len[0] // batch_size\n",
    "# # You can feed the initializer with the appropriate filenames for the current\n",
    "# # phase of execution, e.g. training vs. validation.\n",
    "# # next_elem = iterator_val.get_next()\n",
    "# # Initialize `iterator` with training data.\n",
    "\n",
    "# if not os.path.exists(os.path.join(Data_path,\"checkpoints\",ckpt_folder)):\n",
    "#     os.makedirs(os.path.join(Data_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "# print(datetime.datetime.now())\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "#     sess.run(iterator_val.initializer, feed_dict={filenames_val: trainfilenames_val})\n",
    "#     print(\"initialized\")\n",
    "#     checkpoint_path = os.path.normpath(os.path.join(Data_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "#     checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#     cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "#         # Save weights, every 5-epochs.\n",
    "# #         period=1)\n",
    "#     early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "#     opt = tf.keras.optimizers.Adamax()\n",
    "# #     opt = tf.train.AdamOptimizer()\n",
    "# #     opt = tf.keras.optimizers.SGD()\n",
    "#     model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "#     history = model.fit( iterator, steps_per_epoch=steps,epochs=epochs_num, callbacks = [cp_callback,early_stop], verbose=1,validation_data=iterator_val,validation_steps=val_steps)\n",
    "# #     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "#     model_json = model.to_json()\n",
    "#     with open(os.path.normpath(os.path.join(Data_path, 'models', \"model_3.json\")), \"w\") as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     # # serialize weights to HDF5\n",
    "#     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3.h5\")))\n",
    "#     print(\"Saved model to disk\")\n",
    "    \n",
    "# print(datetime.datetime.now())\n",
    "# %matplotlib inline\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.normpath(os.path.join(Data_path,'images',ckpt_folder+'.png')))\n",
    "# # model_json = model.to_json()\n",
    "# # with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "# #     json_file.write(model_json)\n",
    "# # model.save_weights(\"model_10h_dataset.h5\")\n",
    "# # print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.2521896, 7.2731686, 7.255978 , 7.2539644, 7.252147 , 7.2675285,\n",
       "        7.252169 , 7.2692385, 7.254997 , 7.254992 , 7.261444 , 7.252759 ,\n",
       "        7.2521086, 7.2590594, 7.2663407, 7.252896 , 7.2520504, 7.2732315,\n",
       "        7.2643743, 7.255691 , 7.26014  , 7.263299 , 7.252203 , 7.2615795,\n",
       "        7.253949 , 7.260395 , 7.2520556, 7.2524915, 7.267142 , 7.2523346,\n",
       "        7.2649045, 7.25217  , 7.2553096, 7.2619376, 7.266017 , 7.2528844,\n",
       "        7.2769527, 7.2522364, 7.253238 , 7.269687 , 7.2550793, 7.25774  ,\n",
       "        7.2523484, 7.253027 , 7.258409 , 7.257243 , 7.2587447, 7.255395 ,\n",
       "        7.270318 , 7.2594857, 7.2520995, 7.268672 , 7.2676725, 7.2521133,\n",
       "        7.2692537, 7.254949 , 7.258311 , 7.25226  , 7.2743955, 7.2525625,\n",
       "        7.265415 , 7.2554445, 7.2715683, 7.2532167, 7.255907 , 7.2529187,\n",
       "        7.275555 , 7.255739 , 7.2581916, 7.255965 , 7.2595234, 7.252056 ,\n",
       "        7.252051 , 7.253619 , 7.26754  , 7.269498 , 7.2536983, 7.260301 ,\n",
       "        7.2581143, 7.2526345, 7.256154 , 7.2678127, 7.252589 , 7.2614546,\n",
       "        7.252218 , 7.253478 , 7.271346 , 7.253293 , 7.2521124, 7.2531404,\n",
       "        7.259767 , 7.2645173, 7.2742376, 7.2520576, 7.2520714, 7.272984 ,\n",
       "        7.252198 , 7.252055 , 7.2667065, 7.253719 , 7.268162 , 7.2674828,\n",
       "        7.2583146, 7.2712846, 7.2560296, 7.252076 , 7.252314 , 7.2609315,\n",
       "        7.2585673, 7.2528486, 7.253634 , 7.252133 , 7.2607617, 7.253003 ,\n",
       "        7.252334 , 7.2662625, 7.271447 , 7.252072 , 7.2562833, 7.2685647,\n",
       "        7.2520504, 7.266336 , 7.264947 , 7.2543554, 7.252072 , 7.2544217,\n",
       "        7.256545 , 7.268482 , 7.2635674, 7.2563205, 7.2528105, 7.255589 ,\n",
       "        7.26041  , 7.2520666, 7.2715235, 7.252057 , 7.2540517, 7.2668395,\n",
       "        7.2539144, 7.2588305, 7.257835 , 7.2549586, 7.257582 , 7.252217 ,\n",
       "        7.2736793, 7.259725 , 7.254981 , 7.259461 , 7.253592 , 7.2597055,\n",
       "        7.2523165, 7.257288 , 7.2557635, 7.265576 , 7.2685003, 7.266192 ,\n",
       "        7.266665 , 7.2743177, 7.2767086, 7.2520504, 7.275026 , 7.254363 ,\n",
       "        7.2558703, 7.2619724, 7.253507 , 7.2597227, 7.2653522, 7.2530165,\n",
       "        7.252789 , 7.262097 , 7.2565446, 7.2522407, 7.26217  , 7.2753677,\n",
       "        7.252812 , 7.2565312, 7.2595854, 7.253286 , 7.254668 , 7.252362 ,\n",
       "        7.2548637, 7.272705 , 7.258677 , 7.2596726, 7.2533236, 7.2755303,\n",
       "        7.254209 , 7.2522726, 7.2546716, 7.2564225, 7.2526355, 7.258819 ,\n",
       "        7.253124 , 7.253172 , 7.2525473, 7.2645593, 7.2621937, 7.262765 ,\n",
       "        7.2521486, 7.2531276, 7.2603135, 7.257497 , 7.2525487, 7.2614765,\n",
       "        7.2625904, 7.261951 , 7.272067 , 7.2524343, 7.2565746, 7.257764 ,\n",
       "        7.253814 , 7.2581816, 7.2545834, 7.252254 , 7.2570887, 7.2520514,\n",
       "        7.257524 , 7.2558193, 7.253832 , 7.257047 , 7.2520504, 7.261072 ,\n",
       "        7.2645006, 7.262108 , 7.2530274, 7.2541876, 7.2520537, 7.2689047,\n",
       "        7.2631063, 7.2524   , 7.269896 , 7.2683125, 7.2582006, 7.2654643,\n",
       "        7.253214 , 7.254671 , 7.2570434, 7.2537837, 7.2761555, 7.252207 ,\n",
       "        7.2598166, 7.252289 , 7.2574263, 7.2595205, 7.259029 , 7.273094 ,\n",
       "        7.254025 , 7.253417 , 7.274228 , 7.2546697, 7.269567 , 7.253628 ,\n",
       "        7.2541127, 7.268604 , 7.261878 , 7.255216 , 7.252924 , 7.256534 ,\n",
       "        7.254285 , 7.2633343, 7.2539425, 7.2570267, 7.2628913, 7.2587113,\n",
       "        7.254566 , 7.255065 , 7.263053 , 7.273941 , 7.2657437, 7.2558484,\n",
       "        7.2534914, 7.267507 , 7.261542 , 7.2563877, 7.2552032, 7.254686 ,\n",
       "        7.2564435, 7.2544603, 7.273911 , 7.2685857, 7.2567806, 7.2547784,\n",
       "        7.255632 , 7.256486 , 7.2598033, 7.262865 , 7.258261 , 7.257424 ,\n",
       "        7.252569 , 7.276708 , 7.258167 , 7.2533073, 7.2638125, 7.2520514,\n",
       "        7.2527466, 7.269077 , 7.2527533, 7.252051 , 7.265605 , 7.253013 ,\n",
       "        7.2542543, 7.252692 , 7.2564855, 7.272302 , 7.2607346, 7.2547307,\n",
       "        7.254903 , 7.256449 , 7.2706223, 7.254951 , 7.2520504, 7.256261 ,\n",
       "        7.255953 , 7.254313 , 7.2520714, 7.252883 , 7.252141 , 7.252061 ,\n",
       "        7.2567706, 7.26653  , 7.256842 , 7.253655 , 7.2607155, 7.2520704,\n",
       "        7.253716 , 7.263011 , 7.252057 , 7.269516 , 7.2668695, 7.2524304,\n",
       "        7.2536564, 7.2632747, 7.2549167, 7.2521386, 7.2579823, 7.2520967,\n",
       "        7.258679 , 7.258236 , 7.2523594, 7.2529664, 7.252534 , 7.25283  ,\n",
       "        7.252051 , 7.2530556, 7.252722 , 7.2520504, 7.2603726, 7.2672615,\n",
       "        7.2599974, 7.2744036, 7.252055 , 7.257719 , 7.2562637, 7.2632203,\n",
       "        7.2640195, 7.258484 , 7.253491 , 7.265666 , 7.269303 , 7.2648034,\n",
       "        7.2606554, 7.266212 , 7.2659707, 7.2594175, 7.260198 , 7.253155 ,\n",
       "        7.2522836, 7.2530375, 7.2640257, 7.2524247, 7.2540417, 7.271794 ,\n",
       "        7.2646036, 7.260942 , 7.252639 , 7.2674117, 7.2584753, 7.2692685,\n",
       "        7.265189 , 7.2523518, 7.2521257, 7.2676473, 7.252393 , 7.2628202,\n",
       "        7.272604 , 7.273045 , 7.264231 , 7.2552114, 7.254432 , 7.2583833,\n",
       "        7.2552643, 7.265928 , 7.256842 , 7.2523956, 7.2579365, 7.2684917,\n",
       "        7.2767286, 7.2525496, 7.254542 , 7.262385 , 7.252453 , 7.2717047,\n",
       "        7.2531385, 7.2529736, 7.253724 , 7.25647  , 7.2602715, 7.25372  ,\n",
       "        7.270166 , 7.266654 , 7.269193 , 7.254107 , 7.2522693, 7.2525506,\n",
       "        7.2634177, 7.25367  , 7.252065 , 7.252145 , 7.2633634, 7.256109 ,\n",
       "        7.2551265, 7.255318 , 7.2531137, 7.2726674, 7.257573 , 7.264926 ,\n",
       "        7.2556887, 7.2658854, 7.2629037, 7.2645187, 7.2549925, 7.2549586,\n",
       "        7.2541404, 7.2576957, 7.2520995, 7.2653117, 7.252062 , 7.2520504,\n",
       "        7.2734857, 7.26498  , 7.2520537, 7.255654 , 7.26149  , 7.269543 ,\n",
       "        7.2622557, 7.252249 , 7.2542367, 7.253221 , 7.2646527, 7.273146 ,\n",
       "        7.2525306, 7.2520533, 7.266033 , 7.256259 , 7.253481 , 7.2529225,\n",
       "        7.2652974, 7.2580824, 7.274396 , 7.252205 , 7.252175 , 7.260967 ,\n",
       "        7.256107 , 7.267331 , 7.252058 , 7.269784 , 7.2525134, 7.2568383,\n",
       "        7.2520504, 7.2620134, 7.255208 , 7.2525983, 7.269194 , 7.255586 ,\n",
       "        7.263018 , 7.2525396, 7.252407 , 7.257029 , 7.254657 , 7.2545147,\n",
       "        7.2558575, 7.2544646, 7.256948 , 7.2548337, 7.2611065, 7.2570014,\n",
       "        7.257564 , 7.2535152, 7.2604604, 7.252183 , 7.2641706, 7.257382 ,\n",
       "        7.2651696, 7.255878 , 7.261457 , 7.2757096, 7.2541075, 7.2599106,\n",
       "        7.2576265, 7.275568 , 7.2700005, 7.2588506, 7.274821 , 7.2532783,\n",
       "        7.2554126, 7.265869 , 7.2587667, 7.264739 , 7.252339 , 7.267448 ,\n",
       "        7.2524304, 7.2551355]], dtype=float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'ftr_refrmd_10h_norm'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr = h5f['ftr_refrmd_10h_norm'][0:126]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = np.matmul(ftr,layer1[0])+layer1[2]\n",
    "# hidden2 = np.matmul(hidden1,layer2[0])+layer2[2]\n",
    "# hidden3 = np.matmul(hidden2,layer3[0])+layer3[2]\n",
    "hidden1_b = np.matmul(hidden1,layer1[0].T)+layer1[1]\n",
    "# hidden2_b = np.matmul(hidden3_b,layer2[0].T)+layer2[1]\n",
    "# hidden1_b = np.matmul(hidden2_b,layer1[0].T)+layer1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9299576 , -0.7363741 , -0.01299872, ..., -0.17403848,\n",
       "        -0.15316603,  1.1178471 ],\n",
       "       [ 0.387127  ,  0.17811282, -1.0233028 , ...,  0.76897204,\n",
       "         0.31726673, -1.6071396 ],\n",
       "       [ 0.9451304 ,  0.1530272 , -1.1989824 , ...,  0.76721895,\n",
       "         1.0839846 , -0.23943205],\n",
       "       ...,\n",
       "       [ 1.1624964 ,  0.6948519 ,  0.46188653, ...,  1.315947  ,\n",
       "        -0.6755554 ,  1.4293516 ],\n",
       "       [ 1.7025452 ,  0.59827197,  1.2019066 , ..., -1.1621488 ,\n",
       "        -0.25516018, -0.43353298],\n",
       "       [-0.14567445, -1.1976821 ,  0.3586812 , ...,  0.46719837,\n",
       "        -2.026935  ,  0.66572046]], dtype=float32)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3411713 ,  1.3411713 ,  1.3411713 , ..., -0.67082363,\n",
       "        -0.6855072 , -1.206295  ],\n",
       "       [ 5.9091597 ,  5.521963  ,  4.2596936 , ..., -0.26952586,\n",
       "        -0.01232617,  0.28907207],\n",
       "       [ 5.593701  ,  5.6730733 ,  4.610169  , ..., -0.4117906 ,\n",
       "        -1.0919688 , -1.0797864 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.70710677, -0.70710677, -0.70710677, ...,  1.4142135 ,\n",
       "         1.4142135 ,  1.4142135 ]], dtype=float32)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  791.22516 ,  1403.8185  ,   344.96936 , ...,   165.61496 ,\n",
       "          312.95496 ,   -46.464916],\n",
       "       [ 2987.522   ,  2818.261   ,  1701.1521  , ..., -1097.625   ,\n",
       "         -941.82965 ,   696.58417 ],\n",
       "       [ 3090.814   ,  2840.0752  ,  2953.833   , ..., -1645.8604  ,\n",
       "         -518.17456 ,  -926.0135  ],\n",
       "       ...,\n",
       "       [   34.79614 ,   121.58834 ,    73.61811 , ...,   115.97396 ,\n",
       "          -94.66215 ,   115.987236],\n",
       "       [   34.79614 ,   121.58834 ,    73.61811 , ...,   115.97396 ,\n",
       "          -94.66215 ,   115.987236],\n",
       "       [ -827.8012  , -1122.1414  ,   -92.172356, ...,   437.81903 ,\n",
       "          628.75653 ,  1888.1415  ]], dtype=float32)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 257), (?,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
