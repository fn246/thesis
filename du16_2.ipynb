{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "i was here\n",
      "initialized\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_6' with dtype string and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-29-46cc06871996>:113) ]]\n\nCaused by op 'Placeholder_6', defined at:\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-29-46cc06871996>\", line 113, in <module>\n    filenames = tf.placeholder(tf.string, shape=[None])\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6833, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_6' with dtype string and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-29-46cc06871996>:113) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_6' with dtype string and shape [?]\n\t [[{{node Placeholder_6}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-46cc06871996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mval_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;31m#     history = model.fit( iterator, steps_per_epoch=steps,epochs=epochs_num, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;31m#     a=sess.run(next_elem[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2198\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_iterator_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2199\u001b[0m           \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2200\u001b[1;33m         \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2202\u001b[0m     \u001b[1;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_6' with dtype string and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-29-46cc06871996>:113) ]]\n\nCaused by op 'Placeholder_6', defined at:\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-29-46cc06871996>\", line 113, in <module>\n    filenames = tf.placeholder(tf.string, shape=[None])\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6833, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_6' with dtype string and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-29-46cc06871996>:113) ]]\n"
     ]
    }
   ],
   "source": [
    "#import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#import sounddevice as sd\n",
    "# import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "input_tr_name = 'ftr_refrmd_10h'\n",
    "input_val_name = 'ftr_refrmd_test'\n",
    "target_tr_name = 'single_dataset_log_16'\n",
    "target_val_name = 'clean_data'\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'tfrecord_files_10h'\n",
    "input_tr_path = os.path.normpath(os.path.join(Data_path,input_tr_name))\n",
    "input_val_path = os.path.normpath(os.path.join(Data_path,input_val_name))\n",
    "target_tr_path = os.path.normpath(os.path.join(Data_path,target_tr_name))\n",
    "target_val_path = os.path.normpath(os.path.join(Data_path,target_val_name))\n",
    "\n",
    "file = h5py.File(target_val_path+'.hdf5','r')\n",
    "val_y = file[target_val_name][0:]\n",
    "file.close()\n",
    "file = h5py.File(input_val_path+'.hdf5','r')\n",
    "val_x = file[input_val_name][0:]\n",
    "file.close()\n",
    "\n",
    "hh = h5py.File(target_tr_path+'.hdf5', 'r')\n",
    "d=hh[target_tr_name]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "\n",
    "file = h5py.File(target_val_path+'.hdf5','r')\n",
    "val_y = file[target_val_name]\n",
    "val_len=val_y.shape\n",
    "file.close()\n",
    "\n",
    "w=3\n",
    "# print(len_data)\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=200, win_length=500, window='hann')\n",
    "    return recon\n",
    "#######################\n",
    "I=0\n",
    "global batch_size\n",
    "batch_size=128\n",
    "# epochs_num=50\n",
    "global datalen\n",
    "datalen=len_data[0]\n",
    "\n",
    "file.close()\n",
    "h = [1024,512]\n",
    "seed = 7\n",
    "from tensorflow.keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(act)\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1]))\n",
    "model.add(act)\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(len_data[1]))\n",
    "#############################################\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    print(\"i was here\")\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "# orig_path = os.getcwd()\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "trainfilenames = []\n",
    "for i in sorted_names:\n",
    "    trainfilenames.append(tfrecord_path)\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(128)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "next_elem = iterator.get_next()\n",
    "# Initialize `iterator` with training data.\n",
    "with tf.Session() as sess:\n",
    "    a=sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "    print(\"initialized\")\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    epochs_num=50\n",
    "    steps = len_data[0] // batch_size\n",
    "    val_steps = val_len[0] // batch_size\n",
    "    history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num, verbose=1,validation_data=(val_x,val_y),validation_steps=val_steps)\n",
    "#     history = model.fit( iterator, steps_per_epoch=steps,epochs=epochs_num, verbose=1)\n",
    "#     a=sess.run(next_elem[0])\n",
    "#     print(a.shape)\n",
    "#############################################\n",
    "# I=0\n",
    "# def da():\n",
    "#     load_size = batch_size\n",
    "#     data_len=datalen\n",
    "#     global I\n",
    "#     h5f1 = h5py.File('ftr_refrmd_10h.hdf5','r')\n",
    "#     h5f2 = h5py.File('single_dataset_log_16.hdf5','r')\n",
    "#     indx = np.arange(0, data_len, 1)\n",
    "#     indx = shuffle(indx, random_state=1)\n",
    "#     print(indx)\n",
    "#     while True:             #this line is just because keras needs infinite generators\n",
    "#         for I in indx: \n",
    "#             Data1 = h5f1['ftr_refrmd_10h'][I]\n",
    "#             Data2 = h5f2['single_dataset_log_16'][I]\n",
    "#             yield (np.reshape(Data1,(1,len(Data1))), np.reshape(Data2,(1,len(Data2))))\n",
    "            \n",
    "# ds = tf.data.Dataset.from_generator(\n",
    "#     da, (tf.float32, tf.float32), ((1,257*w), (1,257)))\n",
    "\n",
    "#############################################\n",
    "# is_training=True\n",
    "# h5f1 = h5py.File('ftr_refrmd_10h.hdf5','r')\n",
    "# h5f2 = h5py.File('single_dataset_log_16.hdf5','r')\n",
    "# def get_record(i):\n",
    "#     # load the ith record using standard python, return numpy arrays\n",
    "#     Data1 = h5f1['ftr_refrmd_10h'][i]\n",
    "#     Data2 = h5f2['single_dataset_log_16'][i]\n",
    "#     return (Data1, Data2)\n",
    "\n",
    "# def tf_map_fn(index):\n",
    "#     features, labels = tf.py_func(\n",
    "#         get_record, (index,), (tf.float32, tf.float32), stateful=False)\n",
    "#     features.set_shape((w*len_data[1],))\n",
    "#     labels.set_shape((len_data[1]))\n",
    "# #     do data augmentation here\n",
    "#     return features, labels\n",
    "# # global epochs_num\n",
    "# epochs_num=30\n",
    "# epoch_size = epochs_num\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(get_record)\n",
    "# if is_training:\n",
    "#     dataset = dataset.repeat().shuffle(epoch_size)\n",
    "# dataset = dataset.map(tf_map_fn)\n",
    "# dataset = dataset.batch(batch_size)\n",
    "# # prefetch data to CPU while GPU processes previous batch\n",
    "# dataset = dataset.prefetch(1)\n",
    "##############################################\n",
    "# h5f1val = h5py.File('ftr_refrmd_test.hdf5','r')\n",
    "# h5f2val = h5py.File('clean_data.hdf5','r')\n",
    "# def get_record(i):\n",
    "#     # load the ith record using standard python, return numpy arrays\n",
    "#     Data1 = h5f1val['ftr_refrmd_test'][i]\n",
    "#     Data2 = h5f2val['clean_data'][i]\n",
    "#     return (Data1, Data2)\n",
    "\n",
    "# def tf_map_fn(index):\n",
    "#     features, labels = tf.py_func(\n",
    "#         get_record, (index,), (tf.float32, tf.float32), stateful=False)\n",
    "#     features.set_shape((w*len_data[1],))\n",
    "#     labels.set_shape((len_data[1]))\n",
    "# #     do data augmentation here\n",
    "#     return features, labels\n",
    "# # global epochs_num\n",
    "# epochs_num=30\n",
    "# epoch_size = epochs_num\n",
    "# dataset_val = tf.data.Dataset.from_tensor_slices((tf.range(epoch_size,)))\n",
    "# if is_training:\n",
    "#     dataset_val = dataset_val.repeat().shuffle(epoch_size)\n",
    "# dataset_val = dataset_val.map(tf_map_fn)\n",
    "# dataset_val = dataset_val.batch(batch_size)\n",
    "# # prefetch data to CPU while GPU processes previous batch\n",
    "# dataset_val = dataset_val.prefetch(1)\n",
    "############################****************************#######################\n",
    "# ds=tf.data.Dataset.from_generator(\n",
    "#         da, (tf.float32, tf.float32))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# epochs_num=50\n",
    "# steps = len_data[0] // batch_size\n",
    "# val_steps = val_len[0] // batch_size\n",
    "# # history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num, verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "# history = model.fit( iterator, steps_per_epoch=steps,epochs=epochs_num, verbose=1)\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "model_json = model.to_json()\n",
    "with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_10h_dataset.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff\\\\tfrecord_files_10h'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(Data_path,'tfrecord_files_10h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 257), (?,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
