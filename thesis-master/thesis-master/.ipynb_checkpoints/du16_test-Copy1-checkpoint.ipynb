{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from pystoi.stoi import stoi\n",
    "import mir_eval #https://github.com/craffel/mir_eval\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "# import keras\n",
    "from scipy.io import wavfile\n",
    "from pypesq import pesq #https://github.com/vBaiCai/python-pesq\n",
    "import vqmetrics\n",
    "from natsort import natsorted\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon\n",
    "def reconstruct2(wave,angle):\n",
    "#     recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = wave*np.cos(angle)+wave*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon\n",
    "def change_order(first):\n",
    "    sec=np.copy(first)\n",
    "    sec=np.copy(first)\n",
    "    sec[0] = first[5]\n",
    "    sec[1] = first[4]\n",
    "    sec[2] = first[3]\n",
    "    sec[5] = first[2]\n",
    "    sec[4] = first[1]\n",
    "    sec[3] = first[0]\n",
    "    return sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = load_model(os.path.normpath(os.path.join(Data_path,'partly_trained.h5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from keras.models import load_model\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# w=3\n",
    "# h = [1024,512]\n",
    "# len_data = (684108, 257)\n",
    "# model = Sequential()\n",
    "# act=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(act)\n",
    "# act=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Dense(h[1]))\n",
    "# model.add(act)\n",
    "# act=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Dense(len_data[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.normpath(os.path.join(Data_path,'checkpoints', 'normal','cp-0050.ckpt'))\n",
    "# model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# # json_file = open(os.path.normpath(os.path.join(Data_path,'models','model_3h_dataset.json')), 'r')\n",
    "# model_path = os.path.normpath(os.path.join(Data_path,'models','model_3h_dataset.json'))\n",
    "# # loaded_model_json = json_file.read()\n",
    "# # json_file.close()\n",
    "# # loaded_model = model_from_json(loaded_model_json)\n",
    "# loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "json_file = open(os.path.normpath(os.path.join(Data_path,'models','model_3h_dataset.json')),'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(os.path.normpath(os.path.join(Data_path,'models','model_3h_dataset.h5')))\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Streaming restore not supported from name-based checkpoints. File a feature request if this limitation bothers you.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-b0c08cdeeb78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# load weights into new model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'checkpoints'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cp-0007.ckpt.index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loaded model from disk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name)\u001b[0m\n\u001b[0;32m   1526\u001b[0m         \u001b[1;31m# Restore existing variables (if any) immediately, and set up a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m         \u001b[1;31m# streaming restore for any variables created in the future.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1528\u001b[1;33m         \u001b[0mcheckpointable_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreaming_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1529\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\util.py\u001b[0m in \u001b[0;36mstreaming_restore\u001b[1;34m(status, session)\u001b[0m\n\u001b[0;32m    881\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNameBasedSaverStatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     raise NotImplementedError(\n\u001b[1;32m--> 883\u001b[1;33m         \u001b[1;34m\"Streaming restore not supported from name-based checkpoints. File a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m         \"feature request if this limitation bothers you.\")\n\u001b[0;32m    885\u001b[0m   \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_restore_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Streaming restore not supported from name-based checkpoints. File a feature request if this limitation bothers you."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "json_file = open(os.path.normpath(os.path.join(Data_path,'models','model_3h_dataset_3.json')),'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(os.path.normpath(os.path.join(Data_path,'checkpoints','3','cp-0007.ckpt.index')))\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(os.path.normpath(os.path.join(Data_path,'checkpoints','3','cp-0007.ckpt.meta')))\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0\n",
      "0\n",
      "0\n",
      "blah\n",
      "3\n",
      "3\n",
      "3\n",
      "blah\n",
      "6\n",
      "6\n",
      "6\n",
      "blah\n",
      "m3\n",
      "m3\n",
      "m3\n",
      "blah\n",
      "m6\n",
      "m6\n",
      "m6\n",
      "blah\n",
      "m9\n",
      "m9\n",
      "m9\n",
      "blah\n"
     ]
    }
   ],
   "source": [
    "stoi_eval=[]\n",
    "sdr=[]\n",
    "stoi_mixed=[]\n",
    "sdr_mixed=[]\n",
    "mean_sdr=[]\n",
    "mean_sdr_mixed=[]\n",
    "mean_stoi=[]\n",
    "mean_stoi_mixed=[]\n",
    "# orig_path=os.getcwd()\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "w = 3\n",
    "parent = 'results'\n",
    "foldername = 'results_3h'\n",
    "input_test = 'ftr_refrmd_test'\n",
    "image = 'images'\n",
    "inputs = os.listdir(os.path.join(Data_path,input_test))\n",
    "inputs = natsorted(inputs)\n",
    "mixed = os.listdir(os.path.join(Data_path,'test_log2'))\n",
    "mixed = natsorted(mixed)\n",
    "clean1 = os.listdir(os.path.join(Data_path,'test_log','clean'))\n",
    "clean1 = natsorted(clean1)\n",
    "clean = clean1.copy()\n",
    "j=0\n",
    "print(len(clean))\n",
    "for i in range(len(inputs)-1):\n",
    "    clean=np.concatenate((clean,clean1),axis=0)\n",
    "phase = os.listdir(os.path.join(Data_path,'test_phase2'))\n",
    "phase = natsorted(phase)\n",
    "if not os.path.exists(os.path.join(Data_path,parent,foldername)):\n",
    "    os.makedirs(os.path.join(Data_path,parent,foldername))\n",
    "if not os.path.exists(os.path.join(Data_path,parent,foldername,'pesq')):\n",
    "#     print('creating pesq')\n",
    "    os.makedirs(os.path.join(Data_path,parent,foldername,'pesq'))\n",
    "if not os.path.exists(os.path.join(Data_path,parent,foldername,'sdr')):\n",
    "    os.makedirs(os.path.join(Data_path,parent,foldername,'sdr'))\n",
    "if not os.path.exists(os.path.join(Data_path,parent,foldername,'stoi')):\n",
    "    os.makedirs(os.path.join(Data_path,parent,foldername,'stoi'))\n",
    "for filename1,filename2,filename4 in zip(inputs,phase,mixed):\n",
    "    print(filename1)\n",
    "    print(filename2)\n",
    "    print(filename4)\n",
    "    a = os.listdir(os.path.join(Data_path,input_test,filename1))\n",
    "    b = os.listdir(os.path.join(Data_path,'test_phase2',filename2))\n",
    "    print('blah')\n",
    "    c = os.listdir(os.path.join(Data_path,'test_log2',filename4))\n",
    "    stoi_eval=[]\n",
    "    pesq_eval=[]\n",
    "    sdr=[]\n",
    "    stoi_mixed=[]\n",
    "    sdr_mixed=[]\n",
    "    if not os.path.exists(os.path.join(Data_path,parent,foldername,filename1)):\n",
    "        os.mkdir(os.path.join(Data_path,parent,foldername,filename1))\n",
    "    for filename11,filename22,filename33,filename44 in zip(a,b,clean,c):\n",
    "        X_log=np.loadtxt(os.path.join(Data_path,input_test,filename1,filename11),delimiter=',')\n",
    "        X_phase=np.loadtxt(os.path.join(Data_path,'test_phase2',filename2,filename22),delimiter=',')\n",
    "        target = np.loadtxt(os.path.join(Data_path,'test_log','clean',filename33),delimiter=',')\n",
    "        framed_data=librosa.core.stft(target, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "        abslt=np.absolute(framed_data)**2\n",
    "        dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "        mixed_files=np.loadtxt(os.path.join(Data_path,'test_log2',filename4,filename44),delimiter=',')\n",
    "        prediction = loaded_model.predict(X_log)\n",
    "        prediction2 = np.sqrt(np.power(10,prediction))*3\n",
    "        recon_out = reconstruct2(prediction2, X_phase)\n",
    "        recon_mixed = reconstruct(mixed_files, X_phase)\n",
    "        recon_clean = reconstruct(dft_signal.T, X_phase)\n",
    "        wavfile.write(os.path.join(Data_path,parent,foldername,filename1)+'\\\\'+filename11.replace('.txt','')+'.wav',16000,recon_out)\n",
    "#         librosa.output.write_wav(os.path.join(Data_path,parent,foldername,filename1)+'\\\\'+filename11.replace('.txt','')+'.wav', recon_out, 16000, norm=False)\n",
    "        target = target[0:len(recon_out)]\n",
    "        #target = np.reshape(target,(1,len(recon_out)))\n",
    "        #recon_out=np.reshape(recon_out,(1,len(recon_out)))\n",
    "#         pesq_eval.append(get_pesq(target, recon_out,16000))\n",
    "        sdr.append(mir_eval.separation.bss_eval_sources(target, recon_out, compute_permutation=False)[0][0])\n",
    "        stoi_eval.append(stoi(target, recon_out, 16000, extended=False))\n",
    "        sdr_mixed.append(mir_eval.separation.bss_eval_sources(target, recon_mixed, compute_permutation=False)[0][0])\n",
    "        stoi_mixed.append(stoi(target, recon_mixed, 16000, extended=False))\n",
    "    mean_stoi.append(np.mean(stoi_eval))\n",
    "    mean_stoi_mixed.append(np.mean(stoi_mixed))\n",
    "    mean_sdr.append(np.mean(sdr))\n",
    "    mean_sdr_mixed.append(np.mean(sdr_mixed))\n",
    "mean_sdr2 = change_order(mean_sdr)\n",
    "mean_stoi2 = change_order(mean_stoi)\n",
    "mean_sdr_mixed2 = change_order(mean_sdr_mixed)\n",
    "mean_stoi_mixed2 = change_order(mean_stoi_mixed)\n",
    "np.savetxt(os.path.join(Data_path,parent,foldername,'sdr')+'\\\\'+'predictedsdr.txt',mean_sdr2, fmt='%1.4f')\n",
    "np.savetxt(os.path.join(Data_path,parent,foldername,'sdr')+'\\\\'+'mixedsdr.txt',mean_sdr_mixed2, fmt='%1.4f')\n",
    "np.savetxt(os.path.join(Data_path,parent,foldername,'stoi')+'\\\\'+'predictedstoi.txt',mean_stoi2, fmt='%1.4f')\n",
    "np.savetxt(os.path.join(Data_path,parent,foldername,'stoi')+'\\\\'+'mixedstoi.txt',mean_stoi_mixed2, fmt='%1.4f')\n",
    "if not os.path.exists(os.path.join(Data_path, parent, image, foldername)):\n",
    "    os.makedirs(os.path.join(Data_path, parent, image, foldername))\n",
    "plt.plot([-9,-6,-3,0,3,6], mean_sdr2, 'r-o', [-9,-6,-3,0,3,6], mean_sdr_mixed2, 'b-o')\n",
    "plt.gca().legend(('pred','mix'))\n",
    "x=[-9,-6,-3,0,3,6]\n",
    "\n",
    "plt.show\n",
    "plt.savefig(os.path.normpath(os.path.join(Data_path, parent, image, foldername, 'sdr.jpg')))\n",
    "plt.close()\n",
    "plt.plot([-9,-6,-3,0,3,6], mean_stoi2, 'r-o', [-9,-6,-3,0,3,6], mean_stoi_mixed2, 'b-o')\n",
    "plt.gca().legend(('pred','mix'))\n",
    "\n",
    "plt.show\n",
    "plt.savefig(os.path.normpath(os.path.join(Data_path, parent, image, foldername, 'stoi2.jpg')))\n",
    "plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\studies\\\\university\\\\thesis\\\\speech_separation_codes\\\\du16\\\\donesomestuff\\\\results\\\\images\\\\results_3h\\\\stoi.jpg'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.normpath(os.path.join(Data_path, parent, image, foldername, 'stoi.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_3h'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldername"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6693284360807059,\n",
       " 1.8824042421698857,\n",
       " 2.8409742174070693,\n",
       " -0.7172409709172369,\n",
       " -2.3017126997260933,\n",
       " -4.025477839106158]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pesq(reference, degraded, sample_rate=None, program='pesq'):\n",
    "    \"\"\" Return PESQ quality estimation (two values: PESQ MOS and MOS LQO) based\n",
    "    on reference and degraded speech samples comparison.\n",
    "    Sample rate must be 8000 or 16000 (or can be defined reading reference file\n",
    "    header).\n",
    "    PESQ utility must be installed.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(reference) or not os.path.isfile(degraded):\n",
    "        raise ValueError('reference or degraded file does not exist')\n",
    "    if not sample_rate:\n",
    "        import wave\n",
    "        w = wave.open(reference, 'r')\n",
    "        sample_rate = w.getframerate()\n",
    "        w.close()\n",
    "    if sample_rate not in (8000, 16000):\n",
    "        raise ValueError('sample rate must be 8000 or 16000')\n",
    "    import subprocess\n",
    "    args = [ program, '+%d' % sample_rate, reference, degraded  ]\n",
    "    pipe = subprocess.Popen(args, stdout=subprocess.PIPE)\n",
    "    out, _ = pipe.communicate()\n",
    "    last_line = out.split('\\n')[-2]\n",
    "    if not last_line.startswith('P.862 Prediction'):\n",
    "        raise ValueError(last_line)\n",
    "    return tuple(map(float, last_line.split()[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile('down_mixed501_1_16bit.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(target,(1,target.shape[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sdr_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_stoi_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "sd.play(recon_out,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.get_weights()[7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdr_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_data=librosa.core.stft(target, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "abslt=np.absolute(framed_data)**2\n",
    "dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "reconstruct(dft_signal.T,X_phase).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "# recon = reconstruct(X_log[:,0:257], X_phase)\n",
    "samplerate=16000\n",
    "sd.play(recon_clean, samplerate)\n",
    "#sd.play(target.T, samplerate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_mixed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path=os.getcwd()\n",
    "w = 7\n",
    "inputs = os.listdir(os.path.join(orig_path,'ftr_refrmd_test'))\n",
    "inputs = natsorted(inputs)\n",
    "clean1 = os.listdir(os.path.join(orig_path,'test_log','clean'))\n",
    "clean1 = natsorted(clean1)\n",
    "clean = clean1.copy()\n",
    "print(len(clean))\n",
    "for i in range(len(inputs)-1):\n",
    "    clean=np.concatenate((clean,clean1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log=np.loadtxt('test_log\\\\'+'3'+'\\\\'+'t2_bwbn2s_m2_brif5p'+'.txt', delimiter=',')\n",
    "X_phase=np.loadtxt('test_phase\\\\'+'3'+'\\\\'+'t2_bwbn2s_m2_brif5p'+'.txt', delimiter=',')\n",
    "recon = reconstruct(X_log, X_phase)\n",
    "sdr=mir_eval.separation.bss_eval_sources(recon, recon, compute_permutation=True)[0][0]\n",
    "sir=mir_eval.separation.bss_eval_sources(recon, recon, compute_permutation=True)[1][0]\n",
    "sar=mir_eval.separation.bss_eval_sources(recon, recon, compute_permutation=True)[2][0]\n",
    "stoi_ev=stoi(recon, recon, 16000, extended=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq_ev=vqmetrics.pesq('t2_bwigzs_m2_sbit3p.wav', 't2_bwigzs_m2_sbit3p.wav',16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq_ev=pesq(recon, recon, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "process = subprocess.Popen(command, stdout=tempFile, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
