{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\studies\\\\university\\\\thesis\\\\speech_separation_codes\\\\du16\\\\donesomestuff\\\\10hdata\\\\mixed_log_10h_nozeroinsert\\\\mean_mixed_log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-4895d1d793d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mean_mixed_log.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtot_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'std_mixed_log.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtot_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;31m############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m         \u001b[1;31m# datasource doesn't support creating a new file ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[0mown_fh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\studies\\\\university\\\\thesis\\\\speech_separation_codes\\\\du16\\\\donesomestuff\\\\10hdata\\\\mixed_log_10h_nozeroinsert\\\\mean_mixed_log.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import csv\n",
    "import math as m\n",
    "from natsort import natsorted\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "import soundfile as sf\n",
    "#in this section the features for mixed voices are extracted.\n",
    "# orig_path=os.getcwd()\n",
    "i = 1\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_10h'))\n",
    "# f.close()\n",
    "# a = os.listdir(os.path.join(mixed_folder))\n",
    "# a = natsorted(a)\n",
    "# mixed_logname = 'mixed_log_10h_norm'\n",
    "# mixed_phasename = 'mixed_phase_10h_norm'\n",
    "# mixed_logpath = os.path.normpath(os.path.join(write_path,mixed_logname))\n",
    "# mixed_phasepath = os.path.normpath(os.path.join(write_path,mixed_phasename))\n",
    "\n",
    "# f = h5py.File(mixed_logpath+'.hdf5', 'w')\n",
    "# d = f.create_dataset(mixed_logname, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "# # ff.close()\n",
    "# ff = h5py.File(mixed_phasepath+'.hdf5', 'w')\n",
    "# dd = ff.create_dataset(mixed_phasename, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "# #################################\n",
    "# w=3\n",
    "# # #################################\n",
    "# for filename in a:\n",
    "#     wav_data, sr = librosa.load(os.path.join(mixed_folder,filename), sr=16000) \n",
    "#     framed_data=librosa.core.stft(wav_data, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "#     print(framed_data.shape)\n",
    "#     abslt=np.absolute(framed_data)**2\n",
    "#     dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "#     for i in range(m.floor(w/2)):\n",
    "#         dft_signal=np.insert(dft_signal,0,0,axis = 1)\n",
    "#     data_phase=np.angle(framed_data)\n",
    "#     for i in range(m.floor(w/2)):\n",
    "#         data_phase=np.insert(data_phase,0,0,axis = 1)\n",
    "#     d.resize(d.shape[0]+dft_signal.shape[1], axis=0)   \n",
    "#     d[-1*dft_signal.shape[1]:] = dft_signal.T\n",
    "#     dd.resize(dd.shape[0]+data_phase.shape[1], axis=0)   \n",
    "#     dd[-1*data_phase.shape[1]:] = data_phase.T\n",
    "#     i=i+1\n",
    "# f.close()\n",
    "# ff.close()\n",
    "###################################################\n",
    "# a = os.listdir(os.path.join(mixed_folder))\n",
    "# a = natsorted(a)\n",
    "# mixed_logname = 'mixed_log_10h_nozeroinsert'\n",
    "# mixed_phasename = 'mixed_phase_10h_nozeroinsert'\n",
    "# mixed_logpath = os.path.normpath(os.path.join(write_path,mixed_logname))\n",
    "# mixed_phasepath = os.path.normpath(os.path.join(write_path,mixed_phasename))\n",
    "\n",
    "# f = h5py.File(mixed_logpath+'.hdf5', 'w')\n",
    "# d = f.create_dataset(mixed_logname, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "# # ff.close()\n",
    "# ff = h5py.File(mixed_phasepath+'.hdf5', 'w')\n",
    "# dd = ff.create_dataset(mixed_phasename, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "# # #################################\n",
    "# for filename in a:\n",
    "#     wav_data, sr = librosa.load(os.path.join(mixed_folder,filename), sr=16000) \n",
    "#     framed_data=librosa.core.stft(wav_data, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "# #     print(framed_data.shape)\n",
    "#     abslt=np.absolute(framed_data)**2\n",
    "#     dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "# #     for i in range(m.floor(w/2)):\n",
    "# #         dft_signal=np.insert(dft_signal,0,0,axis = 1)\n",
    "#     data_phase=np.angle(framed_data)\n",
    "# #     for i in range(m.floor(w/2)):\n",
    "# #         data_phase=np.insert(data_phase,0,0,axis = 1)\n",
    "#     d.resize(d.shape[0]+dft_signal.shape[1], axis=0)   \n",
    "#     d[-1*dft_signal.shape[1]:] = dft_signal.T\n",
    "#     dd.resize(dd.shape[0]+data_phase.shape[1], axis=0)   \n",
    "#     dd[-1*data_phase.shape[1]:] = data_phase.T\n",
    "#     i=i+1\n",
    "# f.close()\n",
    "# ff.close()\n",
    "# print('finished')\n",
    "###################################################\n",
    "# import h5py\n",
    "# import math as m\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import librosa\n",
    "# from natsort import natsorted\n",
    "# orig_path=os.getcwd()\n",
    "# wav=[]\n",
    "# clean_raw = 'clean_10h'\n",
    "# l=os.listdir(os.path.normpath(os.path.join(write_path,clean_raw)))\n",
    "# l=natsorted(l)\n",
    "# filename1 = l[1]\n",
    "# i=0\n",
    "# ###################################################\n",
    "# w=3\n",
    "# ###################################################\n",
    "# num_of_snrs=6\n",
    "# num_of_speakers=6\n",
    "# # files = os.listdir(os.path.normpath(os.path.join(Data_path,clean_raw,filename1)))\n",
    "# files = os.listdir(os.path.normpath(os.path.join(write_path,clean_raw)))\n",
    "# files=natsorted(files)\n",
    "# # f.close()\n",
    "# # ff.close()\n",
    "# clean_log = 'single_dataset_log_10h_norm'\n",
    "# clean_phase = 'single_dataset_phase_10h_norm'\n",
    "# clean_log_path = os.path.normpath(os.path.join(write_path,clean_log))\n",
    "# clean_phase_path = os.path.normpath(os.path.join(write_path,clean_phase))\n",
    "# f = h5py.File(clean_log_path+'.hdf5', 'w')\n",
    "# d = f.create_dataset(clean_log, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "# ff = h5py.File(clean_phase_path+'.hdf5', 'w')\n",
    "# dd = ff.create_dataset(clean_phase, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "# #in this section the features for clean voices are extracted.\n",
    "# for voice in files:\n",
    "#     wav_data, sr = librosa.load(os.path.join(write_path,clean_raw,voice), sr=16000) \n",
    "#     framed_data=librosa.core.stft(wav_data, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "#     print(framed_data.shape)\n",
    "#     abslt=np.absolute(framed_data)**2\n",
    "#     dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "#     data_phase=np.angle(framed_data)\n",
    "# #     for i in range(m.floor(w/2)):\n",
    "# #         dft_signal=np.insert(dft_signal,0,0,axis = 1)\n",
    "# #     for i in range(num_of_snrs*num_of_speakers):\n",
    "#     d.resize(d.shape[0]+dft_signal.shape[1], axis=0)   \n",
    "#     d[-1*dft_signal.shape[1]:] = dft_signal.T\n",
    "#     dd.resize(dd.shape[0]+data_phase.shape[1], axis=0)   \n",
    "#     dd[-1*data_phase.shape[1]:] = data_phase.T\n",
    "# ##############################################\n",
    "# ##############################################\n",
    "# import math as m\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "# ##############################################\n",
    "# w = 3\n",
    "# ###############################################\n",
    "# mixed_logname='mixed_log_10h'\n",
    "# mixed_phasename='mixed_phase_10h'\n",
    "# mixed_log_path = os.path.normpath(os.path.join(write_path,mixed_logname))\n",
    "# clean_phase_path = os.path.normpath(os.path.join(write_path,mixed_phasename))\n",
    "# import h5py\n",
    "# h5f = h5py.File(mixed_log_path+'.hdf5','r')\n",
    "# ftr = h5f[mixed_logname][0:]\n",
    "# # h5f.close()\n",
    "# ftr_refrmd=[]\n",
    "# ftr_refrmd_norm=[]\n",
    "# print('0')\n",
    "# for i in range(m.floor(w/2)):\n",
    "#     p=-m.floor(w/2)\n",
    "#     temp = []\n",
    "#     for j in range(w):\n",
    "#         if np.all(ftr[i]==0):\n",
    "#             break\n",
    "#         temp.extend(ftr[i+p+j])\n",
    "#     if np.any(temp!=[]):\n",
    "#         ftr_refrmd.append(temp)\n",
    "# # f.close()\n",
    "# print('1')\n",
    "# refrmd_file='ftr_refrmd_10h_norm'\n",
    "# refrmd_file_path = os.path.normpath(os.path.join(write_path,refrmd_file))\n",
    "# f = h5py.File(refrmd_file_path+'.hdf5', 'w')\n",
    "# d = f.create_dataset(refrmd_file, (0,257*w),maxshape=(None,257*w), dtype='f', chunks=True)\n",
    "# print('2')\n",
    "# for i in range(m.floor(w/2),ftr.shape[0]-m.floor(w/2)-1):\n",
    "#     k=-m.floor(w/2)\n",
    "#     temp = []\n",
    "#     for j in range(w):\n",
    "#         if np.all(ftr[i]==0):\n",
    "#             break\n",
    "#         temp.extend(ftr[i+k])\n",
    "#         k=k+1\n",
    "#     if np.any(temp!=[]):\n",
    "#         ftr_refrmd.append(temp)\n",
    "#     if len(ftr_refrmd)>50000 or i==ftr.shape[0]-m.floor(w/2)-2:\n",
    "#         print(len(ftr_refrmd))\n",
    "# #         for ftr_num in range(len(ftr_refrmd)):\n",
    "#         ftr_refrmd_norm = preprocessing.scale(ftr_refrmd, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "# #             print(ftr_refrmd_norm.shape)\n",
    "#         d.resize(d.shape[0]+len(ftr_refrmd_norm), axis=0)   \n",
    "#         d[-1*len(ftr_refrmd_norm):] = ftr_refrmd_norm\n",
    "#         print(len(ftr_refrmd_norm))\n",
    "#         ftr_refrmd_norm=[]\n",
    "#         ftr_refrmd=[]\n",
    "# #     if i%10000==0:\n",
    "# #         print(i)\n",
    "# for i in range(ftr.shape[0]-m.floor(w/2)-1,ftr.shape[0]):\n",
    "#     p=-m.floor(w/2)\n",
    "#     temp = []\n",
    "#     for j in range(w):\n",
    "#         if np.all(ftr[i]==0):\n",
    "#             break\n",
    "#         if j>m.floor(w/2)+(ftr.shape[0]-i)-1:\n",
    "#             temp.extend(np.zeros(ftr[ftr.shape[0]-1].shape))\n",
    "#         else:\n",
    "#             temp.extend(ftr[i+p+j])\n",
    "#     if np.any(temp!=[]):\n",
    "#         ftr_refrmd.append(temp)\n",
    "# # for ftr_num in range(len(ftr_refrmd)):\n",
    "# ftr_refrmd_norm = preprocessing.scale(ftr_refrmd, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "# d.resize(d.shape[0]+len(ftr_refrmd_norm), axis=0)   \n",
    "# d[-1*len(ftr_refrmd_norm):] = ftr_refrmd_norm\n",
    "# ftr_refrmd=[]\n",
    "# ftr_refrmd_norm=[]\n",
    "# f.close()\n",
    "# h5f.close()\n",
    "# ##############################################\n",
    "# ##############################################\n",
    "import h5py\n",
    "import numpy as np\n",
    "file='mixed_log_10h_nozeroinsert'\n",
    "file_path = os.path.normpath(os.path.join(write_path,file))\n",
    "h5f = h5py.File(file_path+'.hdf5', 'r')\n",
    "d=h5f[file]\n",
    "tot_len=d.shape[0]\n",
    "dim = d.shape[1]\n",
    "lst=0\n",
    "tot_num=(tot_len//10000)*10000\n",
    "for i in range(0,tot_num,10000):\n",
    "    print(i)\n",
    "    mixed = h5f[file][i:i+10000]\n",
    "    lst=lst+np.sum(mixed,axis=0)\n",
    "\n",
    "mixed = h5f[file][tot_num:]\n",
    "lst=lst+np.sum(mixed,axis=0)\n",
    "# print(mixed.shape)\n",
    "tot_mean=lst/tot_len\n",
    "lst=0\n",
    "h5f = h5py.File(file_path+'.hdf5','r')\n",
    "for i in range(0,tot_num,10000):\n",
    "    mixed = h5f[file][i:i+10000]\n",
    "    lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "del mixed  \n",
    "mixed = h5f[file][tot_num:]\n",
    "lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "lst=lst/tot_len\n",
    "tot_std=np.sqrt(lst)\n",
    "h5f.close()\n",
    "normalized_file = 'mixed_log_10h_norm2'\n",
    "normalized_file_path =  os.path.normpath(os.path.join(write_path,normalized_file))\n",
    "f = h5py.File(normalized_file_path+'.hdf5', 'w')\n",
    "dd = f.create_dataset(normalized_file, (0,dim),maxshape=(None,dim), dtype='f', chunks=True)\n",
    "h5f = h5py.File(file_path+'.hdf5','r')\n",
    "for i in range(0,tot_num,10000):\n",
    "    print(i)\n",
    "    mixed = h5f[file][i:i+10000]\n",
    "    scaled=(mixed-tot_mean)/tot_std\n",
    "    dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "    dd[-1*scaled.shape[0]:] = scaled\n",
    "del mixed    \n",
    "mixed = h5f[file][tot_num:]\n",
    "# print(mixed.shape)\n",
    "scaled=(mixed-tot_mean)/tot_std\n",
    "dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "dd[-1*scaled.shape[0]:] = scaled\n",
    "\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'mean_mixed_log.txt')),tot_mean)\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'std_mixed_log.txt')),tot_std)\n",
    "############################################\n",
    "############################################\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# refrmd_file='ftr_refrmd_10h'\n",
    "# refrmd_file_path = os.path.normpath(os.path.join(write_path,refrmd_file))\n",
    "# h5f = h5py.File(refrmd_file_path+'.hdf5', 'r')\n",
    "# d=h5f[refrmd_file]\n",
    "# tot_len=d.shape[0]\n",
    "# dim = d.shape[1]\n",
    "# lst=0\n",
    "\n",
    "# for i in range(0,tot_num,10000):\n",
    "#     print(i)\n",
    "#     mixed = h5f[refrmd_file][i:i+10000]\n",
    "#     lst=lst+np.sum(mixed,axis=0)\n",
    "# tot_num=(tot_len//10000)*10000\n",
    "# mixed = h5f[refrmd_file][tot_num:]\n",
    "# lst=lst+np.sum(mixed,axis=0)\n",
    "# print(mixed.shape)\n",
    "# tot_mean=lst/tot_len\n",
    "# lst=0\n",
    "# h5f = h5py.File(refrmd_file_path+'.hdf5','r')\n",
    "# for i in range(0,tot_num,10000):\n",
    "#     mixed = h5f[refrmd_file][i:i+10000]\n",
    "#     lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "# del mixed  \n",
    "# mixed = h5f[refrmd_file][tot_num:]\n",
    "# lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "# lst=lst/tot_len\n",
    "# tot_std=np.sqrt(lst)\n",
    "# h5f.close()\n",
    "# normalized_file = 'ftr_refrmd_10h_norm2'\n",
    "# normalized_file_path =  os.path.normpath(os.path.join(write_path,normalized_file))\n",
    "# f = h5py.File(normalized_file_path+'.hdf5', 'w')\n",
    "# dd = f.create_dataset(normalized_file, (0,dim),maxshape=(None,dim), dtype='f', chunks=True)\n",
    "# h5f = h5py.File(refrmd_file_path+'.hdf5','r')\n",
    "# for i in range(0,tot_num,10000):\n",
    "#     print(i)\n",
    "#     mixed = h5f[refrmd_file][i:i+10000]\n",
    "#     scaled=(mixed-tot_mean)/tot_std\n",
    "#     dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "#     dd[-1*scaled.shape[0]:] = scaled\n",
    "# del mixed    \n",
    "# mixed = h5f[refrmd_file][tot_num:]\n",
    "# print(mixed.shape)\n",
    "# scaled=(mixed-tot_mean)/tot_std\n",
    "# dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "# dd[-1*scaled.shape[0]:] = scaled\n",
    "\n",
    "# np.savetxt(os.path.normpath(os.path.join(write_path,refrmd_file,'mean.txt')),tot_mean)\n",
    "# np.savetxt(os.path.normpath(os.path.join(write_path,refrmd_file,'std.txt')),tot_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.normpath(os.path.join(write_path,'mean.txt')),tot_mean)\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'std.txt')),tot_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_data, sr = librosa.load(os.path.join(mixed_folder,filename), 16000) \n",
    "import sounddevice as sd\n",
    "sd.play(wav_data, 16000)\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.normpath(os.path.join(write_path,'mean_mixed_log.txt')),tot_mean)\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'std_mixed_log.txt')),tot_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() missing 1 required positional argument: 'samplerate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-e2d9be91b222>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrecon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdft_signal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_phase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\\\studies\\\\university\\\\thesis\\\\speech_separation_codes\\\\du16\\\\donesomestuff\\\\trash\\\\1.wav'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: write() missing 1 required positional argument: 'samplerate'"
     ]
    }
   ],
   "source": [
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon\n",
    "recon = reconstruct(dft_signal.T, data_phase.T)\n",
    "sd.play(recon, 16000)\n",
    "sf.write('D:\\\\studies\\\\university\\\\thesis\\\\speech_separation_codes\\\\du16\\\\donesomestuff\\\\trash\\\\1.wav',recon,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 126)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_phase.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tot_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7c50b6fa2c0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtot_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_len\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmixed2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnormalized_file\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmixed2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tot_len' is not defined"
     ]
    }
   ],
   "source": [
    "tot_num=(tot_len//10000)*10000\n",
    "mixed2 = h5f[normalized_file]\n",
    "mixed2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[10,2,100,50,46,365],[20,35,10,35,211,5]]\n",
    "at = preprocessing.scale(a, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "at\n",
    "np.std(at[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "sd.play(wav_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum0.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum0.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum1.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum1.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum2.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum2.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum3.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum3.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum4.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum4.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum5.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum5.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum6.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum6.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum7.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum7.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum8.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum8.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum9.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum9.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum10.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum10.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum11.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum11.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum12.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum12.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum13.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum13.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum14.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum14.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum15.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum15.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum16.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum16.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum17.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum17.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum18.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum18.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum19.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum19.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum20.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum20.tfrecords done!\n",
      "Serializing 97278 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum21.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_10h_norm2\\filenum21.tfrecords done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "__author__ = \"Sangwoong Yoon\"\n",
    "\n",
    "def np_to_tfrecords(X, Y, file_path_prefix, verbose=True):\n",
    "    \"\"\"\n",
    "    Converts a Numpy array (or two Numpy arrays) into a tfrecord file.\n",
    "    For supervised learning, feed training inputs to X and training labels to Y.\n",
    "    For unsupervised learning, only feed training inputs to X, and feed None to Y.\n",
    "    The length of the first dimensions of X and Y should be the number of samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray of rank 2\n",
    "        Numpy array for training inputs. Its dtype should be float32, float64, or int64.\n",
    "        If X has a higher rank, it should be rshape before fed to this function.\n",
    "    Y : numpy.ndarray of rank 2 or None\n",
    "        Numpy array for training labels. Its dtype should be float32, float64, or int64.\n",
    "        None if there is no label array.\n",
    "    file_path_prefix : str\n",
    "        The path and name of the resulting tfrecord file to be generated, without '.tfrecords'\n",
    "    verbose : bool\n",
    "        If true, progress is reported.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input type is not float (64 or 32) or int.\n",
    "    \n",
    "    \"\"\"\n",
    "    def _dtype_feature(ndarray):\n",
    "        \"\"\"match appropriate tf.train.Feature class with dtype of ndarray. \"\"\"\n",
    "        assert isinstance(ndarray, np.ndarray)\n",
    "        dtype_ = ndarray.dtype\n",
    "        if dtype_ == np.float64 or dtype_ == np.float32:\n",
    "            return lambda array: tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "        elif dtype_ == np.int64:\n",
    "            return lambda array: tf.train.Feature(int64_list=tf.train.Int64List(value=array))\n",
    "        else:  \n",
    "            raise ValueError(\"The input should be numpy ndarray. \\\n",
    "                               Instaed got {}\".format(ndarray.dtype))\n",
    "            \n",
    "    assert isinstance(X, np.ndarray)\n",
    "    assert len(X.shape) == 2  # If X has a higher rank, \n",
    "                               # it should be rshape before fed to this function.\n",
    "    assert isinstance(Y, np.ndarray) or Y is None\n",
    "    \n",
    "    # load appropriate tf.train.Feature class depending on dtype\n",
    "    dtype_feature_x = _dtype_feature(X)\n",
    "    if Y is not None:\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        assert len(Y.shape) == 2\n",
    "        dtype_feature_y = _dtype_feature(Y)            \n",
    "    \n",
    "    # Generate tfrecord writer\n",
    "    result_tf_file = file_path_prefix + '.tfrecords'\n",
    "    writer = tf.python_io.TFRecordWriter(result_tf_file)\n",
    "    if verbose:\n",
    "        print(\"Serializing {:d} examples into {}\".format(X.shape[0], result_tf_file))\n",
    "        \n",
    "    # iterate over each sample,\n",
    "    # and serialize it as ProtoBuf.\n",
    "    for idx in range(X.shape[0]):\n",
    "        x = X[idx]\n",
    "        if Y is not None:\n",
    "            y = Y[idx]\n",
    "        \n",
    "        d_feature = {}\n",
    "        d_feature['X'] = dtype_feature_x(x)\n",
    "        if Y is not None:\n",
    "            d_feature['Y'] = dtype_feature_y(y)\n",
    "            \n",
    "        features = tf.train.Features(feature=d_feature)\n",
    "        example = tf.train.Example(features=features)\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Writing {} done!\".format(result_tf_file))\n",
    "\n",
    "        \n",
    "#################################    \n",
    "##      Test and Use Cases     ##\n",
    "#################################\n",
    "import h5py\n",
    "load_size = 100000\n",
    "k = 0\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "input_name = 'mixed_log_10h_norm2'\n",
    "input_path = os.path.normpath(os.path.join(write_path,input_name))\n",
    "target_name = 'single_dataset_log_10h'\n",
    "target_path = os.path.normpath(os.path.join(write_path,target_name))\n",
    "h5f1 = h5py.File(input_path+'.hdf5','r')\n",
    "hh = h5py.File(target_path+'.hdf5', 'r')\n",
    "d=hh[target_name]\n",
    "data_len = d.shape[0]\n",
    "index = np.arange(0, data_len, load_size)\n",
    "for I in index:\n",
    "    if data_len-I<load_size:\n",
    "        xx = h5f1[input_name][I:]\n",
    "        yy = hh[target_name][I:]\n",
    "    else:\n",
    "        xx = h5f1[input_name][I:I+load_size]\n",
    "        yy = hh[target_name][I:I+load_size]\n",
    "    np_to_tfrecords(xx, yy, os.path.normpath(os.path.join(Data_path,'tfrecord_files','mixed_10h_norm2','filenum'+str(k))), verbose=True)\n",
    "    k = k+1\n",
    "# 1-2. Check if the data is stored correctly\n",
    "# open the saved file and check the first entries\n",
    "# for serialized_example in tf.python_io.tf_record_iterator('test.tfrecords'):\n",
    "#     example = tf.train.Example()\n",
    "#     example.ParseFromString(serialized_example)\n",
    "#     x_1 = np.array(example.features.feature['X'].float_list.value)\n",
    "#     y_1 = np.array(example.features.feature['Y'].float_list.value)\n",
    "#     break\n",
    "\n",
    "# np_to_tfrecords(X, Y, file_path_prefix, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serialized_example in tf.python_io.tf_record_iterator('tfrecord_files/test3.tfrecords'):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "    x_1 = np.array(example.features.feature['X'].float_list.value)\n",
    "    y_1 = np.array(example.features.feature['Y'].float_list.value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2204556, 771)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "input_name = 'ftr_refrmd_norm2'\n",
    "input_path = os.path.normpath(os.path.join(write_path,input_name))\n",
    "target_name = 'single_dataset_log_10h_norm'\n",
    "target_path = os.path.normpath(os.path.join(write_path,target_name))\n",
    "h5f1 = h5py.File(input_path+'.hdf5','r')\n",
    "hh = h5py.File(target_path+'.hdf5', 'r')\n",
    "d=hh[target_name]\n",
    "data_len = d.shape[0]\n",
    "x = h5f1[input_name]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.266125  ,  0.1387287 , -0.71211874, ..., -1.601058  ,\n",
       "        -1.10846   , -0.46532393],\n",
       "       [ 0.5477439 ,  0.24598686, -1.4461049 , ..., -0.6047218 ,\n",
       "        -0.48930135, -0.44956982],\n",
       "       [-0.06976136, -0.07189065, -0.5253663 , ..., -1.0566396 ,\n",
       "        -1.2633575 , -1.1670849 ],\n",
       "       ...,\n",
       "       [-0.10297483, -0.31201565, -0.76192707, ..., -1.1310498 ,\n",
       "        -1.169964  , -1.3364681 ],\n",
       "       [-0.7514766 , -0.39640373, -0.7915892 , ..., -0.99282587,\n",
       "        -1.0084419 , -1.2564884 ],\n",
       "       [-0.89049244, -0.2596623 , -0.49888033, ...,  1.9272414 ,\n",
       "         1.9187233 ,  1.9395918 ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed = h5f1[normalized_file][2000000:]\n",
    "mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh.close()\n",
    "# f.close()\n",
    "h5f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2197278"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import librosa\n",
    "h5f = h5py.File('mixed_log2.hdf5','r')\n",
    "log = h5f['mixed_log2'][105:209]\n",
    "h5f.close()\n",
    "h5f = h5py.File('mixed_phase2.hdf5','r')\n",
    "phase = h5f['mixed_phase2'][105:209]\n",
    "h5f.close()\n",
    "recon_mixed=reconstruct(log, phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "h5f = h5py.File('single_dataset_log_16.hdf5','r')\n",
    "log_single = h5f['single_dataset_log_16'][0:1000]\n",
    "h5f.close()\n",
    "h5f = h5py.File('single_dataset_phase_16.hdf5','r')\n",
    "phase_single = h5f['single_dataset_phase_16'][0:1000]\n",
    "h5f.close()\n",
    "recon_single=reconstruct(log_single, phase_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "sd.play(recon_single,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_single[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('mixed_log2.hdf5','r')\n",
    "log = h5f['mixed_log2'][0:1000]\n",
    "h5f.close()\n",
    "h5ff = h5py.File('ftr_refrmd.hdf5','r')\n",
    "log3 = h5ff['ftr_refrmd'][0:1000]\n",
    "h5ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('mixed_log2.hdf5','r')\n",
    "mix = h5f['mixed_log2'][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log3[291,257*4:257*5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import h5py\n",
    "w = 7\n",
    "import h5py\n",
    "h5f = h5py.File('mixed_log2.hdf5','r')\n",
    "ftr = h5f['mixed_log2'][0:]\n",
    "h5f.close()\n",
    "ftr_refrmd=[]\n",
    "for i in range(m.floor(w/2)):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        temp.extend(ftr[i+p+j])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "f.close()\n",
    "f = h5py.File('ftr_refrmd.hdf5', 'w')\n",
    "d = f.create_dataset('ftr_refrmd', (0,257*7),maxshape=(None,257*7), dtype='f', chunks=True)\n",
    "for i in range(m.floor(w/2),ftr.shape[0]-m.floor(w/2)-1):\n",
    "    k=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        temp.extend(ftr[i+k])\n",
    "        k=k+1\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "    if len(ftr_refrmd)>100000 or i==ftr.shape[0]-m.floor(w/2)-2:\n",
    "        d.resize(d.shape[0]+len(ftr_refrmd), axis=0)   \n",
    "        d[-1*len(ftr_refrmd):] = ftr_refrmd\n",
    "        ftr_refrmd=[]\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "for i in range(ftr.shape[0]-m.floor(w/2)-1,ftr.shape[0]):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        if j>m.floor(w/2)+(ftr.shape[0]-i)-1:\n",
    "            temp.extend(np.zeros(ftr[ftr.shape[0]-1].shape))\n",
    "        else:\n",
    "            temp.extend(ftr[i+p+j])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "d.resize(d.shape[0]+len(ftr_refrmd), axis=0)   \n",
    "d[-1*len(ftr_refrmd):] = ftr_refrmd\n",
    "ftr_refrmd=[]\n",
    "##############################################\n",
    "##############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5.close()\n",
    "# h5f.close()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "h5 = h5py.File('ftr_refrmd.hdf5', 'r')\n",
    "d=h5['ftr_refrmd']\n",
    "tot_len=d.shape[0]\n",
    "lst=0\n",
    "h5.close()\n",
    "on=0\n",
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "for i in range(0,tot_len,1000):\n",
    "    mixed = h5f['ftr_refrmd'][i:i+1000]\n",
    "    lst=lst+np.sum(mixed,axis=0)\n",
    "    on=on+len(mixed)\n",
    "tot_mean=lst/tot_len\n",
    "h5f.close()\n",
    "lst=0\n",
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "for i in range(0,tot_len,1000):\n",
    "    mixed = h5f['ftr_refrmd'][i:i+1000]\n",
    "    lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "lst=lst/tot_len\n",
    "tot_std=np.sqrt(lst)\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('ftr_scaled.hdf5', 'w')\n",
    "dd = f.create_dataset('ftr_scaled', (0,1799),maxshape=(None,1799), dtype='f', chunks=True)\n",
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "for i in range(0,tot_len,1000):\n",
    "    mixed = h5f['ftr_refrmd'][i:i+1000]\n",
    "    scaled=(mixed-tot_mean)/tot_std\n",
    "    dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "    dd[-1*scaled.shape[0]:] = scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('mean.txt',tot_mean)\n",
    "np.savetxt('std.txt',tot_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "h5 = h5py.File('ftr_refrmd.hdf5', 'r')\n",
    "d=h5['ftr_refrmd']\n",
    "tot_len=d.shape[0]\n",
    "lst=0\n",
    "h5.close()\n",
    "on=0\n",
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "for i in range(0,tot_len,1000):\n",
    "    mixed = h5f['ftr_refrmd'][i:i+1000]\n",
    "    lst=lst+np.sum(mixed,axis=0)\n",
    "    on=on+len(mixed)\n",
    "tot_mean=lst/tot_len\n",
    "h5f.close()\n",
    "lst=0\n",
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "for i in range(0,tot_len,1000):\n",
    "    mixed = h5f['ftr_refrmd'][i:i+1000]\n",
    "    lst=lst+np.sum(mixed - tot_mean**2,axis=0)\n",
    "lst=lst/tot_len\n",
    "tot_std=np.sqrt(lst)\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "f = h5py.File('ftr_scaled.hdf5', 'w')\n",
    "dd = f.create_dataset('ftr_scaled', (0,1799),maxshape=(None,1799), dtype='f', chunks=True)\n",
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "for i in range(0,tot_len,1000):\n",
    "    mixed = h5f['ftr_refrmd'][i:i+1000]\n",
    "    scaled=(mixed-tot_mean)/tot_std\n",
    "    dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "    dd[-1*scaled.shape[0]:] = scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "mixed = h5f['ftr_refrmd'][0]\n",
    "h5f.close()\n",
    "np.mean(mixed-tot_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('mixed_log2.hdf5','r')\n",
    "mixed = h5f['mixed_log2'][0:110]\n",
    "h5f.close()\n",
    "print('loaded')\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "scaler = preprocessing.StandardScaler().fit(mixed.T)\n",
    "from sklearn.externals import joblib\n",
    "scaler_filename = \"standard_scaler\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "#inverse_transform(X, copy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now to load...\n",
    "from sklearn.externals import joblib\n",
    "scaler = joblib.load('standard_scaler')\n",
    "import h5py\n",
    "h5f = h5py.File('mixed_log2.hdf5','r')\n",
    "mixed = h5f['mixed_log2'][0:1000]\n",
    "h5f.close()\n",
    "print('loaded')\n",
    "scaled_mix=scaler.transform(mixed.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('scaled_mix.h5', 'w')\n",
    "h5f.create_dataset('scaled_mix', data=scaled_mix.T)\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import math as m\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "orig_path=os.getcwd()\n",
    "wav=[]\n",
    "l=os.listdir(os.path.join(orig_path,'single_dataset_16'))\n",
    "l.sort(key=int)\n",
    "filename1 = l[1]\n",
    "i=0\n",
    "w=7\n",
    "num_of_snrs=21\n",
    "num_of_speakers=6\n",
    "files = os.listdir(os.path.join(orig_path,'single_dataset_16',filename1))\n",
    "f.close()\n",
    "f = h5py.File('single_dataset_log_16.hdf5', 'w')\n",
    "d = f.create_dataset('single_dataset_log_16', (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "#in this section the features for clean voices are extracted.\n",
    "for voice in files:\n",
    "    #sr, wav_data = wavfile.read(os.path.join(orig_path,'single dataset',filename1,filename2)+\"\\\\\"+voice, )\n",
    "    #sr, wav_data = wavfile.read(os.path.join(orig_path,'mixed2_with24')+\"\\\\\"+filename)\n",
    "    wav_data, sr = librosa.load(os.path.join(orig_path,'single_dataset_16',filename1,voice), sr=16000) \n",
    "    framed_data=librosa.core.stft(wav_data, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "    #frame length = win_length * freq *1000(ms)\n",
    "    abslt=np.absolute(framed_data)**2\n",
    "    dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "#     for i in range(m.floor(w/2)):\n",
    "#         dft_signal=np.insert(dft_signal,0,0,axis = 1)\n",
    "    #data_phase=np.angle(framed_data)\n",
    "    for i in range(num_of_snrs*num_of_speakers):\n",
    "        d.resize(d.shape[0]+dft_signal.shape[1], axis=0)   \n",
    "        d[-1*dft_signal.shape[1]:] = dft_signal.T\n",
    "    #np.savetxt('single_dataset_log_16\\\\abs'+'2'+'_'+voice+'.txt', dft_signal, delimiter=',')\n",
    "    #np.savetxt('single_dataset_phase_features_256\\\\phase'+'1'+'_'+str(i+1)+'.txt', data_phase, delimiter=',')\n",
    "f.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('ftr_refrmd.hdf5','r')\n",
    "single = h5f['ftr_refrmd'][-1]\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('single_dataset_log_16.hdf5','r')\n",
    "single = h5f['single_dataset_log_16'][:]\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaled_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('mixed_log2.hdf5','r')\n",
    "ftr = h5f['mixed_log2'][0:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ftr_refrmd = np.zeros((983134,257*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import h5py\n",
    "w = 7\n",
    "ftr_refrmd=[]\n",
    "for i in range(m.floor(w/2)):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        #if j<m.floor(w/2)-i+1:\n",
    "            #temp.extend(np.zeros(ftr[0].shape))\n",
    "        #else:\n",
    "        #if j>m.floor(w/2)-i+1:\n",
    "        temp.extend(ftr[i+p+j])\n",
    "        #temp.extend(ftr[j-(m.floor(w/2)-i+1)])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "f.close()\n",
    "f = h5py.File('ftr_refrmd.hdf5', 'w')\n",
    "d = f.create_dataset('ftr_refrmd', (0,257*7),maxshape=(None,257*7), dtype='f', chunks=True)\n",
    "for i in range(m.floor(w/2),ftr.shape[0]-m.floor(w/2)-1):\n",
    "    k=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        temp.extend(ftr[i+k])\n",
    "        k=k+1\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "    if len(ftr_refrmd)>100000 or i==ftr.shape[0]-m.floor(w/2)-2:\n",
    "        d.resize(d.shape[0]+len(ftr_refrmd), axis=0)   \n",
    "        d[-1*len(ftr_refrmd):] = ftr_refrmd\n",
    "        ftr_refrmd=[]\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "for i in range(ftr.shape[0]-m.floor(w/2)-1,ftr.shape[0]):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        if j>m.floor(w/2)+(ftr.shape[0]-i)-1:\n",
    "            temp.extend(np.zeros(ftr[ftr.shape[0]-1].shape))\n",
    "        else:\n",
    "            temp.extend(ftr[i+p+j])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "d.resize(d.shape[0]+len(ftr_refrmd), axis=0)   \n",
    "d[-1*len(ftr_refrmd):] = ftr_refrmd\n",
    "ftr_refrmd=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_refrmd[0][-257:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('single_dataset_log_16.hdf5','r')\n",
    "single_dataset = h5f['single_dataset_log_16'][0:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.arange(0,21*6*,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path=os.getcwd()\n",
    "from natsort import natsorted\n",
    "# import h5py\n",
    "# h5f = h5py.File('single_dataset_log_16.hdf5','r')\n",
    "# single_dataset = h5f['single_dataset_log_16'][0:]\n",
    "# h5f.close()\n",
    "target1 = single_dataset[0].T \n",
    "for i in \n",
    "target = single_dataset[0].T\n",
    "#target=np.loadtxt('single_dataset_log_16\\\\'+'abs1_1.txt',delimiter=',').T\n",
    "#target1=np.loadtxt('single_dataset_log_16\\\\'+'abs1_1.txt',delimiter=',').T\n",
    "for i in range(21*6):\n",
    "    target1=np.concatenate((target1, target),axis=0)\n",
    "print(target1.shape)\n",
    "j=1\n",
    "a=os.listdir(os.path.join(orig_path,'single_dataset_log_16'))\n",
    "a=natsorted(a)\n",
    "for filename in a[1:]:\n",
    "    f=np.loadtxt('single_dataset_log_16\\\\'+filename,delimiter=',')\n",
    "    print(f.shape)\n",
    "    for i in range(600):\n",
    "        target1=np.concatenate((target1,f.T),axis=0)\n",
    "    #print(j)\n",
    "    print(target1.shape)\n",
    "    if j==4:\n",
    "        break\n",
    "    j=j+1\n",
    "print(target1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import csv\n",
    "import math as m\n",
    "from natsort import natsorted\n",
    "import h5py\n",
    "#in this section the features for mixed voices are extracted.\n",
    "orig_path=os.getcwd()\n",
    "i = 1\n",
    "a = os.listdir(os.path.join(orig_path,'sec_mixed2_6'))\n",
    "a = natsorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
