{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "# from tqdm import tqdm\n",
    "from progressbar import ProgressBar\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "I=0\n",
    "def _parse_function(example_proto):\n",
    "    print('1')\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    print(\"i was here\")\n",
    "    print('2')\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "def rbm_layer(n_visible, n_hidden, num_epochs, num_cases, lr, lrh, ws, bs, layer_n, len_data, directories):\n",
    "    Data_path = directories[0]\n",
    "    tfrecord_folder_parent = directories[1]\n",
    "    tfrecord_folder = directories[2]\n",
    "    \n",
    "    tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "    sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "    trainfilenames_x = []\n",
    "    for i in sorted_names_x:\n",
    "        trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path_x,i)))\n",
    "#     filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "#     dataset_x = tf.data.TFRecordDataset(filenames_x)\n",
    "    dataset_x = tf.data.TFRecordDataset(trainfilenames_x)\n",
    "    dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "#     dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "    dataset_x = dataset_x.batch(num_cases)\n",
    "#     iterator_x = dataset_x.make_initializable_iterator()\n",
    "    \n",
    "    weightcost  = 0.0002\n",
    "    initialmomentum  = 0.3\n",
    "    finalmomentum    = 0.8\n",
    "#     momentum  = 0.5\n",
    "    numcases = 32\n",
    "    W_adder  = tf.zeros((n_visible,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bh_adder = tf.zeros((1,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bv_adder = tf.zeros((1,n_visible),dtype=tf.dtypes.float32)\n",
    "#     x  = tf.placeholder(tf.float32, [None, n_visible], name=\"x\") #The placeholder variable that holds our data\n",
    "#     h = tf.placeholder(tf.float32, [None, n_hidden], name=\"h\")\n",
    "#     h = tf.zeros((numcases, n_hidden),dtype=tf.dtypes.float32)\n",
    "#     h_empty  = np.zeros_like(h)\n",
    "#     h_empty = np.zeros((, n_hidden),dtype=np.float32)\n",
    "#     m  = tf.Variable(0.5, dtype=np.float32, name='m')\n",
    "    W  = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "    bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "    bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "    def sample(probs):\n",
    "        #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "        return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "    def sample_gauss(probs):\n",
    "        #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "        return tf.floor(probs + tf.random.normal(tf.shape(probs),mean=0.0,stddev=1))\n",
    "#     hk = sample(tf.sigmoid(tf.matmul(x, W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "#     #128*512\n",
    "\n",
    "#     #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "#     posprods  = tf.matmul(tf.transpose(x),hk) #771*512\n",
    "#     poshidacts = tf.reduce_sum(hk) #(512)\n",
    "#     posvisacts = tf.reduce_sum(x,axis=0)#771\n",
    "    \n",
    "#     xk   = tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)#128*711\n",
    "#     neghidprobs   = tf.sigmoid(tf.matmul(xk, W) + bh) #128*512\n",
    "#     negprods  = tf.matmul(tf.transpose(xk),neghidprobs); #771*512##################################\n",
    "#     neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "#     negvisacts = tf.reduce_sum(xk, axis=0)\n",
    "#     print(negvisacts)#771\n",
    "# #     print(type(posprod-negprod))\n",
    "# #     print(type(lr))\n",
    "#     m=0.5\n",
    "#     print('here')\n",
    "#     W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "#     bv_adder = (m * bv_adder)+ ((lr/numcases)*(posvisacts-negvisacts))\n",
    "#     bh_adder = (m * bh_adder)+ ((lr/numcases)*(poshidacts-neghidacts))\n",
    "#     #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "#     updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]\n",
    "\n",
    "    ### Run the graph!\n",
    "    # Now it's time to start a session and run the graph! \n",
    "\n",
    "#     with tf.Session() as sess:\n",
    "        #First, we train the model\n",
    "        #initialize the variables of the model\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         print(m)\n",
    "#         sess.run(iterator_x.initializer, feed_dict={filenames_x: trainfilenames_x})\n",
    "#         print(iterator_x.get_next()[0])\n",
    "        #Run through all of the training data num_epochs times\n",
    "#         for epoch in tqdm(range(num_epochs)):\n",
    "            #Train the RBM on batch_size examples at a time\n",
    "#             for X_batch in da(batch_size, layer_n, ws, bs, len_data, name, data_name):\n",
    "#             Data = sess.run(iterator_x.get_next()[0])\n",
    "#             print('blah')\n",
    "#             print(Data.shape)\n",
    "#             for Data in dataset_x:\n",
    "#                 for j in range(layer_n):\n",
    "#                     Data = np.matmul(Data,ws[j])+bs[j]\n",
    "    #             if epoch>5:\n",
    "    #                 momentum=finalmomentum;\n",
    "    #             else:\n",
    "    #                 momentum=initialmomentum;\n",
    "#                 feed_dict = {x: Data}\n",
    "#                 var1 = sess.run(updt, feed_dict)\n",
    "    count=0\n",
    "    pbar = ProgressBar()\n",
    "    for epoch in pbar(range(num_epochs)):\n",
    "        count+=1\n",
    "        incount=0\n",
    "        for x in dataset_x:\n",
    "            incount+=1\n",
    "            if incount%1000==0:\n",
    "#                 plt.imshow(updt[0].numpy(),cmap='gray')\n",
    "                plt.imshow(neghidprobs.numpy(),cmap='gray')\n",
    "                plt.show()\n",
    "#                 print(updt[0])\n",
    "                print(incount)\n",
    "            if layer_n>1:\n",
    "                for j in range(layer_n-1):\n",
    "                    x = np.matmul(x,ws[j])+bs[j]\n",
    "            poshidprobs = tf.sigmoid(tf.matmul(x[0], W) + bh) #Propagate the visible values to sample the hidden values\n",
    "            poshidstates = sample(poshidprobs)\n",
    "            #128*512\n",
    "            #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "            posprods  = tf.matmul(tf.transpose(x[0]),poshidprobs) #771*512\n",
    "            poshidacts = tf.reduce_sum(poshidprobs, axis=0) #(512)\n",
    "            posvisacts = tf.reduce_sum(x[0],axis=0)#771\n",
    "\n",
    "            negdata   = tf.matmul(poshidstates, tf.transpose(W)) + bv+ tf.random.normal((len(x[0]),len(x[0][0])),mean=0.0,stddev=0.05)#128*711\n",
    "#             negdata   = tf.matmul(poshidstates, tf.transpose(W))#128*711\n",
    "            neghidprobs   = tf.sigmoid(tf.matmul(negdata, W) + bh) #128*512\n",
    "            negprods  = tf.matmul(tf.transpose(negdata),neghidprobs); #771*512##################################\n",
    "            neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "            negvisacts = tf.reduce_sum(negdata, axis=0)\n",
    "#             print(negvisacts)#771\n",
    "        #     print(type(posprod-negprod))\n",
    "        #     print(type(lr))\n",
    "            if epoch>5:\n",
    "                m=finalmomentum\n",
    "            else:\n",
    "                m=initialmomentum\n",
    "            W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "            bv_adder = (m * bv_adder)+ ((lr/numcases)*(posvisacts-negvisacts))\n",
    "            bh_adder = (m * bh_adder)+ ((lr/numcases)*(poshidacts-neghidacts))\n",
    "            #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "            updt = [W.assign_add(W_adder).numpy(), bv.assign_add(bv_adder).numpy(), bh.assign_add(bh_adder).numpy()] \n",
    "            \n",
    "        print(count)\n",
    "    return updt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "2019-07-11 14:30:34.787798\n",
      "initialized\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 2.0241\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.02406, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.01.hdf5\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 2.9724 - val_loss: 2.0241\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 1.8573\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.02406 to 1.85732, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.02.hdf5\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 2.2169 - val_loss: 1.8573\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 6s 8ms/step - loss: 1.7570\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.85732 to 1.75696, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.03.hdf5\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 2.0428 - val_loss: 1.7570\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 1.7171\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.75696 to 1.71706, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.04.hdf5\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.8866 - val_loss: 1.7171\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 1.6379\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.71706 to 1.63787, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.05.hdf5\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 1.8216 - val_loss: 1.6379\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 1.5760\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.63787 to 1.57600, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.06.hdf5\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 1.7600 - val_loss: 1.5760\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 1.5434\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.57600 to 1.54335, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.07.hdf5\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 1.6835 - val_loss: 1.5434\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 1.5030\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.54335 to 1.50300, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.08.hdf5\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 1.6451 - val_loss: 1.5030\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 7s 8ms/step - loss: 1.4825\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.50300 to 1.48247, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.09.hdf5\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 1.5993 - val_loss: 1.4825\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 7s 9ms/step - loss: 1.5047\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.48247\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.5693 - val_loss: 1.5047\n",
      "Epoch 11/50\n",
      " 734/1562 [=============>................] - ETA: 13s - loss: 1.5364"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-88ddaa7b5c72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;31m#     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;31m#     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    852\u001b[0m     elif distributed_training_utils.is_tpu_strategy(\n\u001b[0;32m    853\u001b[0m         self._distribution_strategy):\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1189\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3164\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3166\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3167\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m   3168\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Got two values for keyword '{}'.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munused_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Keyword arguments {} unknown.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    267\u001b[0m           \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_proto_serialized\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m           executor_type=function_call_options.executor_type)\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mpartitioned_call\u001b[1;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[0;32m   1081\u001b[0m       outputs = gen_functional_ops.stateful_partitioned_call(\n\u001b[0;32m   1082\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m           executor_type=executor_type)\n\u001b[0m\u001b[0;32m   1084\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m       outputs = gen_functional_ops.partitioned_call(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_functional_ops.py\u001b[0m in \u001b[0;36mstateful_partitioned_call\u001b[1;34m(args, Tout, f, config, config_proto, executor_type, name)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;34m\"StatefulPartitionedCall\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;34m\"Tout\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \"executor_type\", executor_type)\n\u001b[0m\u001b[0;32m    508\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "import sys\n",
    "from natsort import natsorted\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = os.getcwd()\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'tfrecord_files_norm_small'\n",
    "tfrecord_val_folder = 'validation_norm_small'\n",
    "# ckpt_folder = '5'\n",
    "dirs = [Data_path, tfrecord_folder_parent, tfrecord_folder]\n",
    " \n",
    "# len_data = (684108, 257)\n",
    "# len_data = (2197278, 257)\n",
    "# val_len = (97278,257)\n",
    "len_data = (200000, 257)\n",
    "val_len = (100000,257)\n",
    "w=3\n",
    "h = [512]\n",
    "global batch_size\n",
    "batch_size = 128\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon\n",
    "\n",
    "########################\n",
    "visible = w*len_data[1]\n",
    "hidden = h[0]\n",
    "# visible1 = h[0]\n",
    "# hidden1 = h[1]\n",
    "# visible2 = h[1]\n",
    "# hidden2 = len_data[1]\n",
    "\n",
    "layer1 = rbm_layer(visible, hidden, 10, batch_size, 0.001, 0.001, [np.eye(visible,hidden)], [np.zeros((1,visible))], 1, len_data[0],dirs)\n",
    "# layer2 = rbm_layer(visible1, hidden1, 50, batch_size, 0.01, [np.eye(visible,hidden),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, len_data[0], dirs)\n",
    "# layer3 = rbm_layer(visible2, hidden2, 50, batch_size, 0.01, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, len_data[0], dirs)\n",
    "\n",
    "###############################\n",
    "#######################\n",
    "# I=0\n",
    "\n",
    "# # epochs_num=50\n",
    "# global datalen\n",
    "# datalen=len_data[0]\n",
    "\n",
    "\n",
    "# seed = 7\n",
    "# rate1 = 0.1\n",
    "# rate2 = 0.2\n",
    "# buffersize = 1000\n",
    "# from tensorflow.keras.layers import Activation\n",
    "# # from keras.layers import Activation\n",
    "# np.random.seed(seed)\n",
    "# act1 = layers.LeakyReLU(alpha=0.1)\n",
    "# model = Sequential()\n",
    "# # model.add(layers.Dropout(rate1, noise_shape=None, seed=None))\n",
    "# # ,kernel_regularizer=regularizers.l2(0.001)\n",
    "# # model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "# model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(layer1[0].numpy()), bias_initializer = tf.constant_initializer(layer1[2].numpy())))\n",
    "# # , kernel_initializer= tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])\n",
    "# # tf.constant_initializer(layer1[0])\n",
    "# # tf.constant_initializer(layer1[2])\n",
    "# model.add(BatchNormalization())\n",
    "# # model.add(act1)\n",
    "# model.add(Activation('sigmoid'))\n",
    "# # act2=layers.LeakyReLU(alpha=0.1)\n",
    "# # model.add(layers.Dropout(rate2, noise_shape=None, seed=None))\n",
    "# # model.add(Dense(h[1]))\n",
    "# # model.add(act2)\n",
    "# # act3=layers.LeakyReLU(alpha=0.1)\n",
    "# # # model.add(layers.Dropout(rate, noise_shape=None, seed=None))\n",
    "# # model.add(Dense(h[2]))\n",
    "# # model.add(act3)\n",
    "# # act=layers.LeakyReLU(alpha=0.01)\n",
    "# model.add(Dense(len_data[1]))\n",
    "# #############################################\n",
    "# import os\n",
    "\n",
    "# def _parse_function(example_proto):\n",
    "#     features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "#               \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "#     parsed_features = tf.parse_single_example(example_proto, features)\n",
    "#     return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "# tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "# sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "# trainfilenames = []\n",
    "# for i in sorted_names:\n",
    "#     trainfilenames.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# # filenames = tf.placeholder(tf.string, shape=[None])\n",
    "# # dataset = tf.data.TFRecordDataset(filenames)\n",
    "# dataset = tf.data.TFRecordDataset(trainfilenames)\n",
    "# dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "# dataset = dataset.batch(batch_size)\n",
    "# dataset = dataset.shuffle(buffersize)\n",
    "# # iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "# sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "# trainfilenames_val = []\n",
    "# for i in sorted_names_val:\n",
    "#     trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "# # filenames_val = tf.placeholder(tf.string, shape=[None])\n",
    "# # dataset_val = tf.data.TFRecordDataset(filenames_val)\n",
    "# dataset_val = tf.data.TFRecordDataset(trainfilenames_val)\n",
    "# dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "# dataset_val = dataset_val.batch(128)\n",
    "# # iterator_val = dataset_val.make_initializable_iterator()\n",
    "\n",
    "# epochs_num = 50\n",
    "# steps = len_data[0] // batch_size\n",
    "# val_steps = val_len[0] // batch_size\n",
    "# # You can feed the initializer with the appropriate filenames for the current\n",
    "# # phase of execution, e.g. training vs. validation.\n",
    "# # next_elem = iterator_val.get_next()\n",
    "# # Initialize `iterator` with training data.\n",
    "\n",
    "# if not os.path.exists(os.path.join(Data_path,\"checkpoints\",ckpt_folder)):\n",
    "#     os.makedirs(os.path.join(Data_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "# print(datetime.datetime.now())\n",
    "# # with tf.Session() as sess:\n",
    "# #     sess.run(tf.global_variables_initializer())\n",
    "# #     sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "# #     sess.run(iterator_val.initializer, feed_dict={filenames_val: trainfilenames_val})\n",
    "# print(\"initialized\")\n",
    "# checkpoint_path = os.path.normpath(os.path.join(Data_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "#         # Save weights, every 5-epochs.\n",
    "# #         period=1)\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "# # opt = tf.keras.optimizers.Adamax()\n",
    "# # opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.keras.optimizers.SGD()\n",
    "# model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "# history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num, callbacks = [cp_callback,early_stop], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "# #     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# model_json = model.to_json()\n",
    "# with open(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # # serialize weights to HDF5\n",
    "# model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "# print(\"Saved model to disk\")\n",
    "    \n",
    "# print(datetime.datetime.now())\n",
    "# %matplotlib inline\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.normpath(os.path.join(Data_path,'images',ckpt_folder+'.png')))\n",
    "# # model_json = model.to_json()\n",
    "# # with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "# #     json_file.write(model_json)\n",
    "# # model.save_weights(\"model_10h_dataset.h5\")\n",
    "# # print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test_vislinear','layers10.txt')),layer1[0].numpy())\n",
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test_vislinear','layers11.txt')),layer1[1].numpy())\n",
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test_vislinear','layers12.txt')),layer1[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_log_10h_norm_nozeroinsert'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr = h5f['mixed_log_10h_norm_nozeroinsert'][0:126]\n",
    "h5f.close()\n",
    "\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_phase_10h_norm_nozeroinsert'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr_phase = h5f['mixed_phase_10h_norm_nozeroinsert'][0:126]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = np.matmul(ftr,layer1[0].numpy())+layer1[2].numpy()\n",
    "# hidden2 = np.matmul(hidden1,layer2[0])+layer2[2]\n",
    "# hidden3 = np.matmul(hidden2,layer3[0])+layer3[2]\n",
    "hidden1_b = np.matmul(hidden1,layer1[0].numpy().T)+layer1[1].numpy()\n",
    "# hidden2_b = np.matmul(hidden3_b,layer2[0].T)+layer2[1]\n",
    "# hidden1_b = np.matmul(hidden2_b,layer1[0].T)+layer1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon=reconstruct(ftr,ftr_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "sd.play(recon, 16000)\n",
    "sf.write('D:\\\\studies\\\\university\\\\thesis\\\\speech_separation_codes\\\\du16\\\\donesomestuff\\\\trash\\\\1.wav',recon,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24736588, -0.2190181 , -0.15972915, ..., -0.98896515,\n",
       "        -0.9873588 , -1.473857  ],\n",
       "       [ 0.3102975 ,  0.1418973 , -0.6608744 , ..., -1.1224244 ,\n",
       "        -0.9490217 , -0.5731728 ],\n",
       "       [ 0.14628278,  0.2906629 , -0.39500728, ..., -1.1753767 ,\n",
       "        -1.5427078 , -1.2799661 ],\n",
       "       ...,\n",
       "       [-0.6662225 , -1.8685224 , -0.5691139 , ..., -0.8490831 ,\n",
       "         0.52497184,  0.79261017],\n",
       "       [-0.32349524, -0.74083716, -1.070736  , ..., -0.02861573,\n",
       "         0.01894927,  0.04031413],\n",
       "       [-0.1760415 , -0.17762351, -0.798594  , ..., -0.6981956 ,\n",
       "        -1.1395708 , -1.048378  ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.4185099e+01,  3.0500498e+01,  2.9169716e+01, ...,\n",
       "        -3.2018631e+01, -3.4590439e+01, -3.5520889e+01],\n",
       "       [-3.3846517e+00, -1.5700558e+00, -1.1776546e+00, ...,\n",
       "        -2.5596312e+01, -2.8456320e+01, -2.6890779e+01],\n",
       "       [-2.8286324e+00, -8.5185874e-01, -1.9666791e-02, ...,\n",
       "        -2.5275082e+01, -2.8195982e+01, -2.6387232e+01],\n",
       "       ...,\n",
       "       [ 3.6461449e+01,  4.2905083e+01,  3.4969875e+01, ...,\n",
       "         1.8915762e+01,  1.6422970e+01,  1.3159542e+01],\n",
       "       [ 3.6498436e+01,  4.2762051e+01,  3.4271145e+01, ...,\n",
       "         1.4032127e+01,  1.1587976e+01,  8.9029617e+00],\n",
       "       [ 3.5531693e+01,  4.1363499e+01,  3.3589851e+01, ...,\n",
       "        -1.8697060e+00, -4.1226583e+00, -5.8113427e+00]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 257), (?,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
