{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "# # from tensorflow.python.ops import control_flow_ops\n",
    "# from tqdm import tqdm\n",
    "from progressbar import ProgressBar\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "\n",
    "I=0\n",
    "def _parse_function(example_proto):\n",
    "    print('1')\n",
    "    features = {\"X\": tf.FixedLenFeature((257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    print(\"i was here\")\n",
    "    print('2')\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "def rbm_layer(n_visible, n_hidden, num_epochs, num_cases, lr, lrh, ws, bs, layer_n, len_data, directories):\n",
    "    Data_path = directories[0]\n",
    "    tfrecord_folder_parent = directories[1]\n",
    "    tfrecord_folder = directories[2]\n",
    "    \n",
    "    tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "    sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "    trainfilenames_x = []\n",
    "    for i in sorted_names_x:\n",
    "        trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path_x,i)))\n",
    "#     filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "#     dataset_x = tf.data.TFRecordDataset(filenames_x)\n",
    "    dataset_x = tf.data.TFRecordDataset(trainfilenames_x)\n",
    "    dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "#     dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "    dataset_x = dataset_x.batch(num_cases)\n",
    "#     iterator_x = dataset_x.make_initializable_iterator()\n",
    "    \n",
    "    weightcost  = 0.0002\n",
    "    initialmomentum  = 0.5\n",
    "    finalmomentum    = 0.9\n",
    "    numcases = 128\n",
    "    W_adder  = tf.zeros((n_visible,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bh_adder = tf.zeros((1,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bv_adder = tf.zeros((1,n_visible),dtype=tf.dtypes.float32)\n",
    "    \n",
    "    W  = tf.Variable(tf.random_normal([n_visible, n_hidden], mean=0, stddev=0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "    bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "    bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "    def sample(probs):\n",
    "        #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "#         return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "        return tf.cast(probs > tf.random_uniform(tf.shape(probs), 0, 1),tf.float32)\n",
    "        \n",
    "    count=0\n",
    "    pbar = ProgressBar()\n",
    "    for epoch in pbar(range(num_epochs)):\n",
    "        count+=1\n",
    "        incount=0\n",
    "        errsum = 0\n",
    "        for x in dataset_x:\n",
    "\n",
    "                    \n",
    "            poshidprobs = tf.sigmoid(tf.matmul(x[0], W) + bh) #Propagate the visible values to sample the hidden values\n",
    "            poshidstates = sample(poshidprobs)\n",
    "            #128*512\n",
    "            #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "            posprods  = tf.matmul(tf.transpose(x[0]),poshidprobs) #771*512\n",
    "            poshidacts = tf.reduce_sum(poshidprobs, axis=0) #(512)\n",
    "#             print(poshidprobs)\n",
    "#             print(poshidacts.numpy())\n",
    "            posvisacts = tf.reduce_sum(x[0],axis=0)#771\n",
    "            \n",
    "            negdata   = tf.sigmoid(tf.matmul(poshidstates, tf.transpose(W)) + bv)#128*711\n",
    "            neghidprobs   = tf.sigmoid(tf.matmul(negdata, W) + bh) #128*512\n",
    "            negprods  = tf.matmul(tf.transpose(negdata),neghidprobs); #771*512##################################\n",
    "            neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "            negvisacts = tf.reduce_sum(negdata, axis=0)\n",
    "            \n",
    "            if incount%100==0:\n",
    "                print(incount)\n",
    "            incount+=1\n",
    "            if epoch>5:\n",
    "                m=finalmomentum\n",
    "            else:\n",
    "                m=initialmomentum\n",
    "#             [posprods,negprods,posvisacts,negvisacts,poshidacts,neghidacts] = gibbs_step(x[0])\n",
    "            W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "            bv_adder = (m * bv_adder)+ ((lrh/numcases)*(posvisacts-negvisacts))\n",
    "            bh_adder = (m * bh_adder)+ ((lrh/numcases)*(poshidacts-neghidacts))\n",
    "#             print(posprods)\n",
    "#             print(poshidacts-neghidacts)\n",
    "#             print(bv_adder)\n",
    "            #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "            updt = [W.assign_add(W_adder).numpy(), bv.assign_add(bv_adder).numpy(), bh.assign_add(bh_adder).numpy()] \n",
    "            err= tf.reduce_sum(tf.reduce_sum((x[0]-negdata)**2 ,axis =0),axis=0)\n",
    "            errsum = err + errsum;\n",
    "        print(errsum)\n",
    "#         print(count)\n",
    "    return updt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "1\n",
      "i was here\n",
      "2\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% |#######                                                                 |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25459236.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% |##############                                                          |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25877740.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% |#####################                                                   |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25886400.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% |############################                                            |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25888386.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% |####################################                                    |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25889182.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% |###########################################                             |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25889560.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% |##################################################                      |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25871744.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% |#########################################################               |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25891428.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% |################################################################        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25891570.0, shape=(), dtype=float32)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25891606.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "import sys\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = os.getcwd()\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'mixed_10h_norm2'\n",
    "tfrecord_val_folder = 'validation_norm_small'\n",
    "ckpt_folder = '5'\n",
    "dirs = [Data_path, tfrecord_folder_parent, tfrecord_folder]\n",
    " \n",
    "# len_data = (684108, 257)\n",
    "# len_data = (2197278, 257)\n",
    "# val_len = (97278,257)\n",
    "len_data = (200000, 257)\n",
    "val_len = (100000,257)\n",
    "w=3\n",
    "h = [512]\n",
    "global batch_size\n",
    "batch_size = 128\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon\n",
    "\n",
    "########################\n",
    "# visible = w*len_data[1]\n",
    "visible = len_data[1]\n",
    "hidden = h[0]\n",
    "# visible1 = h[0]\n",
    "# hidden1 = h[1]\n",
    "# visible2 = h[1]\n",
    "# hidden2 = len_data[1]\n",
    "\n",
    "layer1 = rbm_layer(visible, hidden, 10, batch_size, 0.1, 0.1, [np.eye(visible,hidden)], [np.zeros((1,visible))], 1, len_data[0],dirs)\n",
    "# layer2 = rbm_layer(visible1, hidden1, 50, batch_size, 0.01, [np.eye(visible,hidden),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, len_data[0], dirs)\n",
    "# layer3 = rbm_layer(visible2, hidden2, 50, batch_size, 0.01, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, len_data[0], dirs)\n",
    "\n",
    "###############################\n",
    "#######################\n",
    "# I=0\n",
    "\n",
    "# # epochs_num=50\n",
    "# global datalen\n",
    "# datalen=len_data[0]\n",
    "\n",
    "\n",
    "# seed = 7\n",
    "# rate1 = 0.1\n",
    "# rate2 = 0.2\n",
    "# buffersize = 1000\n",
    "# from tensorflow.keras.layers import Activation\n",
    "# # from keras.layers import Activation\n",
    "# np.random.seed(seed)\n",
    "# act1 = layers.LeakyReLU(alpha=0.1)\n",
    "# model = Sequential()\n",
    "# # model.add(layers.Dropout(rate1, noise_shape=None, seed=None))\n",
    "# # ,kernel_regularizer=regularizers.l2(0.001)\n",
    "# # model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "# model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])))\n",
    "# # , kernel_initializer= tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])\n",
    "# # tf.constant_initializer(layer1[0])\n",
    "# # tf.constant_initializer(layer1[2])\n",
    "# model.add(BatchNormalization())\n",
    "# # model.add(act1)\n",
    "# model.add(Activation('sigmoid'))\n",
    "# # act2=layers.LeakyReLU(alpha=0.1)\n",
    "# # model.add(layers.Dropout(rate2, noise_shape=None, seed=None))\n",
    "# # model.add(Dense(h[1]))\n",
    "# # model.add(act2)\n",
    "# # act3=layers.LeakyReLU(alpha=0.1)\n",
    "# # # model.add(layers.Dropout(rate, noise_shape=None, seed=None))\n",
    "# # model.add(Dense(h[2]))\n",
    "# # model.add(act3)\n",
    "# # act=layers.LeakyReLU(alpha=0.01)\n",
    "# model.add(Dense(len_data[1]))\n",
    "# #############################################\n",
    "# import os\n",
    "# from natsort import natsorted\n",
    "\n",
    "# def _parse_function(example_proto):\n",
    "#     features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "#               \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "#     parsed_features = tf.parse_single_example(example_proto, features)\n",
    "#     return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "# tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "# sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "# trainfilenames = []\n",
    "# for i in sorted_names:\n",
    "#     trainfilenames.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# # filenames = tf.placeholder(tf.string, shape=[None])\n",
    "# # dataset = tf.data.TFRecordDataset(filenames)\n",
    "# dataset = tf.data.TFRecordDataset(trainfilenames)\n",
    "# dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "# dataset = dataset.batch(batch_size)\n",
    "# dataset = dataset.shuffle(buffersize)\n",
    "# # iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "# sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "# trainfilenames_val = []\n",
    "# for i in sorted_names_val:\n",
    "#     trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "# # filenames_val = tf.placeholder(tf.string, shape=[None])\n",
    "# # dataset_val = tf.data.TFRecordDataset(filenames_val)\n",
    "# dataset_val = tf.data.TFRecordDataset(trainfilenames_val)\n",
    "# dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "# dataset_val = dataset_val.batch(128)\n",
    "# # iterator_val = dataset_val.make_initializable_iterator()\n",
    "\n",
    "# epochs_num = 50\n",
    "# steps = len_data[0] // batch_size\n",
    "# val_steps = val_len[0] // batch_size\n",
    "# # You can feed the initializer with the appropriate filenames for the current\n",
    "# # phase of execution, e.g. training vs. validation.\n",
    "# # next_elem = iterator_val.get_next()\n",
    "# # Initialize `iterator` with training data.\n",
    "\n",
    "# if not os.path.exists(os.path.join(Data_path,\"checkpoints\",ckpt_folder)):\n",
    "#     os.makedirs(os.path.join(Data_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "# print(datetime.datetime.now())\n",
    "# # with tf.Session() as sess:\n",
    "# #     sess.run(tf.global_variables_initializer())\n",
    "# #     sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "# #     sess.run(iterator_val.initializer, feed_dict={filenames_val: trainfilenames_val})\n",
    "# print(\"initialized\")\n",
    "# checkpoint_path = os.path.normpath(os.path.join(Data_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "#         # Save weights, every 5-epochs.\n",
    "# #         period=1)\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "# # opt = tf.keras.optimizers.Adamax()\n",
    "# # opt = tf.train.AdamOptimizer()\n",
    "# opt = tf.keras.optimizers.SGD()\n",
    "# model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "# history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num, callbacks = [cp_callback,early_stop], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "# #     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# model_json = model.to_json()\n",
    "# with open(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # # serialize weights to HDF5\n",
    "# model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "# print(\"Saved model to disk\")\n",
    "    \n",
    "# print(datetime.datetime.now())\n",
    "# %matplotlib inline\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.normpath(os.path.join(Data_path,'images',ckpt_folder+'.png')))\n",
    "# # model_json = model.to_json()\n",
    "# # with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "# #     json_file.write(model_json)\n",
    "# # model.save_weights(\"model_10h_dataset.h5\")\n",
    "# # print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test','layers10.txt')),layer1[0])\n",
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test','layers11.txt')),layer1[1])\n",
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test','layers12.txt')),layer1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8f4d521f3bdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwrite_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmixed_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mixed_log_10h_norm2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mh5f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmixed_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.hdf5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mftr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mixed_log_10h_norm2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m126\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_log_10h_norm2'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr = h5f['mixed_log_10h_norm2'][0:126]\n",
    "h5f.close()\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_phase_10h_nozeroinsert'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr_phase = h5f['mixed_phase_10h_nozeroinsert'][0:126]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = np.matmul(ftr,layer1[0])+layer1[2]\n",
    "# hidden2 = np.matmul(hidden1,layer2[0])+layer2[2]\n",
    "# hidden3 = np.matmul(hidden2,layer3[0])+layer3[2]\n",
    "hidden1_b = np.matmul(hidden1,layer1[0].T)+layer1[1]\n",
    "# hidden2_b = np.matmul(hidden3_b,layer2[0].T)+layer2[1]\n",
    "# hidden1_b = np.matmul(hidden2_b,layer1[0].T)+layer1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.ones((300,257))-mean)/std)*std+mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.53837216, 1.32554345, 1.46483478, 1.73449735, 1.78078742,\n",
       "       1.79493836, 1.91780831, 1.95251315, 1.86977405, 1.77915561,\n",
       "       1.76519178, 1.86027562, 1.96966473, 1.99097837, 1.95238138,\n",
       "       1.94580237, 1.95707608, 1.92973856, 1.93212023, 1.93764876,\n",
       "       1.89617586, 1.8539489 , 1.8376548 , 1.82487941, 1.77231266,\n",
       "       1.74453302, 1.71202875, 1.68147863, 1.67128448, 1.65948598,\n",
       "       1.66315731, 1.70590532, 1.71562715, 1.68648245, 1.65367047,\n",
       "       1.62804796, 1.63988119, 1.67398255, 1.68546989, 1.67245087,\n",
       "       1.66041666, 1.63824927, 1.64779135, 1.65841155, 1.6545928 ,\n",
       "       1.64101815, 1.66718663, 1.65744804, 1.63700542, 1.61295695,\n",
       "       1.62263098, 1.61948365, 1.64646589, 1.65286744, 1.62544737,\n",
       "       1.62465334, 1.63801187, 1.63111888, 1.61911113, 1.60926431,\n",
       "       1.59281308, 1.61490726, 1.63060496, 1.63046297, 1.5992348 ,\n",
       "       1.57998928, 1.57301462, 1.57245691, 1.59928312, 1.61450441,\n",
       "       1.60943051, 1.6009857 , 1.61223461, 1.64187435, 1.64957907,\n",
       "       1.65647023, 1.66178929, 1.64520189, 1.67069965, 1.6732718 ,\n",
       "       1.67527737, 1.67367009, 1.66962047, 1.65693543, 1.65220092,\n",
       "       1.66174239, 1.6591522 , 1.64953697, 1.64104908, 1.62818496,\n",
       "       1.60880705, 1.58946299, 1.59075677, 1.5971241 , 1.61063514,\n",
       "       1.60625265, 1.5971064 , 1.61235001, 1.62826852, 1.62127674,\n",
       "       1.63706825, 1.61412395, 1.60533708, 1.63465344, 1.65561679,\n",
       "       1.63534617, 1.62789048, 1.61188362, 1.6258544 , 1.62413191,\n",
       "       1.65185702, 1.65412602, 1.63609945, 1.62711338, 1.62396271,\n",
       "       1.63889658, 1.6747625 , 1.64003203, 1.62972365, 1.63534045,\n",
       "       1.63350704, 1.6233942 , 1.62398813, 1.64647106, 1.65807301,\n",
       "       1.67104804, 1.68676062, 1.69658005, 1.71372432, 1.71733409,\n",
       "       1.73525408, 1.74532039, 1.77105616, 1.7810691 , 1.7943395 ,\n",
       "       1.80599974, 1.79857648, 1.80159487, 1.79877693, 1.83284842,\n",
       "       1.82889765, 1.84066756, 1.8582497 , 1.85350738, 1.86603582,\n",
       "       1.89636167, 1.90054216, 1.89365827, 1.89864854, 1.91528798,\n",
       "       1.92526786, 1.92802798, 1.94689846, 1.92809867, 1.9204651 ,\n",
       "       1.90636141, 1.9187921 , 1.94116313, 1.95117111, 1.9655389 ,\n",
       "       1.93947775, 1.9166685 , 1.9116237 , 1.90934965, 1.89830946,\n",
       "       1.900451  , 1.89028507, 1.8864494 , 1.87472422, 1.87868794,\n",
       "       1.90122933, 1.91036903, 1.90690299, 1.88894186, 1.87791868,\n",
       "       1.89224035, 1.89410267, 1.90353017, 1.89768448, 1.89049482,\n",
       "       1.89828165, 1.90801723, 1.90711288, 1.91062814, 1.89845119,\n",
       "       1.89876234, 1.89767508, 1.88893987, 1.87200859, 1.8748869 ,\n",
       "       1.88675834, 1.8821163 , 1.88702863, 1.88138608, 1.87218533,\n",
       "       1.86822509, 1.9012542 , 1.91870943, 1.88870085, 1.84781057,\n",
       "       1.87511697, 1.88826897, 1.88166978, 1.87723756, 1.84889348,\n",
       "       1.83986096, 1.84749325, 1.85135245, 1.85606602, 1.85521255,\n",
       "       1.84505874, 1.84652752, 1.85875606, 1.87563853, 1.84177674,\n",
       "       1.82050661, 1.83551816, 1.84084167, 1.8150925 , 1.80546645,\n",
       "       1.79985322, 1.82384193, 1.81481327, 1.78982388, 1.7942771 ,\n",
       "       1.78653103, 1.77774335, 1.76931774, 1.7819863 , 1.77695725,\n",
       "       1.77213492, 1.75099956, 1.73901159, 1.72705831, 1.7317625 ,\n",
       "       1.72909149, 1.73914994, 1.7437086 , 1.73556124, 1.74293563,\n",
       "       1.72885091, 1.72667924, 1.73070176, 1.71141178, 1.72683238,\n",
       "       1.71035809, 1.71040728, 1.68937217, 1.67234879, 1.684472  ,\n",
       "       1.69828222, 1.6793291 , 1.66685251, 1.67108334, 1.66601278,\n",
       "       1.67841497, 1.78769705])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28766647,  0.11526573, -0.7069241 , ..., -0.55612373,\n",
       "        -0.5525803 , -0.50696164],\n",
       "       [ 0.12223668,  0.26682785, -0.4372746 , ..., -0.99636066,\n",
       "        -1.0043348 , -1.5150769 ],\n",
       "       [ 0.8128672 ,  0.6103882 , -0.9415356 , ..., -1.1297925 ,\n",
       "        -0.9657317 , -0.59864193],\n",
       "       ...,\n",
       "       [-4.825073  , -5.533287  , -4.9773455 , ..., -2.2312639 ,\n",
       "        -2.2086217 , -1.930099  ],\n",
       "       [-4.825073  , -5.533287  , -4.9773455 , ..., -2.2312639 ,\n",
       "        -2.2086217 , -1.930099  ],\n",
       "       [-4.825073  , -5.533287  , -4.9773455 , ..., -2.2312639 ,\n",
       "        -2.2086217 , -1.930099  ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.40254208e+08, 5.10191392e+08, 3.74471776e+08, ...,\n",
       "        1.04809747e+09, 1.04292179e+09, 9.52056960e+08],\n",
       "       [4.53852896e+08, 5.25950912e+08, 3.86038240e+08, ...,\n",
       "        1.08047168e+09, 1.07513408e+09, 9.81463616e+08],\n",
       "       [4.37720800e+08, 5.07256640e+08, 3.72316192e+08, ...,\n",
       "        1.04206726e+09, 1.03691443e+09, 9.46573568e+08],\n",
       "       ...,\n",
       "       [7.66589632e+08, 8.88369344e+08, 6.52044736e+08, ...,\n",
       "        1.82498317e+09, 1.81597146e+09, 1.65776307e+09],\n",
       "       [7.66589632e+08, 8.88369344e+08, 6.52044736e+08, ...,\n",
       "        1.82498317e+09, 1.81597146e+09, 1.65776307e+09],\n",
       "       [7.66589632e+08, 8.88369344e+08, 6.52044736e+08, ...,\n",
       "        1.82498317e+09, 1.81597146e+09, 1.65776307e+09]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:56: RuntimeWarning: overflow encountered in power\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:57: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "mean = np.loadtxt(os.path.normpath(os.path.join(write_path,'mean_mixed_log.txt')))\n",
    "std = np.loadtxt(os.path.normpath(os.path.join(write_path,'std_mixed_log.txt')))\n",
    "hidden1_b = hidden1_b*std+mean\n",
    "recon = reconstruct(hidden1_b, ftr_phase)\n",
    "import sounddevice as sd\n",
    "sd.play(recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04190072e+09, 8.96439668e+08, 8.03519419e+08, ...,\n",
       "        2.90909786e+09, 2.93799079e+09, 3.04264144e+09],\n",
       "       [1.07408323e+09, 9.24130176e+08, 8.28338054e+08, ...,\n",
       "        2.99895567e+09, 3.02873527e+09, 3.13662102e+09],\n",
       "       [1.03590519e+09, 8.91283117e+08, 7.98894094e+08, ...,\n",
       "        2.89236042e+09, 2.92106759e+09, 3.02511729e+09],\n",
       "       ...,\n",
       "       [1.81420252e+09, 1.56092308e+09, 1.39911908e+09, ...,\n",
       "        5.06542070e+09, 5.11573107e+09, 5.29797990e+09],\n",
       "       [1.81420252e+09, 1.56092308e+09, 1.39911908e+09, ...,\n",
       "        5.06542070e+09, 5.11573107e+09, 5.29797990e+09],\n",
       "       [1.81420252e+09, 1.56092308e+09, 1.39911908e+09, ...,\n",
       "        5.06542070e+09, 5.11573107e+09, 5.29797990e+09]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "eval is not supported when eager execution is enabled, is .numpy() what you're looking for?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d6c562f1d99d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlabel_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    960\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m     raise NotImplementedError(\n\u001b[1;32m--> 962\u001b[1;33m         \u001b[1;34m\"eval is not supported when eager execution is enabled, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         \u001b[1;34m\"is .numpy() what you're looking for?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: eval is not supported when eager execution is enabled, is .numpy() what you're looking for?"
     ]
    }
   ],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([True, False])\n",
    "x=tf.cast(x, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
