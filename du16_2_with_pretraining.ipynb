{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "# # from tensorflow.python.ops import control_flow_ops\n",
    "# from tqdm import tqdm\n",
    "from progressbar import ProgressBar\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "I=0\n",
    "def _parse_function(example_proto):\n",
    "    print('1')\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    print(\"i was here\")\n",
    "    print('2')\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "def rbm_layer(n_visible, n_hidden, num_epochs, num_cases, lr, lrh, ws, bs, layer_n, len_data, directories):\n",
    "    Data_path = directories[0]\n",
    "    tfrecord_folder_parent = directories[1]\n",
    "    tfrecord_folder = directories[2]\n",
    "    \n",
    "    tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "    sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "    trainfilenames_x = []\n",
    "    for i in sorted_names_x:\n",
    "        trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path_x,i)))\n",
    "#     filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "#     dataset_x = tf.data.TFRecordDataset(filenames_x)\n",
    "    dataset_x = tf.data.TFRecordDataset(trainfilenames_x)\n",
    "    dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "#     dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "    dataset_x = dataset_x.batch(num_cases)\n",
    "#     iterator_x = dataset_x.make_initializable_iterator()\n",
    "    \n",
    "    weightcost  = 0.0002\n",
    "    initialmomentum  = 0.5\n",
    "    finalmomentum    = 0.9\n",
    "    numcases = 128\n",
    "    W_adder  = tf.zeros((n_visible,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bh_adder = tf.zeros((1,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bv_adder = tf.zeros((1,n_visible),dtype=tf.dtypes.float32)\n",
    "    \n",
    "    W  = tf.Variable(tf.random_normal([n_visible, n_hidden], mean=0, stddev=0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "    bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "    bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "    def sample(probs):\n",
    "        #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "#         return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "        return tf.cast(probs > tf.random_uniform(tf.shape(probs), 0, 1),tf.float32)\n",
    "        \n",
    "    count=0\n",
    "    pbar = ProgressBar()\n",
    "    for epoch in pbar(range(num_epochs)):\n",
    "        count+=1\n",
    "        incount=0\n",
    "        errsum = 0\n",
    "        for x in dataset_x:\n",
    "\n",
    "                    \n",
    "            poshidprobs = tf.sigmoid(tf.matmul(x[0], W) + bh) #Propagate the visible values to sample the hidden values\n",
    "            poshidstates = sample(poshidprobs)\n",
    "            #128*512\n",
    "            #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "            posprods  = tf.matmul(tf.transpose(x[0]),poshidprobs) #771*512\n",
    "            poshidacts = tf.reduce_sum(poshidprobs, axis=0) #(512)\n",
    "#             print(poshidprobs)\n",
    "#             print(poshidacts.numpy())\n",
    "            posvisacts = tf.reduce_sum(x[0],axis=0)#771\n",
    "            \n",
    "            negdata   = tf.sigmoid(tf.matmul(poshidstates, tf.transpose(W)) + bv)#128*711\n",
    "            neghidprobs   = tf.sigmoid(tf.matmul(negdata, W) + bh) #128*512\n",
    "            negprods  = tf.matmul(tf.transpose(negdata),neghidprobs); #771*512##################################\n",
    "            neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "            negvisacts = tf.reduce_sum(negdata, axis=0)\n",
    "            \n",
    "            if incount%100==0:\n",
    "                print(incount)\n",
    "            incount+=1\n",
    "            if epoch>5:\n",
    "                m=finalmomentum\n",
    "            else:\n",
    "                m=initialmomentum\n",
    "#             [posprods,negprods,posvisacts,negvisacts,poshidacts,neghidacts] = gibbs_step(x[0])\n",
    "            W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "            bv_adder = (m * bv_adder)+ ((lrh/numcases)*(posvisacts-negvisacts))\n",
    "            bh_adder = (m * bh_adder)+ ((lrh/numcases)*(poshidacts-neghidacts))\n",
    "#             print(posprods)\n",
    "#             print(poshidacts-neghidacts)\n",
    "#             print(bv_adder)\n",
    "            #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "            updt = [W.assign_add(W_adder).numpy(), bv.assign_add(bv_adder).numpy(), bh.assign_add(bh_adder).numpy()] \n",
    "            err= tf.reduce_sum(tf.reduce_sum((x[0]-negdata)**2 ,axis =0),axis=0)\n",
    "            errsum = err + errsum;\n",
    "        print(errsum)\n",
    "#         print(count)\n",
    "    return updt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "2019-07-11 13:21:18.328357\n",
      "initialized\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3806\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.38055, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.01.hdf5\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 3.2202 - val_loss: 2.3806\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3697\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.38055 to 2.36968, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.02.hdf5\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 2.8382 - val_loss: 2.3697\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 9s 11ms/step - loss: 2.3622\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.36968 to 2.36217, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.03.hdf5\n",
      "1562/1562 [==============================] - 43s 27ms/step - loss: 2.8240 - val_loss: 2.3622\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3667\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.36217\n",
      "1562/1562 [==============================] - 41s 26ms/step - loss: 2.8377 - val_loss: 2.3667\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.36217\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 2.8633 - val_loss: 2.3688\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3657\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.36217\n",
      "1562/1562 [==============================] - 41s 26ms/step - loss: 2.7888 - val_loss: 2.3657\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3721\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.36217\n",
      "1562/1562 [==============================] - 41s 26ms/step - loss: 2.8018 - val_loss: 2.3721\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3620\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.36217 to 2.36199, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.08.hdf5\n",
      "1562/1562 [==============================] - 41s 26ms/step - loss: 2.8275 - val_loss: 2.3620\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3614\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.36199 to 2.36143, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.09.hdf5\n",
      "1562/1562 [==============================] - 42s 27ms/step - loss: 2.7760 - val_loss: 2.3614\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3577\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.36143 to 2.35773, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.10.hdf5\n",
      "1562/1562 [==============================] - 41s 27ms/step - loss: 2.7634 - val_loss: 2.3577\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 8s 10ms/step - loss: 2.3391\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.35773 to 2.33915, saving model to D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\checkpoints\\5\\weights.11.hdf5\n",
      "1562/1562 [==============================] - 41s 26ms/step - loss: 2.7751 - val_loss: 2.3391\n",
      "Epoch 12/50\n",
      " 737/1562 [=============>................] - ETA: 17s - loss: 2.7309"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-984e3abaf262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;31m#     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;31m#     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    852\u001b[0m     elif distributed_training_utils.is_tpu_strategy(\n\u001b[0;32m    853\u001b[0m         self._distribution_strategy):\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m       \u001b[0mbatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[1;34m(output_generator, mode)\u001b[0m\n\u001b[0;32m    256\u001b[0m   \u001b[1;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# Returning `None` will trigger looping to stop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \"\"\"\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m         \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   1864\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "import sys\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = os.getcwd()\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'tfrecord_files_norm_small'\n",
    "tfrecord_val_folder = 'validation_norm_small'\n",
    "ckpt_folder = '5'\n",
    "dirs = [Data_path, tfrecord_folder_parent, tfrecord_folder]\n",
    " \n",
    "# len_data = (684108, 257)\n",
    "# len_data = (2197278, 257)\n",
    "# val_len = (97278,257)\n",
    "len_data = (200000, 257)\n",
    "val_len = (100000,257)\n",
    "w=3\n",
    "h = [512]\n",
    "global batch_size\n",
    "batch_size = 128\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon\n",
    "\n",
    "########################\n",
    "visible = w*len_data[1]\n",
    "hidden = h[0]\n",
    "# visible1 = h[0]\n",
    "# hidden1 = h[1]\n",
    "# visible2 = h[1]\n",
    "# hidden2 = len_data[1]\n",
    "\n",
    "# layer1 = rbm_layer(visible, hidden, 10, batch_size, 0.1, 0.1, [np.eye(visible,hidden)], [np.zeros((1,visible))], 1, len_data[0],dirs)\n",
    "# layer2 = rbm_layer(visible1, hidden1, 50, batch_size, 0.01, [np.eye(visible,hidden),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, len_data[0], dirs)\n",
    "# layer3 = rbm_layer(visible2, hidden2, 50, batch_size, 0.01, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, len_data[0], dirs)\n",
    "\n",
    "###############################\n",
    "#######################\n",
    "I=0\n",
    "\n",
    "# epochs_num=50\n",
    "global datalen\n",
    "datalen=len_data[0]\n",
    "\n",
    "\n",
    "seed = 7\n",
    "rate1 = 0.1\n",
    "rate2 = 0.2\n",
    "buffersize = 1000\n",
    "from tensorflow.keras.layers import Activation\n",
    "# from keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "# model.add(layers.Dropout(rate1, noise_shape=None, seed=None))\n",
    "# ,kernel_regularizer=regularizers.l2(0.001)\n",
    "# model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])))\n",
    "# , kernel_initializer= tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])\n",
    "# tf.constant_initializer(layer1[0])\n",
    "# tf.constant_initializer(layer1[2])\n",
    "model.add(BatchNormalization())\n",
    "# model.add(act1)\n",
    "model.add(Activation('sigmoid'))\n",
    "# act2=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(layers.Dropout(rate2, noise_shape=None, seed=None))\n",
    "# model.add(Dense(h[1]))\n",
    "# model.add(act2)\n",
    "# act3=layers.LeakyReLU(alpha=0.1)\n",
    "# # model.add(layers.Dropout(rate, noise_shape=None, seed=None))\n",
    "# model.add(Dense(h[2]))\n",
    "# model.add(act3)\n",
    "# act=layers.LeakyReLU(alpha=0.01)\n",
    "model.add(Dense(len_data[1]))\n",
    "#############################################\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "trainfilenames = []\n",
    "for i in sorted_names:\n",
    "    trainfilenames.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# filenames = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = tf.data.TFRecordDataset(trainfilenames)\n",
    "dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.shuffle(buffersize)\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "trainfilenames_val = []\n",
    "for i in sorted_names_val:\n",
    "    trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "# filenames_val = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset_val = tf.data.TFRecordDataset(filenames_val)\n",
    "dataset_val = tf.data.TFRecordDataset(trainfilenames_val)\n",
    "dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "dataset_val = dataset_val.batch(128)\n",
    "# iterator_val = dataset_val.make_initializable_iterator()\n",
    "\n",
    "epochs_num = 50\n",
    "steps = len_data[0] // batch_size\n",
    "val_steps = val_len[0] // batch_size\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "# next_elem = iterator_val.get_next()\n",
    "# Initialize `iterator` with training data.\n",
    "\n",
    "if not os.path.exists(os.path.join(Data_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(Data_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "#     sess.run(iterator_val.initializer, feed_dict={filenames_val: trainfilenames_val})\n",
    "print(\"initialized\")\n",
    "checkpoint_path = os.path.normpath(os.path.join(Data_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "        # Save weights, every 5-epochs.\n",
    "#         period=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "# opt = tf.keras.optimizers.Adamax()\n",
    "# opt = tf.train.AdamOptimizer()\n",
    "opt = tf.keras.optimizers.SGD()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num, callbacks = [cp_callback,early_stop], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "#     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "#     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "#     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")\n",
    "    \n",
    "print(datetime.datetime.now())\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(os.path.normpath(os.path.join(Data_path,'images',ckpt_folder+'.png')))\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# model.save_weights(\"model_10h_dataset.h5\")\n",
    "# print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test','layers10.txt')),layer1[0])\n",
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test','layers11.txt')),layer1[1])\n",
    "np.savetxt(os.path.normpath(os.path.join(Data_path,'results','rbm','test','layers12.txt')),layer1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'ftr_refrmd_10h_norm2'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr = h5f['ftr_refrmd_10h_norm2'][0:126]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = np.matmul(tf.abs(ftr)/5,layer1[0].numpy())+layer1[2].numpy()\n",
    "# hidden2 = np.matmul(hidden1,layer2[0])+layer2[2]\n",
    "# hidden3 = np.matmul(hidden2,layer3[0])+layer3[2]\n",
    "hidden1_b = np.matmul(hidden1,layer1[0].numpy().T)+layer1[1].numpy()\n",
    "# hidden2_b = np.matmul(hidden3_b,layer2[0].T)+layer2[1]\n",
    "# hidden1_b = np.matmul(hidden2_b,layer1[0].T)+layer1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(771, 512) dtype=float32, numpy=\n",
       "array([[-0.01466631, -0.00084128,  0.02330974, ...,  0.01120422,\n",
       "        -0.02315165,  0.00434515],\n",
       "       [-0.01963403, -0.00528434,  0.02172554, ...,  0.00420537,\n",
       "        -0.02114365, -0.00322407],\n",
       "       [-0.02408432,  0.01759911,  0.01289755, ...,  0.01116083,\n",
       "        -0.02765297,  0.03028362],\n",
       "       ...,\n",
       "       [-0.02148633,  0.01786764,  0.01300819, ..., -0.02138482,\n",
       "        -0.04399392,  0.010988  ],\n",
       "       [-0.0025764 ,  0.02511008,  0.02134404, ..., -0.02878903,\n",
       "        -0.04057083,  0.01681587],\n",
       "       [-0.0326599 ,  0.0177589 ,  0.01950189, ..., -0.00672941,\n",
       "        -0.05072301, -0.00034183]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=65007, shape=(126, 771), dtype=float32, numpy=\n",
       "array([[0.0606065 , 0.05738253, 0.04570109, ..., 0.2018005 , 0.20329157,\n",
       "        0.3033479 ],\n",
       "       [0.05667256, 0.02031567, 0.15214147, ..., 0.22820044, 0.19565551,\n",
       "        0.12245405],\n",
       "       [0.02217955, 0.05234206, 0.09567282, ..., 0.23867507, 0.31390697,\n",
       "        0.26440674],\n",
       "       ...,\n",
       "       [1.0093615 , 1.1732726 , 1.0464318 , ..., 0.44613034, 0.44151035,\n",
       "        0.38526854],\n",
       "       [1.0093615 , 1.1732726 , 1.0464318 , ..., 0.44613034, 0.44151035,\n",
       "        0.38526854],\n",
       "       [1.0093615 , 1.1732726 , 1.0464318 , ..., 0.38518018, 0.38347208,\n",
       "        0.38763672]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.abs(ftr)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.2541523,  5.6070485,  7.475915 , ...,  7.5906425,  7.4660616,\n",
       "         4.714741 ],\n",
       "       [ 5.805717 , 10.343396 , 13.01164  , ...,  8.346055 ,  8.134406 ,\n",
       "         5.0012155],\n",
       "       [ 5.2506895,  9.602298 , 12.362278 , ...,  8.404636 ,  8.317651 ,\n",
       "         5.233562 ],\n",
       "       ...,\n",
       "       [18.567364 , 27.930397 , 30.374174 , ..., 15.384658 , 15.095654 ,\n",
       "         9.432099 ],\n",
       "       [18.567364 , 27.930397 , 30.374174 , ..., 15.384658 , 15.095654 ,\n",
       "         9.432099 ],\n",
       "       [12.165686 , 19.746794 , 24.37861  , ..., 11.276628 , 11.496893 ,\n",
       "         7.4698954]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "eval is not supported when eager execution is enabled, is .numpy() what you're looking for?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d6c562f1d99d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlabel_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    960\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m     raise NotImplementedError(\n\u001b[1;32m--> 962\u001b[1;33m         \u001b[1;34m\"eval is not supported when eager execution is enabled, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         \u001b[1;34m\"is .numpy() what you're looking for?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: eval is not supported when eager execution is enabled, is .numpy() what you're looking for?"
     ]
    }
   ],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([True, False])\n",
    "x=tf.cast(x, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
