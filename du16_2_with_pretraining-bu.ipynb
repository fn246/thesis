{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.ops import control_flow_ops\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "I=0\n",
    "def _parse_function(example_proto):\n",
    "    print('1')\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    print(\"i was here\")\n",
    "    print('2')\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "def rbm_layer(n_visible, n_hidden, num_epochs, num_cases, lr, lrh, ws, bs, layer_n, len_data, directories):\n",
    "    Data_path = directories[0]\n",
    "    tfrecord_folder_parent = directories[1]\n",
    "    tfrecord_folder = directories[2]\n",
    "    \n",
    "    tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "    sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "    trainfilenames_x = []\n",
    "    for i in sorted_names_x:\n",
    "        trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path_x,i)))\n",
    "#     filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "#     dataset_x = tf.data.TFRecordDataset(filenames_x)\n",
    "    dataset_x = tf.data.TFRecordDataset(trainfilenames_x)\n",
    "    dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "#     dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "    dataset_x = dataset_x.batch(num_cases)\n",
    "#     iterator_x = dataset_x.make_initializable_iterator()\n",
    "    \n",
    "    weightcost  = 0.0002\n",
    "    initialmomentum  = 0.5\n",
    "    finalmomentum    = 0.9\n",
    "    numcases = 128\n",
    "    W_adder  = tf.zeros((n_visible,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bh_adder = tf.zeros((1,n_hidden),dtype=tf.dtypes.float32)\n",
    "    bv_adder = tf.zeros((1,n_visible),dtype=tf.dtypes.float32)\n",
    "#     x  = tf.placeholder(tf.float32, [None, n_visible], name=\"x\") #The placeholder variable that holds our data\n",
    "#     h = tf.placeholder(tf.float32, [None, n_hidden], name=\"h\")\n",
    "#     h = tf.zeros((numcases, n_hidden),dtype=tf.dtypes.float32)\n",
    "#     h_empty  = np.zeros_like(h)\n",
    "#     h_empty = np.zeros((, n_hidden),dtype=np.float32)\n",
    "#     m  = tf.Variable(0.5, dtype=np.float32, name='m')\n",
    "    W  = tf.Variable(tf.random_normal([n_visible, n_hidden], mean=0, stddev=0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "    bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "    bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "    def sample(probs):\n",
    "        #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "#         return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "        return tf.cast(probs > tf.random_uniform(tf.shape(probs), 0, 1),tf.float32)\n",
    "\n",
    "#     hk = sample(tf.sigmoid(tf.matmul(x, W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "#     #128*512\n",
    "\n",
    "#     #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "#     posprods  = tf.matmul(tf.transpose(x),hk) #771*512\n",
    "#     poshidacts = tf.reduce_sum(hk) #(512)\n",
    "#     posvisacts = tf.reduce_sum(x,axis=0)#771\n",
    "    \n",
    "#     xk   = tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)#128*711\n",
    "#     neghidprobs   = tf.sigmoid(tf.matmul(xk, W) + bh) #128*512\n",
    "#     negprods  = tf.matmul(tf.transpose(xk),neghidprobs); #771*512##################################\n",
    "#     neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "#     negvisacts = tf.reduce_sum(xk, axis=0)\n",
    "#     print(negvisacts)#771\n",
    "# #     print(type(posprod-negprod))\n",
    "# #     print(type(lr))\n",
    "#     m=0.5\n",
    "#     print('here')\n",
    "#     W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "#     bv_adder = (m * bv_adder)+ ((lr/numcases)*(posvisacts-negvisacts))\n",
    "#     bh_adder = (m * bh_adder)+ ((lr/numcases)*(poshidacts-neghidacts))\n",
    "#     #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "#     updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]\n",
    "\n",
    "    ### Run the graph!\n",
    "    # Now it's time to start a session and run the graph! \n",
    "\n",
    "#     with tf.Session() as sess:\n",
    "        #First, we train the model\n",
    "        #initialize the variables of the model\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         print(m)\n",
    "#         sess.run(iterator_x.initializer, feed_dict={filenames_x: trainfilenames_x})\n",
    "#         print(iterator_x.get_next()[0])\n",
    "        #Run through all of the training data num_epochs times\n",
    "#         for epoch in tqdm(range(num_epochs)):\n",
    "            #Train the RBM on batch_size examples at a time\n",
    "#             for X_batch in da(batch_size, layer_n, ws, bs, len_data, name, data_name):\n",
    "#             Data = sess.run(iterator_x.get_next()[0])\n",
    "#             print('blah')\n",
    "#             print(Data.shape)\n",
    "#             for Data in dataset_x:\n",
    "#                 for j in range(layer_n):\n",
    "#                     Data = np.matmul(Data,ws[j])+bs[j]\n",
    "    #             if epoch>5:\n",
    "    #                 momentum=finalmomentum;\n",
    "    #             else:\n",
    "    #                 momentum=initialmomentum;\n",
    "#                 feed_dict = {x: Data}\n",
    "#                 var1 = sess.run(updt, feed_dict)\n",
    "\n",
    "\n",
    "#     def gibbs_step(inpt):\n",
    "#         poshidprobs = tf.sigmoid(tf.matmul(inpt, W) + bh) #Propagate the visible values to sample the hidden values\n",
    "#         poshidstates = sample(poshidprobs)\n",
    "#         #128*512\n",
    "#         #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "#         posprods  = tf.matmul(tf.transpose(inpt),poshidprobs) #771*512\n",
    "#         poshidacts = tf.reduce_sum(poshidprobs, axis=0) #(512)\n",
    "# #             print(poshidprobs)\n",
    "# #             print(poshidacts.numpy())\n",
    "#         posvisacts = tf.reduce_sum(inpt,axis=0)#771\n",
    "\n",
    "#         negdata   = tf.sigmoid(tf.matmul(poshidstates, tf.transpose(W)) + bv)#128*711\n",
    "#         neghidprobs   = tf.sigmoid(tf.matmul(negdata, W) + bh) #128*512\n",
    "#         neghidstates = sample(neghidprobs)\n",
    "#         negprods  = tf.matmul(tf.transpose(negdata),neghidstates); #771*512##################################\n",
    "#         neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "#         negvisacts = tf.reduce_sum(negdata, axis=0)\n",
    "        \n",
    "#         negdata   = tf.sigmoid(tf.matmul(neghidstates, tf.transpose(W)) + bv)#128*711\n",
    "#         neghidprobs   = tf.sigmoid(tf.matmul(negdata, W) + bh) #128*512\n",
    "# #         neghidstates = sample(neghidprobs)\n",
    "#         negprods  = tf.matmul(tf.transpose(negdata),neghidprobs); #771*512##################################\n",
    "#         neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "#         negvisacts = tf.reduce_sum(negdata, axis=0)\n",
    "#         return posprods,negprods,posvisacts,negvisacts,poshidacts,neghidacts\n",
    "        \n",
    "    count=0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        count+=1\n",
    "        incount=0\n",
    "        errsum = 0\n",
    "        for x in dataset_x:\n",
    "#             print(x)\n",
    "#             if incount%1000==0:\n",
    "#                 print(incount)\n",
    "#                 plt.imshow(neghidprobs.numpy(),cmap='gray')\n",
    "#                 plt.show()\n",
    "#                 print(W)\n",
    "#                 print(poshidprobs[10])\n",
    "#                 print(W_adder)\n",
    "#                 print(bv_adder)\n",
    "#             if layer_n>1:\n",
    "# #                 for j in range(layer_n-1):\n",
    "# #                     x = np.matmul(x,ws[j])+bs[j]\n",
    "#                 x = poshidprobs\n",
    "                    \n",
    "            poshidprobs = tf.sigmoid(tf.matmul(x[0], W) + bh) #Propagate the visible values to sample the hidden values\n",
    "            poshidstates = sample(poshidprobs)\n",
    "            #128*512\n",
    "            #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "            posprods  = tf.matmul(tf.transpose(x[0]),poshidprobs) #771*512\n",
    "            poshidacts = tf.reduce_sum(poshidprobs, axis=0) #(512)\n",
    "#             print(poshidprobs)\n",
    "#             print(poshidacts.numpy())\n",
    "            posvisacts = tf.reduce_sum(x[0],axis=0)#771\n",
    "            \n",
    "            negdata   = tf.sigmoid(tf.matmul(poshidstates, tf.transpose(W)) + bv)#128*711\n",
    "            neghidprobs   = tf.sigmoid(tf.matmul(negdata, W) + bh) #128*512\n",
    "            negprods  = tf.matmul(tf.transpose(negdata),neghidprobs); #771*512##################################\n",
    "            neghidacts = tf.reduce_sum(neghidprobs, axis=0)\n",
    "            negvisacts = tf.reduce_sum(negdata, axis=0)\n",
    "            \n",
    "            if incount%100==0:\n",
    "#                 print(tf.abs(x[0]))\n",
    "#                 print(x[0])\n",
    "                print(incount)\n",
    "#                 print(tf.transpose(W).numpy())\n",
    "#                 print(poshidstates.numpy())\n",
    "#                 print(negdata.numpy())\n",
    "#                 print(tf.matmul(poshidstates, tf.transpose(W)).numpy())\n",
    "#                 plt.imshow(tf.transpose(W).numpy(),cmap='gray')\n",
    "#                 plt.show()\n",
    "#                 plt.imshow(poshidstates.numpy(),cmap='gray')\n",
    "#                 plt.show()\n",
    "#                 plt.imshow(tf.matmul(poshidstates, tf.transpose(W)).numpy(),cmap='gray')\n",
    "#                 plt.show()\n",
    "#                 print(W)\n",
    "#                 print(poshidprobs[10])\n",
    "#                 print(W_adder)\n",
    "#                 print(bv_adder)\n",
    "            incount+=1\n",
    "            if epoch>5:\n",
    "                m=finalmomentum\n",
    "            else:\n",
    "                m=initialmomentum\n",
    "#             [posprods,negprods,posvisacts,negvisacts,poshidacts,neghidacts] = gibbs_step(x[0])\n",
    "            W_adder = (m * W_adder)+ (lr*(posprods-negprods)/numcases)-(weightcost*W)\n",
    "            bv_adder = (m * bv_adder)+ ((lrh/numcases)*(posvisacts-negvisacts))\n",
    "            bh_adder = (m * bh_adder)+ ((lrh/numcases)*(poshidacts-neghidacts))\n",
    "#             print(posprods)\n",
    "#             print(poshidacts-neghidacts)\n",
    "#             print(bv_adder)\n",
    "            #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "            updt = [W.assign_add(W_adder).numpy(), bv.assign_add(bv_adder).numpy(), bh.assign_add(bh_adder).numpy()] \n",
    "            err= tf.reduce_sum(tf.reduce_sum((x[0]-negdata)**2 ,axis =0),axis=0)\n",
    "            errsum = err + errsum;\n",
    "        print(errsum)\n",
    "#         print(count)\n",
    "    return updt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "2019-07-11 11:49:52.367236\n",
      "initialized\n",
      "Epoch 1/50\n",
      "  85/1562 [>.............................] - ETA: 1:30:07 - loss: 13.717 - ETA: 30:36 - loss: 13.4866  - ETA: 15:32 - loss: 10.292 - ETA: 11:49 - loss: 9.122 - ETA: 8:43 - loss: 7.4964 - ETA: 6:57 - loss: 6.557 - ETA: 5:48 - loss: 5.812 - ETA: 5:00 - loss: 5.407 - ETA: 4:24 - loss: 5.171 - ETA: 3:57 - loss: 4.934 - ETA: 3:35 - loss: 4.721 - ETA: 3:17 - loss: 4.565 - ETA: 3:02 - loss: 4.397 - ETA: 2:50 - loss: 4.280 - ETA: 2:39 - loss: 4.130 - ETA: 2:30 - loss: 3.992 - ETA: 2:22 - loss: 3.893 - ETA: 2:15 - loss: 3.794 - ETA: 2:08 - loss: 3.746 - ETA: 2:05 - loss: 3.691 - ETA: 2:00 - loss: 3.603 - ETA: 1:55 - loss: 3.542 - ETA: 1:51 - loss: 3.472 - ETA: 1:47 - loss: 3.408 - ETA: 1:43 - loss: 3.349 - ETA: 1:40 - loss: 3.329 - ETA: 1:37 - loss: 3.276 - ETA: 1:34 - loss: 3.243 - ETA: 1:32 - loss: 3.196 - ETA: 1:30 - loss: 3.1572"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-d5f2e30766b2>\", line 176, in <module>\n",
      "    history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num, callbacks = [cp_callback,early_stop], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 851, in fit\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 191, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1191, in train_on_batch\n",
      "    outputs = self._fit_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3166, in __call__\n",
      "    outputs = self._graph_fn(*converted_inputs)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 368, in __call__\n",
      "    return self._call_flat(args)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 433, in _call_flat\n",
      "    outputs = self._inference_function.call(ctx, args)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 269, in call\n",
      "    executor_type=function_call_options.executor_type)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 1083, in partitioned_call\n",
      "    executor_type=executor_type)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_functional_ops.py\", line 507, in stateful_partitioned_call\n",
      "    \"executor_type\", executor_type)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "import sys\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = os.getcwd()\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'tfrecord_files_norm_small'\n",
    "tfrecord_val_folder = 'validation_norm_small'\n",
    "ckpt_folder = '5'\n",
    "dirs = [Data_path, tfrecord_folder_parent, tfrecord_folder]\n",
    " \n",
    "# len_data = (684108, 257)\n",
    "# len_data = (2197278, 257)\n",
    "# val_len = (97278,257)\n",
    "len_data = (200000, 257)\n",
    "val_len = (100000,257)\n",
    "w=3\n",
    "h = [512]\n",
    "global batch_size\n",
    "batch_size = 128\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=200, win_length=500, window='hann')\n",
    "    return recon\n",
    "\n",
    "########################\n",
    "visible = w*len_data[1]\n",
    "hidden = h[0]\n",
    "# visible1 = h[0]\n",
    "# hidden1 = h[1]\n",
    "# visible2 = h[1]\n",
    "# hidden2 = len_data[1]\n",
    "\n",
    "# layer1 = rbm_layer(visible, hidden, 1, batch_size, 0.1, 0.1, [np.eye(visible,hidden)], [np.zeros((1,visible))], 1, len_data[0],dirs)\n",
    "# layer2 = rbm_layer(visible1, hidden1, 50, batch_size, 0.01, [np.eye(visible,hidden),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, len_data[0], dirs)\n",
    "# layer3 = rbm_layer(visible2, hidden2, 50, batch_size, 0.01, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, len_data[0], dirs)\n",
    "\n",
    "###############################\n",
    "#######################\n",
    "I=0\n",
    "\n",
    "# epochs_num=50\n",
    "global datalen\n",
    "datalen=len_data[0]\n",
    "\n",
    "\n",
    "seed = 7\n",
    "rate1 = 0.1\n",
    "rate2 = 0.2\n",
    "buffersize = 1000\n",
    "from tensorflow.keras.layers import Activation\n",
    "# from keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "# model.add(layers.Dropout(rate1, noise_shape=None, seed=None))\n",
    "# ,kernel_regularizer=regularizers.l2(0.001)\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "# , kernel_initializer= tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])\n",
    "# tf.constant_initializer(layer1[0])\n",
    "# tf.constant_initializer(layer1[2])\n",
    "model.add(BatchNormalization())\n",
    "# model.add(act1)\n",
    "model.add(Activation('sigmoid'))\n",
    "# act2=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(layers.Dropout(rate2, noise_shape=None, seed=None))\n",
    "# model.add(Dense(h[1]))\n",
    "# model.add(act2)\n",
    "# act3=layers.LeakyReLU(alpha=0.1)\n",
    "# # model.add(layers.Dropout(rate, noise_shape=None, seed=None))\n",
    "# model.add(Dense(h[2]))\n",
    "# model.add(act3)\n",
    "# act=layers.LeakyReLU(alpha=0.01)\n",
    "model.add(Dense(len_data[1]))\n",
    "#############################################\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "trainfilenames = []\n",
    "for i in sorted_names:\n",
    "    trainfilenames.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# filenames = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = tf.data.TFRecordDataset(trainfilenames)\n",
    "dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.shuffle(buffersize)\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "trainfilenames_val = []\n",
    "for i in sorted_names_val:\n",
    "    trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "# filenames_val = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset_val = tf.data.TFRecordDataset(filenames_val)\n",
    "dataset_val = tf.data.TFRecordDataset(trainfilenames_val)\n",
    "dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "dataset_val = dataset_val.batch(128)\n",
    "# iterator_val = dataset_val.make_initializable_iterator()\n",
    "\n",
    "epochs_num = 50\n",
    "steps = len_data[0] // batch_size\n",
    "val_steps = val_len[0] // batch_size\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "# next_elem = iterator_val.get_next()\n",
    "# Initialize `iterator` with training data.\n",
    "\n",
    "if not os.path.exists(os.path.join(Data_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(Data_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "#     sess.run(iterator_val.initializer, feed_dict={filenames_val: trainfilenames_val})\n",
    "print(\"initialized\")\n",
    "checkpoint_path = os.path.normpath(os.path.join(Data_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "        # Save weights, every 5-epochs.\n",
    "#         period=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "opt = tf.keras.optimizers.Adamax()\n",
    "#     opt = tf.train.AdamOptimizer()\n",
    "#     opt = tf.keras.optimizers.SGD()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num, callbacks = [cp_callback,early_stop], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "#     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "#     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "#     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")\n",
    "    \n",
    "print(datetime.datetime.now())\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(os.path.normpath(os.path.join(Data_path,'images',ckpt_folder+'.png')))\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# model.save_weights(\"model_10h_dataset.h5\")\n",
    "# print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'ftr_refrmd_10h_norm2'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr = h5f['ftr_refrmd_10h_norm2'][0:126]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = np.matmul(tf.abs(ftr)/5,layer1[0].numpy())+layer1[2].numpy()\n",
    "# hidden2 = np.matmul(hidden1,layer2[0])+layer2[2]\n",
    "# hidden3 = np.matmul(hidden2,layer3[0])+layer3[2]\n",
    "hidden1_b = np.matmul(hidden1,layer1[0].numpy().T)+layer1[1].numpy()\n",
    "# hidden2_b = np.matmul(hidden3_b,layer2[0].T)+layer2[1]\n",
    "# hidden1_b = np.matmul(hidden2_b,layer1[0].T)+layer1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(771, 512) dtype=float32, numpy=\n",
       "array([[-0.01466631, -0.00084128,  0.02330974, ...,  0.01120422,\n",
       "        -0.02315165,  0.00434515],\n",
       "       [-0.01963403, -0.00528434,  0.02172554, ...,  0.00420537,\n",
       "        -0.02114365, -0.00322407],\n",
       "       [-0.02408432,  0.01759911,  0.01289755, ...,  0.01116083,\n",
       "        -0.02765297,  0.03028362],\n",
       "       ...,\n",
       "       [-0.02148633,  0.01786764,  0.01300819, ..., -0.02138482,\n",
       "        -0.04399392,  0.010988  ],\n",
       "       [-0.0025764 ,  0.02511008,  0.02134404, ..., -0.02878903,\n",
       "        -0.04057083,  0.01681587],\n",
       "       [-0.0326599 ,  0.0177589 ,  0.01950189, ..., -0.00672941,\n",
       "        -0.05072301, -0.00034183]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=65007, shape=(126, 771), dtype=float32, numpy=\n",
       "array([[0.0606065 , 0.05738253, 0.04570109, ..., 0.2018005 , 0.20329157,\n",
       "        0.3033479 ],\n",
       "       [0.05667256, 0.02031567, 0.15214147, ..., 0.22820044, 0.19565551,\n",
       "        0.12245405],\n",
       "       [0.02217955, 0.05234206, 0.09567282, ..., 0.23867507, 0.31390697,\n",
       "        0.26440674],\n",
       "       ...,\n",
       "       [1.0093615 , 1.1732726 , 1.0464318 , ..., 0.44613034, 0.44151035,\n",
       "        0.38526854],\n",
       "       [1.0093615 , 1.1732726 , 1.0464318 , ..., 0.44613034, 0.44151035,\n",
       "        0.38526854],\n",
       "       [1.0093615 , 1.1732726 , 1.0464318 , ..., 0.38518018, 0.38347208,\n",
       "        0.38763672]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.abs(ftr)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.2541523,  5.6070485,  7.475915 , ...,  7.5906425,  7.4660616,\n",
       "         4.714741 ],\n",
       "       [ 5.805717 , 10.343396 , 13.01164  , ...,  8.346055 ,  8.134406 ,\n",
       "         5.0012155],\n",
       "       [ 5.2506895,  9.602298 , 12.362278 , ...,  8.404636 ,  8.317651 ,\n",
       "         5.233562 ],\n",
       "       ...,\n",
       "       [18.567364 , 27.930397 , 30.374174 , ..., 15.384658 , 15.095654 ,\n",
       "         9.432099 ],\n",
       "       [18.567364 , 27.930397 , 30.374174 , ..., 15.384658 , 15.095654 ,\n",
       "         9.432099 ],\n",
       "       [12.165686 , 19.746794 , 24.37861  , ..., 11.276628 , 11.496893 ,\n",
       "         7.4698954]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "eval is not supported when eager execution is enabled, is .numpy() what you're looking for?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d6c562f1d99d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlabel_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    960\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m     raise NotImplementedError(\n\u001b[1;32m--> 962\u001b[1;33m         \u001b[1;34m\"eval is not supported when eager execution is enabled, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         \u001b[1;34m\"is .numpy() what you're looking for?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: eval is not supported when eager execution is enabled, is .numpy() what you're looking for?"
     ]
    }
   ],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([True, False])\n",
    "x=tf.cast(x, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
