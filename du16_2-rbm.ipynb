{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################i tested rbm from the guy in github, this code has implemented the exact pper from hinton.\n",
    "\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "I=0\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "def rbm_layer(n_visible, n_hidden, num_epochs, num_cases, lr, ws, bs, layer_n, len_data, directories):\n",
    "    \n",
    "    Data_path = directories[0]\n",
    "    tfrecord_folder_parent = directories[1]\n",
    "    tfrecord_folder = directories[2]\n",
    "    \n",
    "    tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "    sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "    trainfilenames_x = []\n",
    "    for i in sorted_names_x:\n",
    "        trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path_x,i)))\n",
    "#     filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "    dataset_x = tf.data.TFRecordDataset(trainfilenames_x)\n",
    "    dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "#     dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "    dataset_x = dataset_x.batch(batch_size)\n",
    "#     iterator_x = dataset_x.make_initializable_iterator()\n",
    "\n",
    "#     x  = tf.placeholder(tf.float32, [None, n_visible], name=\"x\") #The placeholder variable that holds our data\n",
    "    W  = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "    bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "    bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "    def sample(probs):\n",
    "        #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "        return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "\n",
    "\n",
    "    def gibbs_sample(k,data):\n",
    "        #Runs a k-step gibbs chain to sample from the probability distribution of the RBM defined by W, bh, bv\n",
    "        def gibbs_step(count, k, data):\n",
    "            #Runs a single gibbs step. The visible values are initialized to xk\n",
    "            hk = sample(tf.sigmoid(tf.matmul(data, W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "#             xk   = tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)\n",
    "            data = sample(tf.truncated_normal((1,n_visible),tf.matmul(hk, tf.transpose(W)) + bv,1))\n",
    "            return count+1, k, data\n",
    "#         print('2')\n",
    "        ct = tf.constant(0) #counter\n",
    "        #Run gibbs steps for k iterations\n",
    "        [_, _, x_sample]=control_flow_ops.while_loop(lambda count, num_iter, *args: count < num_iter,\n",
    "                                             gibbs_step, [ct, tf.constant(k), data], back_prop = False)\n",
    "#         print('3')\n",
    "        #This is not strictly necessary in this implementation, but if you want to adapt this code to use one of TensorFlow's\n",
    "        #optimizers, you need this in order to stop tensorflow from propagating gradients back through the gibbs step\n",
    "        x_sample = tf.stop_gradient(x_sample) \n",
    "#         print('4')\n",
    "        return x_sample\n",
    "\n",
    "    ### Training Update Code\n",
    "    # Now we implement the contrastive divergence algorithm. First, we get the samples of x and h from the probability distribution\n",
    "    #The sample of x\n",
    "#     print('1')\n",
    "#     x_sample = gibbs_sample(5) \n",
    "#     #The sample of the hidden nodes, starting from the visible state of x\n",
    "#     h = sample(tf.sigmoid(tf.matmul(x, W) + bh)) \n",
    "#     #The sample of the hidden nodes, starting from the visible state of x_sample\n",
    "#     h_sample = sample(tf.sigmoid(tf.matmul(x_sample, W) + bh)) \n",
    "\n",
    "#     #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "# #     size_bt = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "#     W_adder  = tf.multiply(lr, tf.subtract(tf.matmul(tf.transpose(x), h), tf.matmul(tf.transpose(x_sample), h_sample)))\n",
    "#     bv_adder = tf.multiply(lr, tf.reduce_sum(tf.subtract(x, x_sample), 0, True))\n",
    "#     bh_adder = tf.multiply(lr, tf.reduce_sum(tf.subtract(h, h_sample), 0, True))\n",
    "#     #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "#     updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]\n",
    "\n",
    "\n",
    "\n",
    "    ### Run the graph!\n",
    "    # Now it's time to start a session and run the graph! \n",
    "\n",
    "    incount=0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        #Train the RBM on batch_size examples at a time\n",
    "#             for X_batch in da(batch_size, layer_n, ws, bs, len_data, name, data_name):\n",
    "        for x in dataset_x:\n",
    "            incount+=1\n",
    "            if incount%1000==0:\n",
    "                print(incount)\n",
    "#             if layer_n>1:\n",
    "#                 for j in range(layer_n):\n",
    "#                     x_0 = np.matmul(x[0],ws[j])+bs[j]\n",
    "#             else:\n",
    "#                 x_0 = x[0]\n",
    "            x_sample = gibbs_sample(1,x[0]) \n",
    "            #The sample of the hidden nodes, starting from the visible state of x\n",
    "            h = sample(tf.sigmoid(tf.matmul(x[0], W) + bh)) \n",
    "            #The sample of the hidden nodes, starting from the visible state of x_sample\n",
    "            h_sample = sample(tf.sigmoid(tf.matmul(x_sample, W) + bh)) \n",
    "\n",
    "            #Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "        #     size_bt = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "            W_adder  = tf.multiply(lr, tf.subtract(tf.matmul(tf.transpose(x[0]), h), tf.matmul(tf.transpose(x_sample), h_sample)))\n",
    "            bv_adder = tf.multiply(lr, tf.reduce_sum(tf.subtract(x[0], x_sample), 0, True))\n",
    "            bh_adder = tf.multiply(lr, tf.reduce_sum(tf.subtract(h, h_sample), 0, True))\n",
    "            #When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "            updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]\n",
    "    return updt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [06:42<00:00, 402.65s/it]"
     ]
    }
   ],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = os.getcwd()\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'tfrecord_files_10h'\n",
    "tfrecord_val_folder = 'validation_10h'\n",
    "ckpt_folder = '3'\n",
    "dirs = [Data_path, tfrecord_folder_parent, tfrecord_folder]\n",
    " \n",
    "# len_data = (684108, 257)\n",
    "# len_data = (100000, 257)\n",
    "len_data = (2197278, 257)\n",
    "val_len = (97278,257)\n",
    "w=3\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=200, win_length=500, window='hann')\n",
    "    return recon\n",
    "#######################\n",
    "I=0\n",
    "global batch_size\n",
    "batch_size = 128\n",
    "# epochs_num=50\n",
    "global datalen\n",
    "datalen=len_data[0]\n",
    "\n",
    "h = [512,512]\n",
    "seed = 7\n",
    "rate1 = 0.1\n",
    "rate2 = 0.2\n",
    "from tensorflow.keras.layers import Activation\n",
    "# from keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model.add(layers.Dropout(rate1, noise_shape=None, seed=None))\n",
    "# ,kernel_regularizer=regularizers.l2(0.001)\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act1)\n",
    "# model.add(Activation('sigmoid'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(layers.Dropout(rate2, noise_shape=None, seed=None))\n",
    "model.add(Dense(h[1]))\n",
    "model.add(act2)\n",
    "# act3=layers.LeakyReLU(alpha=0.1)\n",
    "# # model.add(layers.Dropout(rate, noise_shape=None, seed=None))\n",
    "# model.add(Dense(h[2]))\n",
    "# model.add(act3)\n",
    "act=layers.LeakyReLU(alpha=0.01)\n",
    "model.add(Dense(len_data[1]))\n",
    "#############################################\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "# def _parse_function(example_proto):\n",
    "#     print('1')\n",
    "#     features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "#               \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "#     parsed_features = tf.parse_single_example(example_proto, features)\n",
    "#     print(\"i was here\")\n",
    "#     print('2')\n",
    "#     return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "# tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "# sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "# trainfilenames = []\n",
    "# for i in sorted_names:\n",
    "#     trainfilenames.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# filenames = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset = tf.data.TFRecordDataset(filenames)\n",
    "# dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "# dataset = dataset.batch(batch_size)\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# # orig_path = os.getcwd()\n",
    "# tfrecord_path_x = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "# sorted_names_x = natsorted(os.listdir(tfrecord_path_x))\n",
    "# trainfilenames_x = []\n",
    "# for i in sorted_names_x:\n",
    "#     trainfilenames_x.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "# filenames_x = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset_x = tf.data.TFRecordDataset(filenames_x)\n",
    "# dataset_x = dataset_x.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset_x = dataset_x.repeat()  # Repeat the input indefinitely.\n",
    "# dataset_x = dataset_x.batch(batch_size)\n",
    "# iterator_x = dataset_x.make_initializable_iterator()\n",
    "\n",
    "########################\n",
    "visible = w*len_data[1]\n",
    "hidden = h[0]\n",
    "visible1 = h[0]\n",
    "hidden1 = h[1]\n",
    "visible2 = h[1]\n",
    "hidden2 = len_data[1]\n",
    "\n",
    "layer1 = rbm_layer(visible, hidden, 1, batch_size, 0.0001, [np.eye(visible)], [np.zeros((1,visible))], 1, len_data[0],dirs)\n",
    "# layer2 = rbm_layer(visible1, hidden1, 50, batch_size, 0.01, [np.eye(visible),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, len_data[0], dirs)\n",
    "# layer3 = rbm_layer(visible2, hidden2, 50, batch_size, 0.01, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, len_data[0], dirs)\n",
    "\n",
    "###############################\n",
    "\n",
    "# tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "# sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "# trainfilenames_val = []\n",
    "# for i in sorted_names_val:\n",
    "#     trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "# filenames_val = tf.placeholder(tf.string, shape=[None])\n",
    "# dataset_val = tf.data.TFRecordDataset(filenames_val)\n",
    "# dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "# dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "# dataset_val = dataset_val.batch(128)\n",
    "# iterator_val = dataset_val.make_initializable_iterator()\n",
    "\n",
    "# epochs_num = 50\n",
    "# steps = len_data[0] // batch_size\n",
    "# val_steps = val_len[0] // batch_size\n",
    "# # You can feed the initializer with the appropriate filenames for the current\n",
    "# # phase of execution, e.g. training vs. validation.\n",
    "# # next_elem = iterator_val.get_next()\n",
    "# # Initialize `iterator` with training data.\n",
    "\n",
    "# if not os.path.exists(os.path.join(Data_path,\"checkpoints\",ckpt_folder)):\n",
    "#     os.makedirs(os.path.join(Data_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "# print(datetime.datetime.now())\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "#     sess.run(iterator_val.initializer, feed_dict={filenames_val: trainfilenames_val})\n",
    "#     print(\"initialized\")\n",
    "#     checkpoint_path = os.path.normpath(os.path.join(Data_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "#     checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#     cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "#         # Save weights, every 5-epochs.\n",
    "# #         period=1)\n",
    "#     early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "#     opt = tf.keras.optimizers.Adamax()\n",
    "# #     opt = tf.train.AdamOptimizer()\n",
    "# #     opt = tf.keras.optimizers.SGD()\n",
    "#     model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "#     history = model.fit( iterator, steps_per_epoch=steps,epochs=epochs_num, callbacks = [cp_callback,early_stop], verbose=1,validation_data=iterator_val,validation_steps=val_steps)\n",
    "# #     model.save(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     tf.keras.models.save_model(model, os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "# #     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3h_dataset.h5\")))\n",
    "#     model_json = model.to_json()\n",
    "#     with open(os.path.normpath(os.path.join(Data_path, 'models', \"model_3.json\")), \"w\") as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     # # serialize weights to HDF5\n",
    "#     model.save_weights(os.path.normpath(os.path.join(Data_path, 'models', \"model_3.h5\")))\n",
    "#     print(\"Saved model to disk\")\n",
    "    \n",
    "# print(datetime.datetime.now())\n",
    "# %matplotlib inline\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.normpath(os.path.join(Data_path,'images',ckpt_folder+'.png')))\n",
    "# # model_json = model.to_json()\n",
    "# # with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "# #     json_file.write(model_json)\n",
    "# # model.save_weights(\"model_10h_dataset.h5\")\n",
    "# # print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.2475158 ,  -1.9742541 ,  -1.4477179 ,  -1.5931294 ,\n",
       "         -3.1088848 ,  -3.1776745 ,  -2.2345972 ,  -2.176909  ,\n",
       "         -1.8289479 ,   2.9643917 ,  -2.6424243 ,  -2.2647831 ,\n",
       "         -1.8813525 ,  -1.7134432 ,  -2.8934083 ,  -2.0282397 ,\n",
       "         -2.3665795 ,  -3.9246156 ,  -2.6973042 ,  -2.1822975 ,\n",
       "         -2.3149798 ,  -2.8562038 ,  -1.1137855 ,  -2.2407992 ,\n",
       "         -2.6504376 ,  -2.1411154 ,  -2.0731375 ,  -2.9420066 ,\n",
       "         -3.1049736 ,  -1.7636405 ,  -1.5152267 ,  -3.0478797 ,\n",
       "         -2.3803682 ,  -1.769547  ,  -1.5145209 ,  -2.624918  ,\n",
       "         -1.476123  ,  -3.7534223 ,   2.540963  ,  -1.6066345 ,\n",
       "         -3.0640843 ,  -1.5624328 ,  -2.0067468 ,  -2.2258003 ,\n",
       "         -0.783995  ,  -2.1117177 ,  -2.8673313 ,  -2.7525141 ,\n",
       "          2.0701401 ,  -2.546133  ,  -2.2391949 ,  -1.6873332 ,\n",
       "          1.5725123 ,   1.4661279 ,  -7.5615625 ,  -2.1778116 ,\n",
       "         -2.0315425 ,  -6.207954  ,  -1.7040414 ,   2.7149253 ,\n",
       "         -2.0245376 ,  -1.6355437 ,  -1.7178394 ,  -2.9900846 ,\n",
       "         -2.3574727 ,   1.2452111 ,   1.049497  ,  -5.7220225 ,\n",
       "         -2.3661761 ,  -1.851753  ,  -1.5179327 ,  -3.1990793 ,\n",
       "         -1.5090268 ,   0.87419516,  -1.2526006 ,  -3.0072763 ,\n",
       "         -1.083296  ,   1.8447468 ,  -2.3547888 ,  -2.7114205 ,\n",
       "         -1.3788238 ,  -2.5454946 ,  -3.2015777 ,  -5.1908584 ,\n",
       "         -1.9892559 ,  -0.7169966 ,   1.4265167 ,  -3.0216777 ,\n",
       "         -2.8747952 ,  -3.0141773 ,  -3.0569706 ,  -1.589831  ,\n",
       "         -2.0400362 ,  -7.147943  ,  -2.6789043 ,  -1.6475384 ,\n",
       "          5.688092  ,  -1.5599277 ,  -1.9616494 ,  -2.537549  ,\n",
       "         -5.5218    ,  -1.5890286 ,  -1.8615452 ,  -2.625129  ,\n",
       "         -2.0472484 ,  -1.6608344 ,  -1.4440122 ,  -2.8930883 ,\n",
       "         -3.12198   ,  -2.971494  ,  -2.5596578 ,   1.4730023 ,\n",
       "         -2.6750193 ,  -1.0184911 ,  -2.7010245 ,  -1.8569553 ,\n",
       "         -3.1425867 ,  -2.8646088 ,  -1.8328458 ,  -3.0549817 ,\n",
       "         -2.682414  ,   2.4806962 ,  -2.0164456 ,  -1.4188263 ,\n",
       "         -1.6343287 ,  -2.9186895 ,  -2.954898  ,  -1.8909501 ,\n",
       "         -2.8042088 ,  -3.1757703 ,  -2.6174273 ,  -1.3845158 ,\n",
       "         -2.8947003 ,  -2.0188527 ,   1.069994  ,  -6.740838  ,\n",
       "         -2.3553886 ,  -2.5714355 ,  -3.0115836 ,  -3.0140986 ,\n",
       "         -2.5112336 ,  -1.6226273 ,  -2.9260087 ,  -2.9735844 ,\n",
       "         -1.6592333 ,  -2.4505508 ,  -2.8697083 ,   2.0594106 ,\n",
       "         -2.8896978 ,  -2.6349316 ,  -2.788564  ,  -2.009255  ,\n",
       "          1.3687228 ,  -1.9126534 ,  -6.18812   ,  -1.8852496 ,\n",
       "         -1.6703358 ,  -1.5770254 ,  -2.8583095 ,  -2.6695147 ,\n",
       "         -1.5017319 ,  -1.3940198 ,  -2.9274933 ,  -2.422772  ,\n",
       "         -1.9667523 ,  -2.9240918 ,  -2.4008725 ,  -2.3589647 ,\n",
       "         -7.5040646 ,  -1.943742  ,  -0.43559882,  -3.0614755 ,\n",
       "         -2.7635283 ,  -1.9202585 ,  -2.9254003 ,  -2.4808567 ,\n",
       "         -2.0459373 ,  -1.8334464 ,  -2.7930186 ,  -1.5186216 ,\n",
       "         -1.5544201 ,  -3.0865905 ,   2.6093433 ,  -0.36810288,\n",
       "         -1.9715583 ,  -2.1159122 ,  -1.7886348 ,  -1.8305506 ,\n",
       "         -2.4155674 ,  -1.4624196 ,  -2.1667    ,  -4.4098635 ,\n",
       "         -2.8642051 ,  -3.1066773 ,  -2.519842  ,  -2.8737843 ,\n",
       "         -1.6196396 ,  -2.2971756 ,  -0.21880242,  -5.8094735 ,\n",
       "         -3.079188  ,  -1.7011169 ,  -2.6797242 ,   0.9170986 ,\n",
       "         -2.5757265 ,   2.040652  ,  -2.5226576 ,  -1.8721408 ,\n",
       "         -2.2101984 ,  -2.3744617 ,  -1.5894375 ,  -2.3223805 ,\n",
       "         -3.1611865 ,  -2.776902  ,  -6.8067107 ,  -2.585226  ,\n",
       "         -0.8166923 ,  -1.7520369 ,  -2.627047  ,  -2.9256542 ,\n",
       "         -1.07369   ,  -2.0856314 ,   2.1010373 ,   0.3255986 ,\n",
       "         -2.303587  ,  -1.9982499 ,  -3.0306673 ,  -1.3148112 ,\n",
       "         -8.938022  ,  -2.103016  ,  -1.3351114 ,   5.95777   ,\n",
       "         -2.1508238 ,   1.456693  , -10.198482  ,  -1.4158168 ,\n",
       "          1.4391286 ,  -7.2324605 ,  -2.7622023 ,  -2.8699143 ,\n",
       "         -2.1870139 ,  -1.9751533 ,  -1.3983114 ,  -1.0897954 ,\n",
       "         -2.805516  ,  -1.5966251 ,  -2.3651798 ,  -2.945296  ,\n",
       "          0.9938994 ,  -2.5206382 ,   1.370321  ,  -1.8168405 ,\n",
       "         -2.1213152 ,  -0.1667983 ,  -3.075293  ,  -1.3275008 ,\n",
       "          2.7577796 ,  -2.805406  ,  -3.193167  ,  -2.8820097 ,\n",
       "         -0.09080114,  -1.2845091 ,  -1.5977429 ,  -2.1908998 ,\n",
       "         -2.9797788 ,  -1.6830345 ,   1.7584437 ,   0.02750122,\n",
       "          3.6037447 ,  -1.6726316 ,  -2.3897765 ,  -1.5942328 ,\n",
       "         -2.5219584 ,  -2.930512  ,  -1.8284498 ,  -6.3962584 ,\n",
       "         -1.5673363 ,  -5.034865  ,  -3.5734003 ,  -3.234963  ,\n",
       "         -1.9022573 ,  -1.5556362 ,  -6.616769  ,  -2.0713036 ,\n",
       "         -0.5311939 ,  -2.4188643 ,  -2.0664418 ,  -2.430172  ,\n",
       "         -6.1233606 ,  -3.0457785 ,  -1.7797405 ,  -4.0759044 ,\n",
       "         -2.045332  ,  -0.75659347,   0.5984006 ,  -1.906557  ,\n",
       "         -2.7693863 ,  -2.8985994 ,  -1.8130517 ,  -2.5362415 ,\n",
       "         -1.7189373 ,   0.83209604,  -0.8392952 ,  -1.5772393 ,\n",
       "         -2.7708225 ,  -2.56763   ,  -2.0735326 ,  -5.577265  ,\n",
       "         -1.4643284 ,  -2.3606687 ,  -6.6174164 ,  -1.7507361 ,\n",
       "         -2.9461904 ,  -2.4999723 ,   1.0456004 ,  -3.1905708 ,\n",
       "         -1.9214513 ,  -1.9426547 ,  -1.7091321 ,  -0.44159946,\n",
       "         -1.300012  ,  -1.9922593 ,  -2.4066567 ,  -2.3383746 ,\n",
       "          2.7712898 ,  -3.0459914 ,  -1.9176455 ,  -1.7549498 ,\n",
       "         -5.2263403 ,  -2.6492293 ,  -3.1775672 ,  -1.5767256 ,\n",
       "          0.15670004,  -2.1436164 ,  -1.9960573 ,  -3.0350866 ,\n",
       "         -2.312773  ,  -2.6532202 ,   0.12840202,  -0.77509457,\n",
       "         -2.3071811 ,  -2.1844053 ,  -2.1629055 ,  -5.0215797 ,\n",
       "         -2.950283  ,  -1.0510937 ,  -2.8888087 ,  -2.1424317 ,\n",
       "         -2.9349997 ,  -2.3535693 ,  -2.5801442 ,  -2.7293246 ,\n",
       "         -2.0172431 ,  -1.7091409 ,  -5.553363  ,   1.3193226 ,\n",
       "          1.4133364 ,  -2.4719431 ,  -2.9419048 ,  -3.0325685 ,\n",
       "         -1.5455135 ,  -1.3718166 ,  -1.5659338 ,  -1.736241  ,\n",
       "         -2.6844099 ,  -2.3846688 ,  -1.6885313 ,  -1.8451462 ,\n",
       "         -3.0043917 ,  -3.1793768 ,  -1.4752172 ,  -1.7160473 ,\n",
       "         -3.1028745 ,  -1.7068338 ,  -1.7995428 ,  -1.9398538 ,\n",
       "         -2.9075959 ,  -2.1826081 ,  -3.1337702 ,  -3.0701838 ,\n",
       "         -2.8553076 ,  -1.5423231 ,  -1.699139  ,  -2.8875144 ,\n",
       "         -2.5111392 ,  -3.0787425 ,  -3.1841843 ,  -1.7768449 ,\n",
       "         -0.05098994,  -1.7670484 ,   2.3912802 ,  -2.76213   ,\n",
       "         -0.577997  ,  -1.393513  ,  -2.5408552 ,  -2.4833565 ,\n",
       "         -2.3873737 ,  -2.5826397 ,  -1.7450364 ,  -1.7860452 ,\n",
       "         -3.7276042 ,  -2.8367088 ,  -2.3788717 ,  -8.082734  ,\n",
       "         -1.720039  ,  -1.9155508 ,  -2.4812691 ,  -0.9097773 ,\n",
       "         -4.8744955 ,  -1.2097054 ,  -2.5985417 ,  -2.7457209 ,\n",
       "         -1.5776309 ,  -1.3930067 ,  -2.0851295 ,  -1.7751486 ,\n",
       "         -1.8274407 ,   1.0956043 ,  -1.7710537 ,   0.6588969 ,\n",
       "         -1.4208246 ,  -1.6278313 ,   2.6846864 ,  -2.5178583 ,\n",
       "         -2.4459631 ,  -6.5610485 ,  -2.6575232 ,  -2.7665174 ,\n",
       "         -1.4971279 ,  -1.4545232 ,  -2.5176396 ,   1.0716999 ,\n",
       "         -1.6837395 ,   0.9516974 ,  -1.7778474 ,  -1.7566432 ,\n",
       "         -3.1660752 ,  -2.4284585 ,  -2.795503  ,   1.9197513 ,\n",
       "         -1.9935489 ,  -1.9067438 ,  -1.4580237 ,  -1.8695396 ,\n",
       "         -2.7863274 ,  -2.9994855 ,   1.9122105 ,  -2.6430426 ,\n",
       "         -2.7883062 ,  -1.0111742 ,  -1.7338423 ,  -2.8533075 ,\n",
       "         -1.6521316 ,  -1.7355417 ,  -6.78195   ,  -3.047482  ,\n",
       "         -0.7049998 ,  -2.4380517 ,  -1.7456434 ,  -1.6198398 ,\n",
       "         -2.0361416 ,  -1.7141454 ,  -2.4546506 ,  -3.1420712 ,\n",
       "         -6.879761  ,  -6.399516  ,  -3.096993  ,  -2.6489422 ,\n",
       "          1.5778322 ,   1.5370332 ,  -0.6441932 ,  -2.678724  ,\n",
       "         -1.396611  ,  -1.910446  ,  -3.1175792 ,  -2.1364124 ,\n",
       "         -1.5220296 ,  -2.4656444 ,  -2.8541    ,  -1.4792202 ,\n",
       "         -1.7222439 ,  -2.7833092 ,  -2.7752059 ,  -1.9848541 ,\n",
       "         -2.9462926 ,  -5.463684  ,  -2.023138  ,  -3.129378  ,\n",
       "          1.9119618 ,  -3.1273885 ,  -2.8663952 ,  -1.5298285 ,\n",
       "         -1.7862464 ,  -2.0540311 ,  -1.4780216 ,  -2.1143284 ,\n",
       "         -2.7611125 ,  -2.4467611 ,  -2.4750357 ,  -1.5641359 ,\n",
       "         -3.0986857 ,  -0.6738961 ,  -1.3113202 ,  -1.7571396 ,\n",
       "         -2.060947  ,  -1.7531325 ,  -1.5818393 ,  -1.4410131 ,\n",
       "          2.3215954 ,  -2.1663213 ,  -2.816913  ,  -1.6737475 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1[2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'ftr_refrmd_10h'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr = h5f['ftr_refrmd_10h'][0:126]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = np.matmul(ftr,layer1[0].numpy())+layer1[2]\n",
    "# hidden2 = np.matmul(hidden1,layer2[0])+layer2[2]\n",
    "# hidden3 = np.matmul(hidden2,layer3[0])+layer3[2]\n",
    "hidden1_b = np.matmul(hidden1,layer1[0].numpy().T)+layer1[1]\n",
    "# hidden2_b = np.matmul(hidden3_b,layer2[0].T)+layer2[1]\n",
    "# hidden1_b = np.matmul(hidden2_b,layer1[0].T)+layer1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4156045, shape=(126, 771), dtype=float32, numpy=\n",
       "array([[  2216.7712,   2014.4908,   -495.7506, ...,  -5437.903 ,\n",
       "         -4312.979 ,  -6173.175 ],\n",
       "       [  4058.818 ,   3410.1104,   -906.2233, ...,  -8007.8457,\n",
       "         -5868.8306,  -8755.482 ],\n",
       "       [  4064.866 ,   3388.3003,   -952.1764, ...,  -8074.025 ,\n",
       "         -5986.185 ,  -8911.699 ],\n",
       "       ...,\n",
       "       [  4463.768 ,   3638.176 ,  -1983.2755, ..., -10517.054 ,\n",
       "         -7572.196 , -11410.955 ],\n",
       "       [  4463.768 ,   3638.176 ,  -1983.2755, ..., -10517.054 ,\n",
       "         -7572.196 , -11410.955 ],\n",
       "       [  2185.529 ,   1799.0817,  -2085.5159, ...,  -5159.5103,\n",
       "         -3243.5583,  -5679.702 ]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ..., -4.9426355 ,\n",
       "        -4.978707  , -6.258066  ],\n",
       "       [ 0.8652955 ,  0.4874023 , -0.74453855, ..., -5.1649346 ,\n",
       "        -4.9139147 , -4.619758  ],\n",
       "       [ 0.61080295,  0.6883045 , -0.34954664, ..., -5.2531357 ,\n",
       "        -5.9172816 , -5.9053864 ],\n",
       "       ...,\n",
       "       [-7.        , -7.        , -7.        , ..., -7.        ,\n",
       "        -7.        , -7.        ],\n",
       "       [-7.        , -7.        , -7.        , ..., -7.        ,\n",
       "        -7.        , -7.        ],\n",
       "       [-7.        , -7.        , -7.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9180.227  ,   6740.271  ,    240.81975, ..., -12260.12   ,\n",
       "        -12897.364  , -13447.202  ],\n",
       "       [ 13047.042  ,   6860.518  ,  -1310.1332 , ..., -16233.931  ,\n",
       "        -20017.99   , -16585.186  ],\n",
       "       [ 13302.395  ,   6682.597  ,   -464.38947, ..., -16913.45   ,\n",
       "        -20491.3    , -16080.6875 ],\n",
       "       ...,\n",
       "       [ 15171.248  ,   6297.668  ,  -3952.972  , ..., -21954.451  ,\n",
       "        -26379.877  , -22744.285  ],\n",
       "       [ 15171.248  ,   6297.668  ,  -3952.972  , ..., -21954.451  ,\n",
       "        -26379.877  , -22744.285  ],\n",
       "       [  5757.045  ,   -735.7306 ,  -4885.1025 , ...,  -9860.973  ,\n",
       "        -16378.743  , -12807.099  ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden3_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 257), (?,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
