{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "i was here\n",
      "i was here\n",
      "initialized\n",
      "18706/18706 [==============================] - 836s 45ms/step - loss: 0.9721 - val_loss: 1.9128\n",
      "Saved model to disk\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-505cdbb4f25b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saved model to disk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "#import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#import sounddevice as sd\n",
    "# import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "input_tr_name = 'ftr_refrmd_10h'\n",
    "input_val_name = 'ftr_refrmd_test'\n",
    "target_tr_name = 'single_dataset_log_16'\n",
    "target_val_name = 'clean_data'\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'tfrecord_files_3h'\n",
    "tfrecord_val_folder = 'test'\n",
    "input_tr_path = os.path.normpath(os.path.join(Data_path,input_tr_name))\n",
    "input_val_path = os.path.normpath(os.path.join(Data_path,input_val_name))\n",
    "target_tr_path = os.path.normpath(os.path.join(Data_path,target_tr_name))\n",
    "target_val_path = os.path.normpath(os.path.join(Data_path,target_val_name))\n",
    "\n",
    "file = h5py.File(target_val_path+'.hdf5','r')\n",
    "val_y = file[target_val_name][0:]\n",
    "file.close()\n",
    "file = h5py.File(input_val_path+'.hdf5','r')\n",
    "val_x = file[input_val_name][0:]\n",
    "file.close()\n",
    "\n",
    "hh = h5py.File(target_tr_path+'.hdf5', 'r')\n",
    "d=hh[target_tr_name]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "\n",
    "file = h5py.File(target_val_path+'.hdf5','r')\n",
    "val_y = file[target_val_name]\n",
    "val_len=val_y.shape\n",
    "file.close()\n",
    "\n",
    "w=3\n",
    "# print(len_data)\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=200, win_length=500, window='hann')\n",
    "    return recon\n",
    "#######################\n",
    "I=0\n",
    "global batch_size\n",
    "batch_size=128\n",
    "# epochs_num=50\n",
    "global datalen\n",
    "datalen=len_data[0]\n",
    "\n",
    "file.close()\n",
    "h = [1024,512]\n",
    "seed = 7\n",
    "from tensorflow.keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(act)\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1]))\n",
    "model.add(act)\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(len_data[1]))\n",
    "#############################################\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((3*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    print(\"i was here\")\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "# orig_path = os.getcwd()\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "sorted_names = natsorted(os.listdir(tfrecord_path))\n",
    "trainfilenames = []\n",
    "for i in sorted_names:\n",
    "    trainfilenames.append(os.path.normpath(os.path.join(tfrecord_path,i)))\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(128)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "trainfilenames_val = []\n",
    "for i in sorted_names_val:\n",
    "    trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "filenames_val = tf.placeholder(tf.string, shape=[None])\n",
    "dataset_val = tf.data.TFRecordDataset(filenames_val)\n",
    "dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "dataset_val = dataset_val.batch(128)\n",
    "iterator_val = dataset_val.make_initializable_iterator()\n",
    "\n",
    "\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "# next_elem = iterator_val.get_next()\n",
    "# Initialize `iterator` with training data.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(iterator.initializer, feed_dict={filenames: trainfilenames})\n",
    "    sess.run(iterator_val.initializer, feed_dict={filenames_val: trainfilenames_val})\n",
    "    print(\"initialized\")\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    epochs_num=1\n",
    "    steps = len_data[0] // batch_size\n",
    "    val_steps = val_len[0] // batch_size\n",
    "    history = model.fit( iterator, steps_per_epoch=steps,epochs=epochs_num, verbose=1,validation_data=iterator_val,validation_steps=val_steps)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model_10h_dataset.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "model_json = model.to_json()\n",
    "with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_10h_dataset.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Error while reading resource variable dense_75/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_75/bias)\n\t [[node dense_75/bias/Read/ReadVariableOp (defined at <ipython-input-53-149e68478e80>:89) ]]\n\nCaused by op 'dense_75/bias/Read/ReadVariableOp', defined at:\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-53-149e68478e80>\", line 89, in <module>\n    model.add(Dense(h[0], input_dim = w*len_data[1]))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 442, in _method_wrapper\n    method(self, *args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 164, in add\n    layer(x)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 538, in __call__\n    self._maybe_build(inputs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1603, in _maybe_build\n    self.build(input_shapes)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 958, in build\n    trainable=True)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 349, in add_weight\n    aggregation=aggregation)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 607, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\", line 145, in make_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2488, in default_variable_creator\n    import_scope=import_scope)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 294, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 446, in _init_from_args\n    value = self._read_variable_op()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 728, in _read_variable_op\n    self._dtype)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 605, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Error while reading resource variable dense_75/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_75/bias)\n\t [[node dense_75/bias/Read/ReadVariableOp (defined at <ipython-input-53-149e68478e80>:89) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Error while reading resource variable dense_75/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_75/bias)\n\t [[{{node dense_75/bias/Read/ReadVariableOp}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-b014bf1e5a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_10h_dataset.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_10h_dataset.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saved model to disk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[0;32m   1413\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'h5'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m         \u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1416\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m     \u001b[0msymbolic_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m     \u001b[0mweight_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    743\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   2817\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2818\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2819\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2820\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2821\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Error while reading resource variable dense_75/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_75/bias)\n\t [[node dense_75/bias/Read/ReadVariableOp (defined at <ipython-input-53-149e68478e80>:89) ]]\n\nCaused by op 'dense_75/bias/Read/ReadVariableOp', defined at:\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-53-149e68478e80>\", line 89, in <module>\n    model.add(Dense(h[0], input_dim = w*len_data[1]))\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 442, in _method_wrapper\n    method(self, *args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 164, in add\n    layer(x)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 538, in __call__\n    self._maybe_build(inputs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1603, in _maybe_build\n    self.build(input_shapes)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 958, in build\n    trainable=True)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 349, in add_weight\n    aggregation=aggregation)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 607, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\", line 145, in make_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2488, in default_variable_creator\n    import_scope=import_scope)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 294, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 446, in _init_from_args\n    value = self._read_variable_op()\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 728, in _read_variable_op\n    self._dtype)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 605, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ASUS\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Error while reading resource variable dense_75/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_75/bias)\n\t [[node dense_75/bias/Read/ReadVariableOp (defined at <ipython-input-53-149e68478e80>:89) ]]\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_10h_dataset.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_10h_dataset.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    label_numpy = a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 257), (?,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
