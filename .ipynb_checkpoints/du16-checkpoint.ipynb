{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "(684108, 257)\n",
      "(684108, 771)\n",
      "[109538 352750 553645 ... 470924 491755 128037]\n",
      "Epoch 1/10\n",
      "   6/5344 [..............................] - ETA: 12:11:26 - loss: 7.4371 - acc: 0.0076"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b95378e0f827>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;31m# ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;31m# model.fit_generator(da(batch_size, len_data[0]), steps_per_epoch=steps,epochs=50, verbose=1, callbacks = lr_callbacks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1189\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#import sounddevice as sd\n",
    "import tensorflow as tf\n",
    "from pystoi.stoi import stoi\n",
    "import h5py\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "# #######################\n",
    "# from RBM import rbm_layer\n",
    "inputfile='lilftr_refrmd'\n",
    "targetfile='lilsingle_dataset_log_16'\n",
    "hh = h5py.File(targetfile+'.hdf5', 'r')\n",
    "d=hh[targetfile]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "w=3\n",
    "print(len_data)\n",
    "hh = h5py.File(inputfile+'.hdf5', 'r')\n",
    "d=hh[inputfile]\n",
    "len_data_in=d.shape\n",
    "hh.close()\n",
    "print(len_data_in)\n",
    "# visible = w*len_data[1]\n",
    "# hidden = 2048\n",
    "# visible1 = 2048\n",
    "# hidden1 = 2048\n",
    "# visible2 = 2048\n",
    "# hidden2 = 2048\n",
    "# visible3 = 2048\n",
    "# hidden3 = len_data[1]\n",
    "# name1='ftr_scaled.hdf5'\n",
    "# name2='ftr_scaled'\n",
    "# layer1 = rbm_layer(visible, hidden, 20, 128, 0.0005, [np.eye(visible)], [np.zeros((1,visible))], 1, len_data[0], name1, name2)\n",
    "# layer2 = rbm_layer(visible1, hidden1, 20, 128, 0.0005, [np.eye(visible),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, len_data[0], name1, name2)\n",
    "# layer3 = rbm_layer(visible2, hidden2, 20, 128, 0.0005, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, len_data[0], name1, name2)\n",
    "# layer4 = rbm_layer(visible3, hidden3, 20, 128, 0.0005, [np.eye(visible),layer1[0],layer2[0],layer3[0]], [np.zeros((1,visible)),layer1[2],layer2[2],layer3[2]], 4, len_data[0], name1, name2)\n",
    "# rbm_layers=[layer1,layer2,layer3,layer4]\n",
    "# h5f = h5py.File('rbm_params.h5', 'w')\n",
    "# h5f.create_dataset('rbm_params', data=rbm_layers)\n",
    "# h5f.close()\n",
    "# np.savetxt('rbm_layers.txt',rbm_layers)\n",
    "# l10=np.loadtxt('layers10.txt')\n",
    "# l11=np.loadtxt('layers11.txt')\n",
    "# l12=np.loadtxt('layers12.txt')\n",
    "# l20=np.loadtxt('layers20.txt')\n",
    "# l21=np.loadtxt('layers21.txt')\n",
    "# l22=np.loadtxt('layers22.txt')\n",
    "# l30=np.loadtxt('layers30.txt')\n",
    "# l31=np.loadtxt('layers31.txt')\n",
    "# l32=np.loadtxt('layers32.txt')\n",
    "# l40=np.loadtxt('layers40.txt')\n",
    "# l41=np.loadtxt('layers41.txt')\n",
    "# l42=np.loadtxt('layers42.txt')\n",
    "# layer1=[l10,l11,l12]\n",
    "# layer2=[l20,l21,l22]\n",
    "# layer3=[l30,l31,l32]\n",
    "# layer4=[l40,l41,l42]\n",
    "# print('saved rbm layers')\n",
    "#######################\n",
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=200, win_length=500, window='hann')\n",
    "    return recon\n",
    "#######################\n",
    "I=0\n",
    "def da(load_size, data_len):\n",
    "    global I\n",
    "    h5f1 = h5py.File(inputfile+'.hdf5','r')\n",
    "    h5f2 = h5py.File(targetfile+'.hdf5','r')\n",
    "    indx = np.arange(0, data_len)\n",
    "    indx = shuffle(indx, random_state=1)\n",
    "#     indx = np.arange(0, data_len, load_size)\n",
    "#     indx = shuffle(indx, random_state=1)\n",
    "    print(indx)\n",
    "    while True:             #this line is just because keras needs infinite generators\n",
    "        for I in range(len(indx)): \n",
    "            if data_len-I<load_size:\n",
    "                Data1 = h5f1[inputfile][indx[I]:indx[-1]]\n",
    "                Data2 = h5f2[targetfile][indx[I]:indx[-1]]\n",
    "            else:\n",
    "                Data1 = h5f1[inputfile][indx[I]:indx[I+load_size]]\n",
    "                Data2 = h5f2[targetfile][indx[I]:indx[I+load_size]]\n",
    "#                 print(Data1)\n",
    "#                 print(Data2)\n",
    "            yield(Data1, Data2)\n",
    "            \n",
    "#     print(indx)\n",
    "#     while True:             #this line is just because keras needs infinite generators\n",
    "#         for I in indx: \n",
    "#             if data_len-I<load_size:\n",
    "#                 Data1 = h5f1['ftr_refrmd_10h'][I:]\n",
    "#                 Data2 = h5f2['single_dataset_log_16'][I:]\n",
    "#             else:\n",
    "#                 Data1 = h5f1['ftr_refrmd_10h'][I:I+load_size]\n",
    "#                 Data2 = h5f2['single_dataset_log_16'][I:I+load_size]\n",
    "#             yield(Data1, Data2)\n",
    "\n",
    "# file = h5py.File('clean_data.hdf5','r')\n",
    "# val_y = file['clean_data'][0:]\n",
    "# file.close()\n",
    "# file = h5py.File('ftr_refrmd_test.hdf5','r')\n",
    "# val_x = file['ftr_refrmd_test'][0:]\n",
    "# file.close()\n",
    "h = [1024,512]\n",
    "seed = 7\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import activations\n",
    "np.random.seed(seed)\n",
    "model = Sequential()\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer=tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])))\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1]))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(act)\n",
    "# model.add(Activation('sigmoid'))\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Dense(h[1], kernel_initializer=tf.constant_initializer(layer2[0]), bias_initializer = tf.constant_initializer(layer2[2])))\n",
    "model.add(Dense(h[1]))\n",
    "model.add(act)\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# act=keras.layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Dense(h[2], kernel_initializer=tf.constant_initializer(layer3[0]), bias_initializer = tf.constant_initializer(layer3[2])))\n",
    "# model.add(Dense(h[2]))\n",
    "# model.add(act)\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "act=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Dense(len_data[1], kernel_initializer=tf.constant_initializer(layer4[0]), bias_initializer = tf.constant_initializer(layer4[2])))\n",
    "model.add(Dense(len_data[1]))\n",
    "# model.add(act)\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# adam_opt = optimizers.adam(lr=0.1, decay=0)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "batch_size=128\n",
    "steps = len_data[0] // batch_size\n",
    "# def lr_scheduler(epoch, lr):\n",
    "#     decay_rate = 0.9\n",
    "#     if epoch>10:\n",
    "#         return lr * decay_rate\n",
    "#     return lr\n",
    "\n",
    "# lr_callbacks = [\n",
    "#     keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "# ]\n",
    "# model.fit_generator(da(batch_size, len_data[0]), steps_per_epoch=steps,epochs=50, verbose=1, callbacks = lr_callbacks)\n",
    "history=model.fit_generator(da(batch_size, len_data[0]), steps_per_epoch=steps, epochs=10, verbose=1)\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "model_json = model.to_json()\n",
    "with open(\"lilmodel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"lilmodel.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "model_json = model.to_json()\n",
    "with open(\"model_10h.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_10h.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7183134, 257)\n"
     ]
    }
   ],
   "source": [
    "hh = h5py.File('single_dataset_log_16.hdf5', 'r')\n",
    "d=hh['single_dataset_log_16']\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "print(len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0:500]\n",
    "data=d\n",
    "hh.close()\n",
    "# d=tf.convert_to_tensor(d,tf.float32)\n",
    "print(type(d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08651759,  0.22336993,  0.44741386,  0.92277074,  0.8565611 ,\n",
       "       -0.21097986,  0.01482241, -0.31860858, -0.69763505, -1.1260797 ,\n",
       "       -1.3145506 , -1.6120622 , -0.8895558 , -1.2584125 , -0.5357813 ,\n",
       "       -0.5961562 , -1.5157962 , -1.0339916 , -1.6847466 , -1.2978625 ,\n",
       "       -1.5814272 , -0.6983488 , -0.72219944, -2.035673  , -0.7506092 ,\n",
       "       -2.5336158 , -0.52485573, -0.5650042 , -1.1807766 , -0.42296278,\n",
       "       -0.8960844 , -2.416711  , -1.2472986 , -1.0052608 , -0.85797095,\n",
       "       -0.62860066, -0.05113654, -0.21714081,  0.0267174 , -0.5353598 ,\n",
       "       -1.9117582 , -1.4858161 , -1.3420835 , -0.76118916, -0.9266766 ,\n",
       "       -2.8992896 , -2.095491  , -0.7868872 , -2.2056518 , -0.985487  ,\n",
       "       -1.4708126 , -2.0216296 , -0.98144644, -1.1434047 , -1.995234  ,\n",
       "       -1.7717012 , -3.1965823 , -1.4338489 , -1.2426492 , -1.4553313 ,\n",
       "       -1.7585936 , -1.7883971 , -1.9998507 , -2.7107284 , -2.3896973 ,\n",
       "       -2.510616  , -2.3158517 , -1.8880926 , -1.8142247 , -1.5310968 ,\n",
       "       -1.5505652 , -1.7525783 , -1.7155819 , -2.6995323 , -2.1542099 ,\n",
       "       -2.8097415 , -2.9314895 , -2.4081826 , -2.3292594 , -3.2324138 ,\n",
       "       -2.3557808 , -2.623616  , -2.6206188 , -2.2355993 , -1.8272797 ,\n",
       "       -1.7533708 , -2.103177  , -1.9587902 , -1.8394071 , -2.0833766 ,\n",
       "       -2.0405283 , -2.2505684 , -2.502323  , -3.130103  , -3.3031301 ,\n",
       "       -2.9811673 , -2.969848  , -2.4992208 , -2.7205513 , -2.5740576 ,\n",
       "       -2.4388764 , -3.1576583 , -3.803427  , -3.3811498 , -2.9848044 ,\n",
       "       -3.0627556 , -2.313945  , -2.9008427 , -4.4764643 , -3.1985762 ,\n",
       "       -2.9095702 , -2.6433218 , -3.900545  , -3.0097353 , -2.8902245 ,\n",
       "       -2.2862785 , -2.4386082 , -3.4372644 , -3.152547  , -2.6984003 ,\n",
       "       -2.9469635 , -2.930965  , -3.1975944 , -4.909848  , -3.3674252 ,\n",
       "       -3.2249484 , -4.5693574 , -5.2479844 , -3.1054478 , -2.8300784 ,\n",
       "       -3.2206635 , -2.993438  , -2.6965063 , -2.3262103 , -2.1500258 ,\n",
       "       -3.0771704 , -2.220333  , -2.332287  , -3.342485  , -2.656542  ,\n",
       "       -3.086965  , -2.8588612 , -3.0131524 , -2.1946697 , -2.595334  ,\n",
       "       -3.1729336 , -2.270293  , -3.0706327 , -2.4531143 , -3.1099966 ,\n",
       "       -2.94397   , -3.3215737 , -2.7887964 , -3.289245  , -3.2000992 ,\n",
       "       -2.555651  , -2.9476821 , -3.1403456 , -3.0001242 , -3.834921  ,\n",
       "       -4.141873  , -3.7440155 , -3.2837477 , -3.144194  , -3.215179  ,\n",
       "       -2.8389947 , -4.225783  , -3.906956  , -4.2401505 , -3.6488729 ,\n",
       "       -3.9692373 , -3.8648396 , -3.5871146 , -3.3437133 , -3.3315434 ,\n",
       "       -4.241883  , -4.7405376 , -3.7936454 , -3.8020084 , -3.45198   ,\n",
       "       -3.9439557 , -4.31398   , -3.9694521 , -3.4781814 , -3.5533354 ,\n",
       "       -3.3830187 , -3.4415715 , -3.9944932 , -5.0108614 , -4.3478208 ,\n",
       "       -4.1680145 , -3.244851  , -2.824113  , -3.1537166 , -3.649742  ,\n",
       "       -3.2307901 , -3.8972216 , -3.2215004 , -3.4623935 , -4.0452185 ,\n",
       "       -3.7369509 , -4.9622736 , -3.1065247 , -3.355665  , -3.8317664 ,\n",
       "       -3.5281456 , -2.9975443 , -3.3804867 , -3.2567003 , -2.6161292 ,\n",
       "       -2.878129  , -3.0455701 , -2.8290598 , -2.369071  , -2.2958918 ,\n",
       "       -2.1989002 , -2.3191469 , -3.061786  , -2.3818464 , -2.7695062 ,\n",
       "       -3.2719913 , -2.50439   , -2.2177298 , -2.7554965 , -2.5743325 ,\n",
       "       -3.010111  , -3.4662964 , -3.1744351 , -3.1710973 , -3.8338306 ,\n",
       "       -2.8989983 , -3.1588335 , -2.8894918 , -2.1629834 , -3.5939667 ,\n",
       "       -2.7932777 , -2.5659697 , -2.8088605 , -2.5204597 , -2.8362772 ,\n",
       "       -2.7269297 , -2.4092858 , -2.1971638 , -2.2280455 , -2.4520452 ,\n",
       "       -3.3085964 , -3.4368994 , -2.783938  , -3.5668538 , -4.4355917 ,\n",
       "       -3.4132628 , -3.8883092 , -3.5258987 , -4.002568  , -3.62588   ,\n",
       "       -3.2363966 , -2.8264663 ,  0.6358349 ,  0.95956635,  0.850665  ,\n",
       "        1.8166697 ,  2.3288014 ,  1.9097763 ,  0.3813621 ,  0.8688599 ,\n",
       "        1.182464  ,  0.9941046 ,  0.3650873 , -0.6870473 ,  0.27861494,\n",
       "        0.48588985, -0.08212745, -0.5937174 , -0.56092733, -1.1527997 ,\n",
       "       -1.1207364 , -0.91790134, -1.1692792 , -1.6056894 , -2.5344272 ,\n",
       "       -1.7379425 , -2.1448085 , -2.0009425 , -1.6618534 , -2.7496119 ,\n",
       "       -3.6624985 , -2.3728282 , -1.6764055 , -2.530418  , -1.6746567 ,\n",
       "       -1.6723028 , -1.2823739 , -1.1218835 , -1.4642068 , -0.58835906,\n",
       "       -0.2573774 , -0.39841646, -0.31413308, -0.26985884, -0.93839115,\n",
       "       -0.37039015, -0.18842605, -0.5129771 , -0.45386454, -0.24947381,\n",
       "       -0.4292941 , -0.6371025 , -0.6364993 , -0.7676488 , -1.1887629 ,\n",
       "       -1.5707607 , -1.559905  , -2.1719713 , -1.5203093 , -2.1884773 ,\n",
       "       -1.322612  , -1.2340221 , -1.9083251 , -1.6989112 , -1.5490341 ,\n",
       "       -1.6890358 , -1.6199641 , -1.8200309 , -2.042552  , -1.5334779 ,\n",
       "       -1.8892136 , -1.7592076 , -1.6170667 , -1.8635665 , -2.7743292 ,\n",
       "       -2.5827188 , -2.4762142 , -2.1584096 , -2.4969282 , -2.9360054 ,\n",
       "       -2.549838  , -2.6349509 , -2.693239  , -4.0421977 , -2.306228  ,\n",
       "       -2.8094268 , -2.4685078 , -2.835504  , -2.681125  , -3.043146  ,\n",
       "       -2.370639  , -2.4235108 , -1.7078887 , -1.7331712 , -2.5390244 ,\n",
       "       -1.5965544 , -1.5621486 , -2.0614688 , -2.3024235 , -1.7245864 ,\n",
       "       -1.8587489 , -1.8742712 , -2.012363  , -2.7769556 , -2.5012865 ,\n",
       "       -2.4457216 , -2.9633362 , -2.488374  , -2.788227  , -3.5675411 ,\n",
       "       -3.5660677 , -2.169616  , -1.9512668 , -2.1057224 , -1.6908064 ,\n",
       "       -2.0641987 , -2.6561165 , -1.8840127 , -1.5396329 , -1.7497504 ,\n",
       "       -2.2916834 , -1.9016055 , -2.815481  , -2.9466517 , -2.8151171 ,\n",
       "       -2.7666218 , -2.8312826 , -3.1273384 , -2.082881  , -2.088301  ,\n",
       "       -3.3846254 , -2.5757132 , -2.7225726 , -2.8853023 , -2.4170113 ,\n",
       "       -3.4666922 , -3.4527066 , -3.3405304 , -2.6520963 , -2.9743624 ,\n",
       "       -2.4105809 , -2.5370002 , -2.7591624 , -2.8867247 , -2.584679  ,\n",
       "       -1.8805563 , -2.6293945 , -2.1686723 , -2.4117029 , -3.776397  ,\n",
       "       -2.4532485 , -2.418748  , -2.1653945 , -2.871368  , -3.6455767 ,\n",
       "       -2.9298267 , -2.639384  , -3.8505213 , -3.4492013 , -2.9138503 ,\n",
       "       -3.280468  , -2.7202985 , -2.520146  , -3.5181856 , -3.9892616 ,\n",
       "       -2.7590036 , -2.5771751 , -2.8452399 , -2.9407582 , -2.7840943 ,\n",
       "       -3.384634  , -2.9012449 , -3.0392678 , -3.3323221 , -2.5255392 ,\n",
       "       -2.5970047 , -2.9497967 , -2.4195127 , -2.4441817 , -2.736283  ,\n",
       "       -3.9193144 , -2.697123  , -2.8974288 , -3.3627532 , -5.108206  ,\n",
       "       -3.1530387 , -2.7026486 , -3.139243  , -3.3318567 , -2.7544487 ,\n",
       "       -2.896391  , -3.8889353 , -3.1453485 , -3.023167  , -3.2362573 ,\n",
       "       -3.7841463 , -4.007997  , -3.3968267 , -4.2023525 , -3.138108  ,\n",
       "       -3.1374671 , -3.8284998 , -3.0242484 , -2.7055547 , -3.606057  ,\n",
       "       -3.8492475 , -2.8706214 , -2.8894963 , -3.5396059 , -2.7214282 ,\n",
       "       -2.4511957 , -2.6989708 , -2.7233949 , -3.186148  , -3.7403104 ,\n",
       "       -3.6310627 , -3.4069238 , -3.8114057 , -3.2536936 , -2.1850235 ,\n",
       "       -1.7535174 , -2.0237164 , -2.777928  , -2.4990206 , -2.0698295 ,\n",
       "       -2.4716785 , -3.355653  , -3.7886553 , -3.9620974 , -2.7091703 ,\n",
       "       -2.7028306 , -2.909605  , -2.38477   , -2.5041237 , -2.9207432 ,\n",
       "       -3.5467389 , -4.0649667 , -3.3014061 , -4.1274276 , -3.294675  ,\n",
       "       -3.6238203 , -4.072887  , -3.350788  , -4.1764736 , -3.8097103 ,\n",
       "       -2.7720313 , -2.8665192 , -3.1795464 , -2.7034333 , -3.506694  ,\n",
       "       -2.9593372 , -2.9532185 , -2.7915633 , -3.0965228 , -3.5483866 ,\n",
       "       -2.81403   , -3.1554086 , -4.0212154 , -5.1852217 ,  1.4714587 ,\n",
       "        1.0831156 ,  1.0834364 ,  2.5630035 ,  2.973536  ,  2.2521439 ,\n",
       "        0.7832329 ,  2.2161443 ,  2.4651766 ,  1.769097  ,  1.1832381 ,\n",
       "        2.1658292 ,  2.3548415 ,  1.8675891 ,  1.0492642 ,  0.75036687,\n",
       "        0.6499073 ,  0.33257776, -0.12531252, -0.9281799 , -0.4648606 ,\n",
       "       -0.06860051, -0.3428464 , -1.3134394 , -0.9736014 , -0.32201818,\n",
       "       -0.37388176, -0.91168   , -0.654234  , -0.38246176, -0.5518    ,\n",
       "       -1.3471677 , -0.35034564,  0.10902573, -0.12408573, -0.898281  ,\n",
       "       -0.4937936 ,  0.05549508, -0.24215332, -1.2853221 , -1.1544385 ,\n",
       "       -0.4622587 , -0.13492388, -0.31792822, -0.62245315, -0.3342664 ,\n",
       "       -0.18764454, -0.62892157, -0.9651712 , -1.1233441 , -1.5221685 ,\n",
       "       -1.874932  , -2.6819558 , -1.9062644 , -1.5255027 , -1.5539601 ,\n",
       "       -1.2896401 , -1.1164336 , -1.2906961 , -1.3794537 , -1.343382  ,\n",
       "       -1.7110724 , -1.2668182 , -1.2143663 , -1.1908383 , -2.1139872 ,\n",
       "       -1.9583807 , -1.1147821 , -0.80846524, -1.3219517 , -1.6687496 ,\n",
       "       -1.1366808 , -1.5066124 , -2.1354954 , -1.7340112 , -1.6875353 ,\n",
       "       -2.141165  , -1.853745  , -1.7454944 , -2.0416608 , -2.1780684 ,\n",
       "       -1.5691433 , -1.1691657 , -1.38686   , -1.7358898 , -1.5747446 ,\n",
       "       -0.85745406, -0.7407376 , -1.3717102 , -1.7443281 , -1.2080842 ,\n",
       "       -1.5028149 , -1.6738133 , -1.8352845 , -1.5317951 , -1.2872262 ,\n",
       "       -1.3539692 , -1.6304977 , -2.0633628 , -1.8735002 , -1.6094728 ,\n",
       "       -2.033994  , -2.454252  , -3.1342916 , -2.3932097 , -2.3500252 ,\n",
       "       -2.644103  , -3.5888155 , -3.1600356 , -2.3309026 , -2.4245195 ,\n",
       "       -2.7680607 , -3.2211597 , -1.9066072 , -2.0718007 , -1.3768303 ,\n",
       "       -1.6187133 , -1.4481896 , -1.5521367 , -1.4903036 , -1.854708  ,\n",
       "       -2.5460522 , -2.6812735 , -2.5093315 , -2.9713082 , -1.9238513 ,\n",
       "       -2.8955772 , -1.4630148 , -1.6518881 , -2.6661255 , -2.9899137 ,\n",
       "       -1.4673834 , -1.7747715 , -2.1502218 , -1.8948622 , -2.3307865 ,\n",
       "       -1.5081815 , -2.2615116 , -0.9055914 , -1.0505689 , -1.1440275 ,\n",
       "       -0.79262584, -0.86359763, -0.81679714, -1.3414692 , -1.2843488 ,\n",
       "       -1.234936  , -1.4177506 , -1.3905056 , -1.771647  , -2.1067984 ,\n",
       "       -2.2272508 , -2.3018363 , -2.1548178 , -2.212851  , -2.7925491 ,\n",
       "       -2.8028922 , -2.0780444 , -2.0709214 , -2.6185868 , -2.0520582 ,\n",
       "       -2.6609395 , -4.0353584 , -4.31075   , -2.806587  , -2.1001325 ,\n",
       "       -2.3664584 , -2.549034  , -2.2888563 , -2.27171   , -2.30618   ,\n",
       "       -3.3252344 , -3.2484875 , -2.5245092 , -1.8160727 , -1.9230446 ,\n",
       "       -2.4937758 , -2.8532238 , -2.086771  , -2.3328521 , -3.202837  ,\n",
       "       -4.0225816 , -3.9918249 , -3.2298577 , -3.0962822 , -3.9138846 ,\n",
       "       -3.4163969 , -2.571321  , -2.3842056 , -2.65757   , -3.4285996 ,\n",
       "       -3.3633177 , -2.7481725 , -3.0199966 , -3.279096  , -2.851871  ,\n",
       "       -2.8931525 , -2.795173  , -2.6737888 , -2.905662  , -3.4489856 ,\n",
       "       -3.551619  , -2.8763494 , -3.0650342 , -3.048171  , -2.845647  ,\n",
       "       -2.9701834 , -3.8732243 , -3.8447347 , -3.366742  , -2.8247652 ,\n",
       "       -2.7844582 , -2.7556453 , -2.6254153 , -3.1548715 , -3.3418686 ,\n",
       "       -3.041568  , -2.7049727 , -2.7207124 , -3.2153258 , -3.2732074 ,\n",
       "       -4.4932027 , -3.3744807 , -3.0678525 , -3.387631  , -3.1174016 ,\n",
       "       -2.607171  , -2.5271173 , -2.4486697 , -2.7667286 , -3.8653662 ,\n",
       "       -3.012652  , -2.8899164 , -2.8674643 , -3.5854726 , -3.5558105 ,\n",
       "       -2.8964949 , -2.6397092 , -2.5853512 , -2.9707384 , -5.073891  ,\n",
       "       -3.1645265 , -3.7000866 , -3.1885393 , -3.0206938 , -3.0930672 ,\n",
       "       -4.732637  , -2.5893142 , -2.4071608 , -2.341166  , -2.771406  ,\n",
       "       -3.501545  , -3.217686  , -2.5499585 , -2.8227675 , -3.13639   ,\n",
       "       -4.7016478 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13692, 257)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('layers10.txt',layer1[0])\n",
    "np.savetxt('layers11.txt',layer1[1])\n",
    "np.savetxt('layers12.txt',layer1[2])\n",
    "np.savetxt('layers20.txt',layer2[0])\n",
    "np.savetxt('layers21.txt',layer2[1])\n",
    "np.savetxt('layers22.txt',layer2[2])\n",
    "np.savetxt('layers30.txt',layer3[0])\n",
    "np.savetxt('layers31.txt',layer3[1])\n",
    "np.savetxt('layers32.txt',layer3[2])\n",
    "np.savetxt('layers40.txt',layer4[0])\n",
    "np.savetxt('layers41.txt',layer4[1])\n",
    "np.savetxt('layers42.txt',layer4[2])\n",
    "\n",
    "\n",
    "# loadit=np.loadtxt('rbm_layers.txt')\n",
    "# rbm_layers.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights[m].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "import sounddevice as sd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from RBM import rbm_layer\n",
    "visible = 7*target.shape[1]\n",
    "hidden = 2048\n",
    "visible1 = 2048\n",
    "hidden1 = 2048\n",
    "visible2 = 2048\n",
    "hidden2 = 2048\n",
    "visible3 = 2048\n",
    "hidden3 = target.shape[1]\n",
    "name1='ftr_scaled.hdf5'\n",
    "name2='ftr_scaled'\n",
    "layer1 = rbm_layer(visible, hidden, 20, 128, 0.0005, [np.eye(visible)], [np.zeros((1,visible))], 1, target.shape[0], name1, name2)\n",
    "layer2 = rbm_layer(visible1, hidden1, 20, 128, 0.0005, [np.eye(visible),layer1[0]], [np.zeros((1,visible)),layer1[2]], 2, target.shape[0], name1, name2)\n",
    "layer3 = rbm_layer(visible2, hidden2, 20, 128, 0.0005, [np.eye(visible),layer1[0],layer2[0]], [np.zeros((1,visible)),layer1[2],layer2[2]], 3, target.shape[0], name1, name2)\n",
    "layer4 = rbm_layer(visible3, hidden3, 20, 128, 0.0005, [np.eye(visible),layer1[0],layer2[0],layer3[0]], [np.zeros((1,visible)),layer1[2],layer2[2],layer3[2]], 4, target.shape[0], name1, name2)\n",
    "rbm_layers=[layer1,layer2,layer3,layer4]\n",
    "h5f = h5py.File('rbm_params.h5', 'w')\n",
    "h5f.create_dataset('rbm_params', data=rbm_layers)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define reconstruct function to reconstruct sound from framed signal.\n",
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=200, win_length=500, window='hann')\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=0\n",
    "def da(batch_size, data_len):\n",
    "    global I\n",
    "    while True:             #this line is just because keras needs infinite generators\n",
    "        for I in range(0,data_len-batch_size,batch_size): \n",
    "            h5f = h5py.File('ftr_scaled.hdf5','r')\n",
    "            Data1 = h5f['ftr_scaled'][I:I+batch_size]\n",
    "            h5f.close()\n",
    "            h5f = h5py.File('single_dataset_log_16.hdf5','r')\n",
    "            Data2 = h5f['single_dataset_log_16'][I:I+batch_size]\n",
    "            h5f.close()\n",
    "            yield(Data1, Data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 2s - loss: 17.01 - 1s 76ms/step - loss: 20.6106\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.11 - 0s 8ms/step - loss: 15.3262\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 9ms/step - loss: 15.3232\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.09000000134110452.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.08100000321865082.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.07290000021457672.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.06560999751091004.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.05904899910092354.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.053144099190831184.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.04782968759536743.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.04304671883583069.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.03874204829335213.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.034867842122912406.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.03138105757534504.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.028242950141429902.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.02541865445673466.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.022876788675785065.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.020589109137654306.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.018530198559165.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.016677179373800755.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.015009460598230362.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.013508514873683453.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.012157663051038981.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.010941896494477988.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.009847706928849221.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 9ms/step - loss: 15.3232\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.008862936403602362.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.007976643182337284.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.007178978528827429.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.006461080675944686.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.0058149725664407015.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.005233475100249053.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.0047101275064051155.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 8ms/step - loss: 15.3232\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.004239114839583636.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.0038152034394443035.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.003433683095499873.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.0030903148697689177.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.002781283319927752.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.002503155008889735.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.002252839528955519.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 7ms/step - loss: 15.3232\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.0020275555551052095.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 9ms/step - loss: 15.3232\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.0018248000415042043.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 9ms/step - loss: 15.3232\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.001642320037353784.\n",
      "7/7 [==============================] - ETA: 0s - loss: 16.09 - 0s 9ms/step - loss: 15.3232\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#training data, which contains 750000 frames.\n",
    "# X = refr_ftr\n",
    "# y = target\n",
    "h = [2048,2048,2048]\n",
    "#test frames, which is one frame to get spectrograms from.\n",
    "#another evaluation is doe lated in the code which computes \n",
    "#stoi and sdr.\n",
    "# X_test = refr_ftr[750000:750000+157]\n",
    "# Y_test = target[750000:750000+157]\n",
    "# angle = phase[750000:750000+157]\n",
    "\n",
    "\n",
    "# sparsity_list = []\n",
    "# zero_nlist = []\n",
    "# reconlist = []\n",
    "# Times = []\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = 7*target.shape[1], kernel_initializer=tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(h[1], kernel_initializer=tf.constant_initializer(layer2[0]), bias_initializer = tf.constant_initializer(layer2[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(h[2], kernel_initializer=tf.constant_initializer(layer3[0]), bias_initializer = tf.constant_initializer(layer3[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(target.shape[1], kernel_initializer=tf.constant_initializer(layer4[0]), bias_initializer = tf.constant_initializer(layer4[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dense(h[0], input_dim = 7*target.shape[1], activation='relu'))\n",
    "# model.add(Dense(h[1], activation='relu'))\n",
    "# model.add(Dense(h[2], activation='relu'))\n",
    "# model.add(Dense(target.shape[1], activation='relu'))\n",
    "# model.add(Dense(target.shape[1], kernel_initializer='normal'))\n",
    "# Compile model\n",
    "adam_opt = optimizers.adam(lr=0.1, decay=0)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam_opt)\n",
    "#estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=150, verbose=1)\n",
    "#kfold = KFold(n_splits=5, random_state=seed)\n",
    "#results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "#print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "#estimator.fit(X, y)\n",
    "batch_size=128\n",
    "steps = target.shape[0] // batch_size\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    if epoch>10:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "lr_callbacks = [\n",
    "    keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "]\n",
    "model.fit_generator(da(batch_size, target.shape[0]), steps_per_epoch=steps,epochs=50, verbose=1, callbacks = lr_callbacks)\n",
    "#prediction = model.predict(X_test)\n",
    "# serialize model to JSON\n",
    "#write and save model.\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "# #reconstruct the predicted output from network.\n",
    "# recon = reconstruct(prediction,angle)\n",
    "# reconlist.append(recon)\n",
    "# #get weights from model.\n",
    "# w0 = estimator.model.get_weights()[0]\n",
    "# w1 = estimator.model.get_weights()[2]\n",
    "# w2 = estimator.model.get_weights()[4]\n",
    "# w=[w0,w1,w2]\n",
    "# #determine sparsity and number of zero neurons.\n",
    "# nonzeros = np.nonzero(w[0])[0].shape + np.nonzero(w[1])[0].shape + np.nonzero(w[2])[0].shape \n",
    "# sparsity = 1 - np.sum(nonzeros)/(refr_ftr.shape[1]*h[0]+h[0]*h[1]+h[1]*target.shape[1])\n",
    "# sparsity_list.append(sparsity)\n",
    "# zero_n = get_zerorows(w)\n",
    "# zero_nlist.append(zero_n)\n",
    "# #save and write necessary date.\n",
    "# np.savetxt('recon.txt',reconlist)\n",
    "# np.savetxt('sparsity_list.txt',sparsity_list)\n",
    "# np.savetxt('zero_nlist.txt',zero_nlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play output voice which is the output of a network with no regularizer.\n",
    "sd.play(reconlist[0]*10, 16000)\n",
    "sf.write(('unseen_None.wav'), reconlist[0]*10, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4193.883036136627,\n",
       " 4849.418083667755,\n",
       " 4886.912912130356,\n",
       " 5412.886659860611,\n",
       " 6004.0426461696625]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savetxt('runtimes.txt',Times)\n",
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play output voice which is the output of a network with l2 regularizer.\n",
    "sd.play(reconlist[1]*10, 16000)\n",
    "sf.write(('unseen_l2.wav'), reconlist[1]*10, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play output voice which is the output of a network with l1 regularizer.\n",
    "sd.play(reconlist[2] *10, 16000)\n",
    "sf.write(('unseen_l1.wav'), reconlist[2]*10, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play output voice which is the output of a network with group regularizer.\n",
    "sd.play(reconlist[3] *10, 16000)\n",
    "sf.write(('unseen_group.wav'), reconlist[3]*10, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play output voice which is the output of a network with SGL regularizer.\n",
    "sd.play(reconlist[4] *10, 16000)\n",
    "sf.write(('unseen_SGL.wav'), reconlist[4]*10, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('model0.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model0.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_clean = reconstruct(Y_test, angle)\n",
    "recon_mixed = reconstruct(X_test, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data.\n",
    "sf.write('unseen_mixed.wav', recon_mixed, 16000)\n",
    "sf.write('unseen_clean.wav', recon_clean, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(recon_mixed, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Loaded model from disk\n",
      "Loaded model from disk\n",
      "Loaded model from disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file_none = open('model0.json', 'r')\n",
    "loaded_model_json_none = json_file_none.read()\n",
    "json_file_none.close()\n",
    "loaded_model_none = model_from_json(loaded_model_json_none)\n",
    "# load weights into new model\n",
    "loaded_model_none.load_weights(\"model0.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "json_file_l2 = open('model1.json', 'r')\n",
    "loaded_model_json_l2 = json_file_l2.read()\n",
    "json_file_l2.close()\n",
    "loaded_model_l2 = model_from_json(loaded_model_json_l2)\n",
    "# load weights into new model\n",
    "loaded_model_l2.load_weights(\"model1.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "json_file_l1 = open('model2.json', 'r')\n",
    "loaded_model_json_l1 = json_file_l1.read()\n",
    "json_file_l1.close()\n",
    "loaded_model_l1 = model_from_json(loaded_model_json_l1)\n",
    "# load weights into new model\n",
    "loaded_model_l1.load_weights(\"model2.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "json_file_group = open('model3.json', 'r')\n",
    "loaded_model_json_group = json_file_group.read()\n",
    "json_file_group.close()\n",
    "loaded_model_group = model_from_json(loaded_model_json_group)\n",
    "# load weights into new model\n",
    "loaded_model_group.load_weights(\"model3.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "json_file_sgl = open('model4.json', 'r')\n",
    "loaded_model_json_sgl = json_file_sgl.read()\n",
    "json_file_sgl.close()\n",
    "loaded_model_sgl = model_from_json(loaded_model_json_sgl)\n",
    "# load weights into new model\n",
    "loaded_model_sgl.load_weights(\"model4.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights=[loaded_model_none, loaded_model_l2, loaded_model_l1, loaded_model_group, loaded_model_sgl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for model no reg\n",
      "sparsity:\n",
      "0.00012747813090685511\n",
      "zero neurons:\n",
      "0\n",
      "prediction stoi: \n",
      "0.8502018781658421\n",
      "mixed stoi: \n",
      "0.761798788924194\n",
      "prediction sdr: \n",
      "6.369405273040033\n",
      "mixed sdr: \n",
      "3.935131239280896\n",
      "-------------------------------------------\n",
      "results for model l2\n",
      "sparsity:\n",
      "0.5703635324629655\n",
      "zero neurons:\n",
      "0\n",
      "prediction stoi: \n",
      "0.837658878935219\n",
      "mixed stoi: \n",
      "0.761798788924194\n",
      "prediction sdr: \n",
      "6.235915937865186\n",
      "mixed sdr: \n",
      "3.935131239280896\n",
      "-------------------------------------------\n",
      "results for model l1\n",
      "sparsity:\n",
      "0.051153896874587934\n",
      "zero neurons:\n",
      "0\n",
      "prediction stoi: \n",
      "0.7041262565103249\n",
      "mixed stoi: \n",
      "0.761798788924194\n",
      "prediction sdr: \n",
      "1.4882448031481184\n",
      "mixed sdr: \n",
      "3.935131239280896\n",
      "-------------------------------------------\n",
      "results for model group\n",
      "sparsity:\n",
      "0.6151918765660029\n",
      "zero neurons:\n",
      "0\n",
      "prediction stoi: \n",
      "0.7120733166330397\n",
      "mixed stoi: \n",
      "0.761798788924194\n",
      "prediction sdr: \n",
      "1.5681190905828215\n",
      "mixed sdr: \n",
      "3.935131239280896\n",
      "-------------------------------------------\n",
      "results for model sgl\n",
      "sparsity:\n",
      "0.006584904830981619\n",
      "zero neurons:\n",
      "0\n",
      "prediction stoi: \n",
      "0.8286815221334671\n",
      "mixed stoi: \n",
      "0.761798788924194\n",
      "prediction sdr: \n",
      "5.874709135882514\n",
      "mixed sdr: \n",
      "3.935131239280896\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#in this part the weights are modified so that if a weight is less than a threshold its zeroed out.\n",
    "from pystoi.stoi import stoi\n",
    "model_names=['no reg','l2','l1','group','sgl']\n",
    "for m in range(len(model_weights)):\n",
    "    print(\"results for model \"+model_names[m])\n",
    "    weights = model_weights[m].get_weights()\n",
    "    sdr_mixed=[]\n",
    "    sdr=[]\n",
    "    stoi_mixed=[]\n",
    "    stoi_eval = []\n",
    "    zeronr = 0\n",
    "    for i in range(0,5,2):\n",
    "        for j in range(len(weights[i])):\n",
    "            for k in range(len(weights[i][j])):\n",
    "                if np.abs(weights[i][j][k]) < 1e-5:\n",
    "                    weights[i][j][k]=0\n",
    "            temp = np.nonzero(weights[i][j])[0].shape\n",
    "            if temp == weights[i].shape[1]:\n",
    "                #if all weights from a neuron are all zero, it means that the neuron is omitted.\n",
    "                zeronr += 1\n",
    "    nonzeros = np.nonzero(weights[0])[0].shape + np.nonzero(weights[2])[0].shape + np.nonzero(weights[4])[0].shape \n",
    "    sparsity = 1 - np.sum(nonzeros)/(ftr.shape[1]*h[0]+h[0]*h[1]+h[1]*target.shape[1])\n",
    "    print(\"sparsity:\")\n",
    "    print(sparsity)\n",
    "    print(\"zero neurons:\")\n",
    "    print(zeronr)\n",
    "    #100 voices are evaluated using SDR and STOI, and its averaged.\n",
    "    for i in range(800000,800000+100*157,157):\n",
    "        recon_mixed = reconstruct(X_test,angle)\n",
    "        recon_clean = reconstruct(Y_test,angle)\n",
    "        X_test = ftr[i:i+157]\n",
    "        Y_test = target[i:i+157]\n",
    "        prediction = model_weights[m].predict(X_test)\n",
    "        recon = reconstruct(prediction, angle)\n",
    "        sdr.append(bss_eval_sources(recon_clean, recon, compute_permutation=True)[0][0])\n",
    "        stoi_eval.append(stoi(recon_clean, recon, 16000, extended=False))\n",
    "        sdr_mixed.append(bss_eval_sources(recon_clean, recon_mixed, compute_permutation=True)[0][0])\n",
    "        stoi_mixed.append(stoi(recon_clean, recon_mixed, 16000, extended=False))\n",
    "    print('prediction stoi: ')\n",
    "    print(np.mean(np.asarray(stoi_eval)))\n",
    "    print('mixed stoi: ')\n",
    "    print(np.mean(np.asarray(stoi_mixed)))\n",
    "    print('prediction sdr: ')\n",
    "    print(np.mean(np.asarray(sdr)))\n",
    "    print('mixed sdr: ')\n",
    "    print(np.mean(np.asarray(sdr_mixed)))\n",
    "    print('-------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10541639, -0.11203337, -0.2028024 , ..., -0.14105639,\n",
       "        -0.02640898, -0.26741874],\n",
       "       [-0.31538433, -0.87092817,  0.15045519, ...,  0.08250466,\n",
       "         0.21377747,  0.23154591],\n",
       "       [-0.37044713, -2.1276214 , -0.7767756 , ..., -1.5462991 ,\n",
       "         0.02828818, -0.40663916],\n",
       "       ...,\n",
       "       [ 0.217184  ,  0.3744303 , -0.25406075, ...,  0.30313197,\n",
       "        -0.10129988, -0.47639948],\n",
       "       [ 0.37956175,  0.49257466,  0.11651181, ...,  0.40288913,\n",
       "        -0.05044101,  0.20564316],\n",
       "       [ 0.30799803,  0.6735903 ,  0.00437123, ...,  0.70599926,\n",
       "        -0.0634295 ,  0.5263629 ]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see first layer weights here.\n",
    "weights_none = loaded_model_none.get_weights()\n",
    "weights_none[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.2748081e-02,  3.6429066e-02,  5.6046619e-33, ...,\n",
       "        -7.4428597e-34,  2.3167983e-03,  2.2266493e-33],\n",
       "       [-6.8101035e-03,  4.3759957e-02, -8.9172193e-33, ...,\n",
       "        -7.2778955e-33, -1.0185386e-01,  2.4851479e-33],\n",
       "       [ 4.0847931e-02,  5.2484374e-02,  2.1118670e-34, ...,\n",
       "        -1.0118736e-33, -6.0549568e-02, -5.8096026e-34],\n",
       "       ...,\n",
       "       [ 4.0277254e-02, -1.0770088e-02,  3.3916663e-33, ...,\n",
       "         2.1351273e-33, -2.3281876e-02,  4.1201191e-33],\n",
       "       [-1.8546725e-02,  8.2013439e-03,  3.4341138e-33, ...,\n",
       "         2.4918140e-33,  5.2734144e-02,  4.2327105e-33],\n",
       "       [-7.3502056e-02, -5.4999903e-02,  3.4387225e-33, ...,\n",
       "         2.3236206e-33,  2.0648794e-02,  4.7854265e-33]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see first layer weights here.\n",
    "#before zeroing\n",
    "weights_l2 = loaded_model_l2.get_weights()\n",
    "weights_l2[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
