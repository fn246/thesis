{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the features for mixed voices are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import csv\n",
    "import math as m\n",
    "from natsort import natsorted\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paths are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path=os.getcwd()\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/devdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_dev_sp1'))\n",
    "clean_raw = 'clean_dev_sp1'\n",
    "w=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mixed_log_30h and mixed_phase_30h are the log and phase of mixed files with some zero elements in between to make the ftr_refrmd file which is the same thing but with context.\n",
    "\n",
    "they're both stored as h5py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = os.listdir(os.path.join(mixed_folder))\n",
    "a = natsorted(a)\n",
    "\n",
    "mixed_logname = 'mixed_log_dev'\n",
    "mixed_phasename = 'mixed_phase_dev'\n",
    "mixed_logpath = os.path.normpath(os.path.join(write_path,mixed_logname))\n",
    "mixed_phasepath = os.path.normpath(os.path.join(write_path,mixed_phasename))\n",
    "\n",
    "f = h5py.File(mixed_logpath+'.hdf5', 'w')\n",
    "d = f.create_dataset(mixed_logname, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "ff = h5py.File(mixed_phasepath+'.hdf5', 'w')\n",
    "dd = ff.create_dataset(mixed_phasename, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "for filename in a:\n",
    "    wav_data, sr = librosa.load(os.path.join(mixed_folder,filename), sr=16000) \n",
    "    framed_data=librosa.core.stft(wav_data, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "    abslt=np.absolute(framed_data)**2\n",
    "    dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "    for i in range(m.floor(w/2)):\n",
    "        dft_signal=np.insert(dft_signal,0,0,axis = 1)\n",
    "    data_phase=np.angle(framed_data)\n",
    "    for i in range(m.floor(w/2)):\n",
    "        data_phase=np.insert(data_phase,0,0,axis = 1)\n",
    "    d.resize(d.shape[0]+dft_signal.shape[1], axis=0)   \n",
    "    d[-1*dft_signal.shape[1]:] = dft_signal.T\n",
    "    dd.resize(dd.shape[0]+data_phase.shape[1], axis=0)   \n",
    "    dd[-1*data_phase.shape[1]:] = data_phase.T\n",
    "\n",
    "f.close()\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mixed_log_30h_nozeroinsert and mixed_phase_30h_nozeroinsert are the log and phase of mixed files without zero elements in between.\n",
    "\n",
    "they're both stored as h5py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = os.listdir(os.path.join(mixed_folder))\n",
    "a = natsorted(a)\n",
    "\n",
    "mixed_logname = 'mixed_log_dev_nozeroinsert'\n",
    "mixed_phasename = 'mixed_phase_dev_nozeroinsert'\n",
    "mixed_logpath = os.path.normpath(os.path.join(write_path,mixed_logname))\n",
    "mixed_phasepath = os.path.normpath(os.path.join(write_path,mixed_phasename))\n",
    "\n",
    "f = h5py.File(mixed_logpath+'.hdf5', 'w')\n",
    "d = f.create_dataset(mixed_logname, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "ff = h5py.File(mixed_phasepath+'.hdf5', 'w')\n",
    "dd = ff.create_dataset(mixed_phasename, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "\n",
    "for filename in a:\n",
    "    wav_data, sr = librosa.load(os.path.join(mixed_folder,filename), sr=16000) \n",
    "    framed_data=librosa.core.stft(wav_data, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "    abslt=np.absolute(framed_data)**2\n",
    "    dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "    data_phase=np.angle(framed_data)\n",
    "    d.resize(d.shape[0]+dft_signal.shape[1], axis=0)   \n",
    "    d[-1*dft_signal.shape[1]:] = dft_signal.T\n",
    "    dd.resize(dd.shape[0]+data_phase.shape[1], axis=0)   \n",
    "    dd[-1*data_phase.shape[1]:] = data_phase.T\n",
    "    i=i+1\n",
    "f.close()\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'single_dataset_log_30h' and 'single_dataset_phase_30h'are log and phase of clean data.\n",
    "they are both without zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_snrs=21\n",
    "num_of_speakers=6\n",
    "\n",
    "files = os.listdir(os.path.normpath(os.path.join(write_path,clean_raw)))\n",
    "files=natsorted(files)\n",
    "\n",
    "clean_log = 'single_dataset_log_dev'\n",
    "clean_phase = 'single_dataset_phase_dev'\n",
    "clean_log_path = os.path.normpath(os.path.join(write_path,clean_log))\n",
    "clean_phase_path = os.path.normpath(os.path.join(write_path,clean_phase))\n",
    "\n",
    "f = h5py.File(clean_log_path+'.hdf5', 'w')\n",
    "d = f.create_dataset(clean_log, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "ff = h5py.File(clean_phase_path+'.hdf5', 'w')\n",
    "dd = ff.create_dataset(clean_phase, (0,257),maxshape=(None,257), dtype='f', chunks=True)\n",
    "\n",
    "for voice in files:\n",
    "    wav_data, sr = librosa.load(os.path.join(write_path,clean_raw,voice), sr=16000) \n",
    "    framed_data=librosa.core.stft(wav_data, n_fft=512, hop_length=256, win_length=512, window='hann')\n",
    "    abslt=np.absolute(framed_data)**2\n",
    "    dft_signal=np.log10(abslt+1e-7*np.ones(np.shape(abslt)))\n",
    "    data_phase=np.angle(framed_data)\n",
    "#     for i in range(num_of_snrs*num_of_speakers):\n",
    "    d.resize(d.shape[0]+dft_signal.shape[1], axis=0)   \n",
    "    d[-1*dft_signal.shape[1]:] = dft_signal.T\n",
    "    dd.resize(dd.shape[0]+data_phase.shape[1], axis=0)   \n",
    "    dd[-1*data_phase.shape[1]:] = data_phase.T\n",
    "f.close()\n",
    "ff.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ftr_refrmd_30h' is the refrmd(contexted) version of mixed_log_30h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_logname='mixed_log_dev'\n",
    "mixed_phasename='mixed_phase_dev'\n",
    "mixed_log_path = os.path.normpath(os.path.join(write_path,mixed_logname))\n",
    "h5f = h5py.File(mixed_log_path+'.hdf5','r')\n",
    "ftr = h5f[mixed_logname][0:]\n",
    "\n",
    "ftr_refrmd=[]\n",
    "for i in range(m.floor(w/2)):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        temp.extend(ftr[i+p+j])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "        \n",
    "refrmd_file='ftr_refrmd_dev'\n",
    "refrmd_file_path = os.path.normpath(os.path.join(write_path,refrmd_file))\n",
    "f = h5py.File(refrmd_file_path+'.hdf5', 'w')\n",
    "d = f.create_dataset(refrmd_file, (0,257*w),maxshape=(None,257*w), dtype='f', chunks=True)\n",
    "\n",
    "for i in range(m.floor(w/2),ftr.shape[0]-m.floor(w/2)-1):\n",
    "    k=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        temp.extend(ftr[i+k])\n",
    "        k=k+1\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "    if len(ftr_refrmd)>50000 or i==ftr.shape[0]-m.floor(w/2)-2:\n",
    "        d.resize(d.shape[0]+len(ftr_refrmd), axis=0)   \n",
    "        d[-1*len(ftr_refrmd):] = ftr_refrmd\n",
    "        ftr_refrmd=[]\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n",
    "for i in range(ftr.shape[0]-m.floor(w/2)-1,ftr.shape[0]):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        if j>m.floor(w/2)+(ftr.shape[0]-i)-1:\n",
    "            temp.extend(np.zeros(ftr[ftr.shape[0]-1].shape))\n",
    "        else:\n",
    "            temp.extend(ftr[i+p+j])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "        \n",
    "d.resize(d.shape[0]+len(ftr_refrmd), axis=0)   \n",
    "d[-1*len(ftr_refrmd):] = ftr_refrmd\n",
    "ftr_refrmd=[]\n",
    "\n",
    "f.close()\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'mixed_log_30h_norm' is the normalized version of 'mixed_log_30h_nozeroinsert'. normalized by features to zero mean and unit variance.(using mean and variance of itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='mixed_log_dev_nozeroinsert'\n",
    "file_path = os.path.normpath(os.path.join(write_path,file))\n",
    "h5f = h5py.File(file_path+'.hdf5', 'r')\n",
    "d=h5f[file]\n",
    "tot_len=d.shape[0]\n",
    "dim = d.shape[1]\n",
    "\n",
    "lst=0\n",
    "tot_num=(tot_len//10000)*10000\n",
    "for i in range(0,tot_num,10000):\n",
    "    mixed = h5f[file][i:i+10000]\n",
    "    lst=lst+np.sum(mixed,axis=0)\n",
    "\n",
    "mixed = h5f[file][tot_num:]\n",
    "lst=lst+np.sum(mixed,axis=0)\n",
    "tot_mean=lst/tot_len\n",
    "lst=0\n",
    "h5f = h5py.File(file_path+'.hdf5','r')\n",
    "for i in range(0,tot_num,10000):\n",
    "    mixed = h5f[file][i:i+10000]\n",
    "    lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "del mixed  \n",
    "\n",
    "mixed = h5f[file][tot_num:]\n",
    "lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "lst=lst/tot_len\n",
    "tot_std=np.sqrt(lst)\n",
    "h5f.close()\n",
    "normalized_file = 'mixed_log_dev_norm'\n",
    "normalized_file_path =  os.path.normpath(os.path.join(write_path,normalized_file))\n",
    "f = h5py.File(normalized_file_path+'.hdf5', 'w')\n",
    "dd = f.create_dataset(normalized_file, (0,dim),maxshape=(None,dim), dtype='f', chunks=True)\n",
    "h5f = h5py.File(file_path+'.hdf5','r')\n",
    "for i in range(0,tot_num,10000):\n",
    "    mixed = h5f[file][i:i+10000]\n",
    "    scaled=(mixed-tot_mean)/tot_std\n",
    "    dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "    dd[-1*scaled.shape[0]:] = scaled\n",
    "del mixed    \n",
    "mixed = h5f[file][tot_num:]\n",
    "scaled=(mixed-tot_mean)/tot_std\n",
    "dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "dd[-1*scaled.shape[0]:] = scaled\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'mean_mixed_log_dev.txt')),tot_mean)\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'std_mixed_log_dev.txt')),tot_std)\n",
    "\n",
    "f.close()\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'mixed_log_30h_norm' is the normalized version of 'mixed_log_30h_nozeroinsert'. normalized by features to zero mean and unit variance.(using mean and variance of train file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='mixed_log_dev_nozeroinsert'\n",
    "file_path = os.path.normpath(os.path.join(write_path,file))\n",
    "train_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/30hdata'\n",
    "tot_mean=np.loadtxt(os.path.normpath(os.path.join(train_path,'mean_mixed_log.txt')))\n",
    "tot_std=np.loadtxt(os.path.normpath(os.path.join(train_path,'std_mixed_log.txt')))\n",
    "\n",
    "normalized_file = 'mixed_log_dev_norm'\n",
    "normalized_file_path =  os.path.normpath(os.path.join(write_path,normalized_file))\n",
    "f = h5py.File(normalized_file_path+'.hdf5', 'w')\n",
    "dd = f.create_dataset(normalized_file, (0,dim),maxshape=(None,dim), dtype='f', chunks=True)\n",
    "h5f = h5py.File(file_path+'.hdf5','r')\n",
    "for i in range(0,tot_num,10000):\n",
    "    mixed = h5f[file][i:i+10000]\n",
    "    scaled=(mixed-tot_mean)/tot_std\n",
    "    dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "    dd[-1*scaled.shape[0]:] = scaled\n",
    "del mixed    \n",
    "mixed = h5f[file][tot_num:]\n",
    "scaled=(mixed-tot_mean)/tot_std\n",
    "dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "dd[-1*scaled.shape[0]:] = scaled\n",
    "\n",
    "f.close()\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ftr_refrmd_10h_norm' is the normalized(by features) of 'ftr_refrmd_10h'.\n",
    "\n",
    "normalized to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "refrmd_file='ftr_refrmd_dev'\n",
    "refrmd_file_path = os.path.normpath(os.path.join(write_path,refrmd_file))\n",
    "h5f = h5py.File(refrmd_file_path+'.hdf5', 'r')\n",
    "d=h5f[refrmd_file]\n",
    "tot_len=d.shape[0]\n",
    "dim = d.shape[1]\n",
    "lst=0\n",
    "\n",
    "for i in range(0,tot_num,10000):\n",
    "    print(i)\n",
    "    mixed = h5f[refrmd_file][i:i+10000]\n",
    "    lst=lst+np.sum(mixed,axis=0)\n",
    "tot_num=(tot_len//10000)*10000\n",
    "mixed = h5f[refrmd_file][tot_num:]\n",
    "lst=lst+np.sum(mixed,axis=0)\n",
    "print(mixed.shape)\n",
    "tot_mean=lst/tot_len\n",
    "lst=0\n",
    "h5f = h5py.File(refrmd_file_path+'.hdf5','r')\n",
    "for i in range(0,tot_num,10000):\n",
    "    mixed = h5f[refrmd_file][i:i+10000]\n",
    "    lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "del mixed  \n",
    "mixed = h5f[refrmd_file][tot_num:]\n",
    "lst=lst+np.sum((mixed - tot_mean)**2,axis=0)\n",
    "lst=lst/tot_len\n",
    "tot_std=np.sqrt(lst)\n",
    "h5f.close()\n",
    "normalized_file = 'ftr_refrmd_dev_norm'\n",
    "normalized_file_path =  os.path.normpath(os.path.join(write_path,normalized_file))\n",
    "f = h5py.File(normalized_file_path+'.hdf5', 'w')\n",
    "dd = f.create_dataset(normalized_file, (0,dim),maxshape=(None,dim), dtype='f', chunks=True)\n",
    "h5f = h5py.File(refrmd_file_path+'.hdf5','r')\n",
    "for i in range(0,tot_num,10000):\n",
    "    print(i)\n",
    "    mixed = h5f[refrmd_file][i:i+10000]\n",
    "    scaled=(mixed-tot_mean)/tot_std\n",
    "    dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "    dd[-1*scaled.shape[0]:] = scaled\n",
    "del mixed    \n",
    "mixed = h5f[refrmd_file][tot_num:]\n",
    "print(mixed.shape)\n",
    "scaled=(mixed-tot_mean)/tot_std\n",
    "dd.resize(dd.shape[0]+scaled.shape[0], axis=0)   \n",
    "dd[-1*scaled.shape[0]:] = scaled\n",
    "\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'mean.txt')),tot_mean)\n",
    "np.savetxt(os.path.normpath(os.path.join(write_path,'std.txt')),tot_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is converted from h5py to tfrecords format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum0.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum0.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum1.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum1.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum2.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum2.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum3.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum3.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum4.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum4.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum5.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum5.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum6.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum6.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum7.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum7.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum8.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum8.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum9.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum9.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum10.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum10.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum11.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum11.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum12.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum12.tfrecords done!\n",
      "Serializing 100000 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum13.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum13.tfrecords done!\n",
      "Serializing 77711 examples into D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum14.tfrecords\n",
      "Writing D:\\studies\\university\\thesis\\speech_separation_codes\\du16\\donesomestuff\\tfrecord_files\\mixed_dev_norm\\filenum14.tfrecords done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "__author__ = \"Sangwoong Yoon\"\n",
    "\n",
    "def np_to_tfrecords(X, Y, file_path_prefix, verbose=True):\n",
    "    \"\"\"\n",
    "    Converts a Numpy array (or two Numpy arrays) into a tfrecord file.\n",
    "    For supervised learning, feed training inputs to X and training labels to Y.\n",
    "    For unsupervised learning, only feed training inputs to X, and feed None to Y.\n",
    "    The length of the first dimensions of X and Y should be the number of samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray of rank 2\n",
    "        Numpy array for training inputs. Its dtype should be float32, float64, or int64.\n",
    "        If X has a higher rank, it should be rshape before fed to this function.\n",
    "    Y : numpy.ndarray of rank 2 or None\n",
    "        Numpy array for training labels. Its dtype should be float32, float64, or int64.\n",
    "        None if there is no label array.\n",
    "    file_path_prefix : str\n",
    "        The path and name of the resulting tfrecord file to be generated, without '.tfrecords'\n",
    "    verbose : bool\n",
    "        If true, progress is reported.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input type is not float (64 or 32) or int.\n",
    "    \n",
    "    \"\"\"\n",
    "    def _dtype_feature(ndarray):\n",
    "        \"\"\"match appropriate tf.train.Feature class with dtype of ndarray. \"\"\"\n",
    "        assert isinstance(ndarray, np.ndarray)\n",
    "        dtype_ = ndarray.dtype\n",
    "        if dtype_ == np.float64 or dtype_ == np.float32:\n",
    "            return lambda array: tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "        elif dtype_ == np.int64:\n",
    "            return lambda array: tf.train.Feature(int64_list=tf.train.Int64List(value=array))\n",
    "        else:  \n",
    "            raise ValueError(\"The input should be numpy ndarray. \\\n",
    "                               Instaed got {}\".format(ndarray.dtype))\n",
    "            \n",
    "    assert isinstance(X, np.ndarray)\n",
    "    assert len(X.shape) == 2  # If X has a higher rank, \n",
    "                               # it should be rshape before fed to this function.\n",
    "    assert isinstance(Y, np.ndarray) or Y is None\n",
    "    \n",
    "    # load appropriate tf.train.Feature class depending on dtype\n",
    "    dtype_feature_x = _dtype_feature(X)\n",
    "    if Y is not None:\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        assert len(Y.shape) == 2\n",
    "        dtype_feature_y = _dtype_feature(Y)            \n",
    "    \n",
    "    # Generate tfrecord writer\n",
    "    result_tf_file = file_path_prefix + '.tfrecords'\n",
    "    writer = tf.python_io.TFRecordWriter(result_tf_file)\n",
    "    if verbose:\n",
    "        print(\"Serializing {:d} examples into {}\".format(X.shape[0], result_tf_file))\n",
    "        \n",
    "    # iterate over each sample,\n",
    "    # and serialize it as ProtoBuf.\n",
    "    for idx in range(X.shape[0]):\n",
    "        x = X[idx]\n",
    "        if Y is not None:\n",
    "            y = Y[idx]\n",
    "        \n",
    "        d_feature = {}\n",
    "        d_feature['X'] = dtype_feature_x(x)\n",
    "        if Y is not None:\n",
    "            d_feature['Y'] = dtype_feature_y(y)\n",
    "            \n",
    "        features = tf.train.Features(feature=d_feature)\n",
    "        example = tf.train.Example(features=features)\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Writing {} done!\".format(result_tf_file))\n",
    "\n",
    "        \n",
    "#################################    \n",
    "##      Test and Use Cases     ##\n",
    "#################################\n",
    "import h5py\n",
    "load_size = 100000\n",
    "k = 0\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/devdata'\n",
    "input_name = 'mixed_log_dev_norm'\n",
    "input_path = os.path.normpath(os.path.join(write_path,input_name))\n",
    "target_name = 'single_dataset_log_dev'\n",
    "target_path = os.path.normpath(os.path.join(write_path,target_name))\n",
    "parent = 'tfrecord_files'\n",
    "tf_folder = 'mixed_dev_norm'\n",
    "h5f1 = h5py.File(input_path+'.hdf5','r')\n",
    "hh = h5py.File(target_path+'.hdf5', 'r')\n",
    "d=hh[target_name]\n",
    "data_len = d.shape[0]\n",
    "index = np.arange(0, data_len, load_size)\n",
    "if not os.path.exists(os.path.join(Data_path,parent,tf_folder)):\n",
    "    os.makedirs(os.path.join(Data_path,parent,tf_folder))\n",
    "for I in index:\n",
    "    if data_len-I<load_size:\n",
    "        xx = h5f1[input_name][I:]\n",
    "        yy = hh[target_name][I:]\n",
    "    else:\n",
    "        xx = h5f1[input_name][I:I+load_size]\n",
    "        yy = hh[target_name][I:I+load_size]\n",
    "    np_to_tfrecords(xx, yy, os.path.normpath(os.path.join(Data_path,parent,tf_folder,'filenum'+str(k))), verbose=True)\n",
    "    k = k+1\n",
    "# 1-2. Check if the data is stored correctly\n",
    "# open the saved file and check the first entries\n",
    "# for serialized_example in tf.python_io.tf_record_iterator('test.tfrecords'):\n",
    "#     example = tf.train.Example()\n",
    "#     example.ParseFromString(serialized_example)\n",
    "#     x_1 = np.array(example.features.feature['X'].float_list.value)\n",
    "#     y_1 = np.array(example.features.feature['Y'].float_list.value)\n",
    "#     break\n",
    "\n",
    "# np_to_tfrecords(X, Y, file_path_prefix, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1477711, 257)\n",
      "(1477711, 257)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/devdata'\n",
    "input_name = 'mixed_log_dev_norm'\n",
    "input_path = os.path.normpath(os.path.join(write_path,input_name))\n",
    "target_name = 'single_dataset_log_dev'\n",
    "target_path = os.path.normpath(os.path.join(write_path,target_name))\n",
    "h5f1 = h5py.File(input_path+'.hdf5','r')\n",
    "hh = h5py.File(target_path+'.hdf5', 'r')\n",
    "d=hh[target_name]\n",
    "data_len = d.shape[0]\n",
    "x = h5f1[input_name]\n",
    "print(x.shape)\n",
    "print(d.shape)\n",
    "h5f1.close()\n",
    "hh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "input_name = 'mixed_log_10h_norm'\n",
    "input_path = os.path.normpath(os.path.join(write_path,input_name))\n",
    "target_name = 'single_dataset_log_10h'\n",
    "target_path = os.path.normpath(os.path.join(write_path,target_name))\n",
    "h5f1 = h5py.File(input_path+'.hdf5','r')\n",
    "hh = h5py.File(target_path+'.hdf5', 'r')\n",
    "d = hh[target_name][0:500000]\n",
    "x = h5f1[input_name][0:500000]\n",
    "small_path = os.path.normpath(os.path.join(write_path,'mixed_log_norm_small.h5'))\n",
    "hf = h5py.File(small_path, 'w')\n",
    "hf.create_dataset('mixed_log_norm_small', data=d)\n",
    "hf.close()\n",
    "small_path = os.path.normpath(os.path.join(write_path,'single_dataset_log_small.h5'))\n",
    "hf = h5py.File(small_path, 'w')\n",
    "hf.create_dataset('single_dataset_log_small', data=x)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_logname='mixed_log_10h'\n",
    "mixed_phasename='mixed_phase_10h'\n",
    "mixed_log_path = os.path.normpath(os.path.join(write_path,mixed_logname))\n",
    "clean_phase_path = os.path.normpath(os.path.join(write_path,mixed_phasename))\n",
    "h5f = h5py.File(mixed_log_path+'.hdf5','r')\n",
    "ftr = h5f[mixed_logname][0:]\n",
    "\n",
    "ftr_refrmd=[]\n",
    "ftr_refrmd_norm=[]\n",
    "print('0')\n",
    "for i in range(m.floor(w/2)):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        temp.extend(ftr[i+p+j])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "\n",
    "print('1')\n",
    "refrmd_file='ftr_refrmd_10h_norm'\n",
    "refrmd_file_path = os.path.normpath(os.path.join(write_path,refrmd_file))\n",
    "f = h5py.File(refrmd_file_path+'.hdf5', 'w')\n",
    "d = f.create_dataset(refrmd_file, (0,257*w),maxshape=(None,257*w), dtype='f', chunks=True)\n",
    "print('2')\n",
    "for i in range(m.floor(w/2),ftr.shape[0]-m.floor(w/2)-1):\n",
    "    k=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        temp.extend(ftr[i+k])\n",
    "        k=k+1\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "    if len(ftr_refrmd)>50000 or i==ftr.shape[0]-m.floor(w/2)-2:\n",
    "        print(len(ftr_refrmd))\n",
    "#         for ftr_num in range(len(ftr_refrmd)):\n",
    "        ftr_refrmd_norm = preprocessing.scale(ftr_refrmd, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "#             print(ftr_refrmd_norm.shape)\n",
    "        d.resize(d.shape[0]+len(ftr_refrmd_norm), axis=0)   \n",
    "        d[-1*len(ftr_refrmd_norm):] = ftr_refrmd_norm\n",
    "        print(len(ftr_refrmd_norm))\n",
    "        ftr_refrmd_norm=[]\n",
    "        ftr_refrmd=[]\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "for i in range(ftr.shape[0]-m.floor(w/2)-1,ftr.shape[0]):\n",
    "    p=-m.floor(w/2)\n",
    "    temp = []\n",
    "    for j in range(w):\n",
    "        if np.all(ftr[i]==0):\n",
    "            break\n",
    "        if j>m.floor(w/2)+(ftr.shape[0]-i)-1:\n",
    "            temp.extend(np.zeros(ftr[ftr.shape[0]-1].shape))\n",
    "        else:\n",
    "            temp.extend(ftr[i+p+j])\n",
    "    if np.any(temp!=[]):\n",
    "        ftr_refrmd.append(temp)\n",
    "# for ftr_num in range(len(ftr_refrmd)):\n",
    "ftr_refrmd_norm = preprocessing.scale(ftr_refrmd, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "d.resize(d.shape[0]+len(ftr_refrmd_norm), axis=0)   \n",
    "d[-1*len(ftr_refrmd_norm):] = ftr_refrmd_norm\n",
    "ftr_refrmd=[]\n",
    "ftr_refrmd_norm=[]\n",
    "f.close()\n",
    "h5f.close()\n",
    "##############################################\n",
    "##############################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
