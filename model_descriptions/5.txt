epochs=50
data=tfrecord_files_3h_norm or ftr_refrmd_3h_norm
w=3
h=[1024,512]
batch_size=128
activation function on all layers is leaky relu except for the output layer 
batch normalization was applied before first layer activation
no activation function is applied on the output layer
shuffle was on
optimizer = adam

