data is randomized here so its different fromthe data in past models.
files
data is normalized in waveform type.
context = 1
clean_10h
clean_10h_bysnr
mixed_10h
mixed_10h_bysnr
mixed_log_10h_norm
mixed_phase_10h_norm
single_dataset_log_10h
single_dataset_phase_10h
mixed_10h_norm
validation_10h_norm
epochs = 30
architecture
len_data = 2100000
val_len = 50000(not sure about the sxact size)
2 hidden layers = [1024, 1024, 1024, 1024]
input>hidden_layer>batch norm>tanh>hidden layer>batch norm>leaky_relu
>hidden layer>batch norm>leaky_relu>hidden layer>batch norm>leaky_relu>output linear layer(257)
opt = SGD( learning_rate= 0.1, decay =0.001)
all layers initialized lecun_uniform
